---
title: 训练集、测试集分布探索分析
author: 王哲峰
date: '2023-02-09'
slug: train-test-distribution
categories:
  - data analysis
tags:
  - machinelearning
---

<style>
details {
    border: 1px solid #aaa;
    border-radius: 4px;
    padding: .5em .5em 0;
}
summary {
    font-weight: bold;
    margin: -.5em -.5em 0;
    padding: .5em;
}
details[open] {
    padding: .5em;
}
details[open] summary {
    border-bottom: 1px solid #aaa;
    margin-bottom: .5em;
}
</style>


<details><summary>目录</summary><p>

- [单变量数据分布不一致](#单变量数据分布不一致)
  - [缺失值情况](#缺失值情况)
  - [数值特征分布](#数值特征分布)
  - [类别特征分布](#类别特征分布)
    - [交集和占比情况](#交集和占比情况)
    - [KL 散度](#kl-散度)
- [多变量数据分布不一致](#多变量数据分布不一致)
  - [多变量的衍生变量](#多变量的衍生变量)
  - [基于模型的对抗验证](#基于模型的对抗验证)
    - [基于模型寻找分布差异最大的特征](#基于模型寻找分布差异最大的特征)
- [参考](#参考)
</p></details><p></p>


机器学习建模的一个非常重要的假设，就是要保证训练数据的分布和测试集样本的分布是相似的，亦或来自同一分布

`$$(X_{train}, Y_{train}), (X_{test}, Y_{test}) i.i.d \sim F(X, Y)$$`

如果测试数据的分布跟训练数据不一致，那么训练得到的模型的效果就很难知晓。
这也是数据竞赛中为什么开始就要先对训练集测试集分布的进行探索的一大重要原因

目前 90% 以上的数据竞赛都是会同时给出训练集和测试集数据，
这种情况下，如果数据的分布不一致，那么模型的预估将大打折扣。
尤其是重要特征的分布不一致的情况下，更会造成严重的线下线上不一致的情况。
该问题在 kaggle 的很多比赛中也是受到非常大关注的，
例如在 kaggle 的 PLAsTicc Astronomical Classification 竞赛中，
很多朋友发现自己模型的线下效果极好，但是线上效果却极差无比，而这其中最大的原因是什么呢？

训练集和测试集的分布不一致造成的，而且是重要特征的分布不一致造成的。
那么如何快速地定位训练集和测试集合的分布不一致呢？本文介绍几种实战中最为常用的技巧

# 单变量数据分布不一致

在很多数据量比较大的情况下，我们使用基于模型的策略其实代价是比较大的，
这个时候我们往往还得回到最初数据探索的方式，而在数据竞赛和数据建模的问题中，
最需要注意的有下面的几点

## 缺失值情况

有些字段在训练集中都很正常，但是在测试集合中缺失极为严重，甚至99%以上的数据都缺失了。
这种时候最简单的策略就是直接将训练集中的字段直接进行删除。这么做虽然线下的验证效果会变差，
但是线上线下的效果会变得更为稳定

## 数值特征分布

数值特征的分布也是非常重要的一环，
而关于数值特征分布一致不一致最为简单和常用的方式就是直接绘制对应字段在训练集合和测试集合中的分布
例如：可以直接使用 seaborn 中的 distplot 函数来绘制，也可以用 kde(kernel density estimation)，
一种非参数检验方法之一

如果发现训练集和测试集的交集非常少，如果不是因为主办方数据划分时的问题，
那么可能就需要研究产生这种情况的原因了

## 类别特征分布

### 交集和占比情况

类别特征在许多竞赛中也是出现次数最为频繁的，而关于类别特征，
需要重点检查的就是训练集和测试集合中元素的差异。最需要检测的就是：

* 在测试集合中的类别是否在训练集合中都存在？

```python
len(set(test["feature"]) - set(train["feature"]))
```

* 在测试集中出现的类别而未在训练集中出现的比例是多少？(Overlap 的比例)

```python
len(set(test["feature"]) - set(train["feature"])) / len(set(train["feature"]))
```

* 在测试集中出现的类别而未在训练集中出现的占总类别个数的比例是多少？(Overlap 的比例)

```python
len(set(test["feature"]) - set(train["feature"])) / len(set(test["feature"]) + set(train["feature"]))
```

### KL 散度

KL 散度经常被用于衡量两个概率分布的匹配程度的指标，两个分布差异越大，KL 散度越大

# 多变量数据分布不一致

多变量不同于单变量的观察，因为多变量没法直接看出来，但还是有一些非常通用的手段，
将其划分为两大类：

* 基于衍生变量的观测探索
* 基于模型的对抗验证策略

## 多变量的衍生变量

该方法和自己做特征类似，先对特征进行演化，常采用下面的策略：

```python
train.groupby(feature)[feature2].agg(stas)
test.groupby(feature)[feature2].agg(stas)
```

然后基于演化之后的特征再进行细致的研究观察，此处可以直接使用单变量观测时候的技巧即可

## 基于模型的对抗验证

该方法是目前为止最为通用的策略，其思路也非常简单：

1. 将训练集的数据全部打标签为 1，将测试集的数据全部打标签为 0
2. 将训练集和测试集的数据合并，然后进行 N 折交叉验证
3. 如果交叉验证的 AUC 接近 0.5，那么说明训练集和测试集的分布是类似的；
   如果 AUC 非常大，例如大于 0.9，那么我们就认为训练集和测试集的分布是存在较大差异的
  
### 基于模型寻找分布差异最大的特征

通过训练好的模型，可以直接输出各个模型的特征重要性，
排在最前面的特征就是造成训练集和测试集合分布不一致的重要因素。
所以很容易就找到训练集和测试集合分布差异大的特征

虽然通过模型的方式得到了各个特征的重要性，但是要注意的是，
这些特征仅仅只是训练集和测试集能分开的重要信息，
此外并不能说明太多，例如最简单的，对训练集和测试集加入 index 信息

```python
train["index"] = list(range(train.shape[0]))
test["index"] = list(range(test.shape[0]) + train.shape[0])
```

这样就可以得到 AUC=1.0 的结果，但是也会发现加入这些到模型中其实线上和线下的 gap 很多时候并不是非常大，
大家是不是好奇为什么了？明明分布差异那么大，但是却影响不大。其实这个也很容易解释，
如果该特征在真实训练的时候并不是强特征，而仅仅只是在对抗训练中是强特征，这最终的影响其实就很小了。
所以在很多时候，在遇到线上分数和线下验证分数严重不一致的时候，还需要判断模型训练中强特在对抗训练中的重要性

* 如果二者都很重要，那么大概率会出现不一致现象
* 如果在目标模型训练中是弱特征，在对抗训练中是强特，那么不一定会出现线上线下不一致的现象

所以要想真的做好这块，需要多次细心的验证

# 参考

* [机器学习避坑指南：训练集/测试集分布一致性检查]()
* [训练集、验证集和测试集]()
* [机器学习中，数据的分布是指什么呢？]()
* [The Iris Dataset]()
* [Kaggle Adversarial validation]()
* [Kullback–Leibler divergence]()
* [训练集测试集分布不一致性探索](https://mp.weixin.qq.com/s?__biz=Mzk0NDE5Nzg1Ng==&mid=2247492564&idx=1&sn=269f32f313dcfad888fdb76d8cfab2cc&chksm=c32afa5bf45d734d91d16303b91385acac38b880fbf1397805fdb9684bbfb646edaacb68d12c&scene=21#wechat_redirect)

