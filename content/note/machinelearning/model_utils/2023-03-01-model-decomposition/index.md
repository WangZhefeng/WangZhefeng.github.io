---
title: 特征降维
author: 王哲峰
date: '2023-03-01'
slug: model-decomposition
categories:
  - feature engine
tags:
  - machinelearning
---

<style>
details {
    border: 1px solid #aaa;
    border-radius: 4px;
    padding: .5em .5em 0;
}
summary {
    font-weight: bold;
    margin: -.5em -.5em 0;
    padding: .5em;
}
details[open] {
    padding: .5em;
}
details[open] summary {
    border-bottom: 1px solid #aaa;
    margin-bottom: .5em;
}
</style>

<details><summary>目录</summary><p>

- [降维和特征选择](#降维和特征选择)
- [特征降维简介](#特征降维简介)
  - [PCA 和 LDA 的区别](#pca-和-lda-的区别)
- [主成分分析](#主成分分析)
  - [原理](#原理)
  - [算法](#算法)
  - [优缺点](#优缺点)
- [线性判别分析](#线性判别分析)
- [t-SNE](#t-sne)
- [独立成分分析](#独立成分分析)
- [LASSO](#lasso)
- [SCAD](#scad)
- [Elastic Net](#elastic-net)
- [Logistic Regression L1](#logistic-regression-l1)
- [其他](#其他)
- [参考](#参考)
</p></details><p></p>

# 降维和特征选择

在机器学习中，特征降维和特征选择是两个常见的概念。特征降维和特征选择的目的都是使数据的特征维数降低，
但实际上两者的区别是很大，它们的本质是完全不同的

特征选择从数据集中选择最重要特征的子集，特征选择不会改变原始特征的含义和数值，只是对原始特征进行筛选。
而降维将数据转换为低维空间，会改变原始特征中特征的含义和数值，可以理解为低维的特征映射。
这两种策略都可以用来提高机器学习模型的性能和可解释性，但它们的运作方式是截然不同的

![img](images/dr_fs.png)

# 特征降维简介

降低数据集中特征的维数，同时保持尽可能多的信息的技术被称为降维。
可以最大限度地降低数据复杂性并提高模型性能、模型的解释能力

当特征选择完成后，可以直接训练模型了，但是可能由于特征矩阵过大，导致计算量大、训练时间长，
因此降低特征矩阵维度也是必不可少的。常见的降维方法有：

* 基于 L1 惩罚项的模型
    - LASSO
    - SCAD
    - Elastic Net
    - Logistic with L1 Loss
* 主成分分析(Principal Components Analysis, PCA)
* 线性判别分析(Linear Discriminant Analysis, LDA)：线性判别分析本身也是一个分类模型
* 因子分析(Factor Analysis, FA)
* 独立成分分析(Independ Component Analysis, ICA)

## PCA 和 LDA 的区别

PCA 和 LDA 有很多的相似点，其本质是要将原始的样本映射到维度更低的样本空间中，
但是 PCA 和 LDA 的映射目标不一样：

* PCA 是为了让映射后的样本具有最大的发散性
* LDA 是为了让映射后的样本有最好的分类性能
 
所以说 PCA 是一种无监督的降维方法，而 LDA 是一种有监督的降维方法

# 主成分分析

主成分分析(Principle Component Analysis, PCA)，是一种传统的统计学方法，
被机器学习领域引入后，通常被认为是一种特殊的非监督学习算法，其可以对复杂或多变量的数据做预处理，
以减少次要变量，便于进一步使用精简后的主要变量进行数学建模和统计学模型的训练

PCA 可识别一组不相关的变量，将原始变量进行线性组合，称为主成分。
第一个主成分解释了数据中最大的方差，然后每个后续成分解释主键变少。
PCA 经常用作机器学习算法的数据预处理步骤，因为它有助于降低数据复杂性并提高模型性能

## 原理

1. 去除平均值
2. 计算协方差矩阵
3. 计算协方差矩阵的特征值和特征向量
4. 将特征值排序
5. 保留前 `$N$` 个最大的特征值对应的特征向量
6. 将数据转换到上面得到的 `$N$` 个特征向量构建的新空间中(实现了特征压缩)

即：

1. 找出第一个主成分的方向，也就是数据方差最大的方向
2. 找出第二个主成分的方向，也就是数据方差次大的方向, 
   并且该方向与第一个主成分方向正交(orthogonal，如果是二维空间就叫垂直)
3. 通过这种方式计算出所有的主成分方向
4. 通过数据集的协方差矩阵及其特征值分析，我们就可以得到这些主成分的值
5. 一旦得到了协方差矩阵的特征值和特征向量，我们就可以保留最大的 N 个特征。
   这些特征向量也给出了 N 个最重要特征的真实结构，
   我们就可以通过将数据乘上这 N 个特征向量从而将它转换到新的空间上 

## 算法

输入：`$m$` 个 `$n$` 维样本数据 `$D = (x^{(1)}, x^{(2)}, \ldots, x^{(m)})$`

输出：`$m$` 个 `$k$` 维样本数据

1. 对样本集进行标准化
2. 计算样本的协方差矩阵 `$XX^{T}$`
3. 对协方差矩阵进行特征分解，得到 `$n$` 个特征向量和其对应的特征值
4. 取出最大的 `$k$` 个特征值对应的特征向量 `$(\omega_1, \omega_2, \ldots, \omega_k)$`，
   将所有的特征向量标准化后，组成特征向量矩阵 `$W$`
5. 对样本集中每一个样本 `$x^{(i)}$`，转化为新的样本 `$z^{(i)}=W^{T}x^{(i)}$`
6. 得到输出的样本数据 `$D_{pca} = (z^{(1)}, z^{(2)}, \ldots, z^{(m)})$`

## 优缺点

优点: 

* 降低数据复杂性, 识别最重要的多个特征

缺点: 

* 可能损失有用信息
* 只适用于数值型数据

# 线性判别分析

线性判别分析（LDA）是一种用于分类工作的统计工具。它的工作原理是确定数据属性的线性组合，
最大限度地分离不同类别。为了提高模型性能，LDA 经常与其他分类技术(如逻辑回归或支持向量机)结合使用

# t-SNE

t-SNE(t-分布随机邻居嵌入)是一种非线性降维方法，特别适用于显示高维数据集。
它保留数据的局部结构来，也就是说在原始空间中靠近的点在低维空间中也会靠近。
t-SNE 经常用于数据可视化，因为它可以帮助识别数据中的模式和关系

# 独立成分分析

独立成分分析（Independent Component Analysis，ICA）实际上也是对数据在原有特征空间中做的一个线性变换。
相对于 PCA 这种降秩操作，ICA 并不是通过在不同方向上方差的大小，即数据在该方向上的分散程度来判断那些是主要成分，
那些是不需要到特征。而 ICA 并没有设定一个所谓主要成分和次要成分的概念，ICA 认为所有的成分同等重要，
而我们的目标并非将重要特征提取出来，而是找到一个线性变换，使得变换后的结果具有最强的独立性。
PCA 中的不相关太弱，我们希望数据的各阶统计量都能利用，即我们利用大于 2 的统计量来表征。而 ICA 并不要求特征是正交的

# LASSO

# SCAD

# Elastic Net

# Logistic Regression L1

# 其他

* 多维缩放
* 自编码器

# 参考

* [深入理解PCA降维](https://mp.weixin.qq.com/s/uAlBtGTmtBSjcnp9bWQr5Q)
* [一文读懂主成分分析](https://mp.weixin.qq.com/s?__biz=MjM5MjAxMDM4MA==&mid=2651890105&idx=1&sn=3c425c538dacd67b1732948c5c015b46&scene=21#wechat_redirect)
