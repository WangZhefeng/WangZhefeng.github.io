---
title: PyTorch 
author: 王哲峰
date: '2018-04-05'
slug: dl-pytorch
categories:
  - deeplearning
tags:
  - tool
output:
  blogdown::html_page:
    toc: true
    fig_width: 6
    dev: "svg"
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#需要掌握的内容">需要掌握的内容</a></li>
<li><a href="#pytorch-tensor-的创建">1.PyTorch tensor 的创建</a>
<ul>
<li><a href="#tensor-的介绍">1.1 tensor 的介绍</a></li>
<li><a href="#tensor-的创建">1.2 tensor 的创建</a>
<ul>
<li><a href="#直接创建">1.2.1 直接创建</a></li>
<li><a href="#依数值创建">1.2.2 依数值创建</a></li>
</ul></li>
<li><a href="#tensor-的操作">1.3 tensor 的操作</a></li>
<li><a href="#cuda-tensor">1.4 cuda tensor</a></li>
</ul></li>
<li><a href="#pytorch-数据读取与预处理">2.PyTorch 数据读取与预处理</a>
<ul>
<li><a href="#dataset-和-dataloader">2.1 Dataset 和 DataLoader</a>
<ul>
<li><a href="#使用-torchvision-torchtext-torchaudio-dataset">2.1.1 使用 TorchVision, TorchText, TorchAudio dataset</a></li>
<li><a href="#使用文件创建自定义的-dataset">2.1.2 使用文件创建自定义的 Dataset</a></li>
<li><a href="#torch.utils.data-api">2.1.3 torch.utils.data API</a></li>
</ul></li>
</ul></li>
<li><a href="#pytorch-数据预处理">3.PyTorch 数据预处理</a>
<ul>
<li><a href="#transforms-示例">3.1 Transforms 示例</a></li>
<li><a href="#torchvision-transforms">3.2 torchvision transforms</a></li>
<li><a href="#torchtext-transforms">3.3 torchtext transforms</a></li>
<li><a href="#torchaudio-transforms">3.4 torchaudio transforms</a></li>
<li><a href="#pytorch-数据并行">2.3 PyTorch 数据并行</a>
<ul>
<li><a href="#让模型跑在-gpu-上">2.3.1 让模型跑在 GPU 上</a></li>
<li><a href="#让模型跑在多个-gpu-上">2.3.2 让模型跑在多个 GPU 上</a></li>
</ul></li>
</ul></li>
<li><a href="#pytorch-nn">3.PyTorch nn</a>
<ul>
<li><a href="#定义神经网">定义神经网</a></li>
</ul></li>
<li><a href="#pytorch-动态图自动求导">4.PyTorch 动态图、自动求导</a>
<ul>
<li><a href="#计算图">4.1 计算图</a></li>
<li><a href="#pytorch-动态图机制">4.2 Pytorch 动态图机制</a></li>
<li><a href="#pytorch-自动求导机制">4.3 PyTorch 自动求导机制</a></li>
</ul></li>
<li><a href="#pytorch-模型">5.PyTorch 模型</a>
<ul>
<li><a href="#pytorch-模型的创建">5.1 PyTorch 模型的创建</a></li>
<li><a href="#pytorch-模型容器">5.2 PyTorch 模型容器</a></li>
</ul></li>
<li><a href="#pytorch-权重初始化">6.PyTorch 权重初始化</a>
<ul>
<li><a href="#权重初始化方法">6.1 权重初始化方法</a></li>
</ul></li>
<li><a href="#pytorch-损失函数">7.PyTorch 损失函数</a></li>
<li><a href="#pytorhc-优化器">8.PyTorhc 优化器</a></li>
<li><a href="#pytorch-评价指标">9.PyTorch 评价指标</a></li>
<li><a href="#pytorch-模型保存和加载">10.PyTorch 模型保存和加载</a>
<ul>
<li><a href="#保存和加载模型权重参数">10.1 保存和加载模型权重参数</a></li>
<li><a href="#保存和加载整个模型">10.2 保存和加载整个模型</a></li>
<li><a href="#导出模型为-onnx">10.3 导出模型为 ONNX</a></li>
<li><a href="#保存和加载-checkpoint">10.4 保存和加载 checkpoint</a></li>
</ul></li>
<li><a href="#pytorch-cheat-sheet">11.PyTorch Cheat Sheet</a>
<ul>
<li><a href="#imports">11.1 Imports</a></li>
<li><a href="#tensors">11.2 Tensors</a></li>
<li><a href="#deep-learning">11.3 Deep Learning</a></li>
<li><a href="#data-utilities">11.4 Data Utilities</a></li>
</ul></li>
</ul>
</div>

<details>
<summary>
目录
</summary>
<p>
<ul>
<li><a href="#需要掌握的内容">需要掌握的内容</a></li>
</ul>
</p>
</details>
<p>
</p>
<div id="需要掌握的内容" class="section level1">
<h1>需要掌握的内容</h1>
<ul class="task-list">
<li><input type="checkbox" disabled="" />
Get Stated<ul class="task-list">
<li><input type="checkbox" disabled="" />
Start Locally</li>
<li><input type="checkbox" disabled="" />
Start via Partners</li>
</ul></li>
<li><input type="checkbox" disabled="" />
Tutorials<ul>
<li>Learn the Basics
<ul class="task-list">
<li><input type="checkbox" disabled="" />
Introduction to PyTorch<ul class="task-list">
<li><input type="checkbox" disabled="" checked="" />
Learn the Basics</li>
<li><input type="checkbox" disabled="" />
Quickstart</li>
<li><input type="checkbox" disabled="" />
Tensors</li>
<li><input type="checkbox" disabled="" />
Datasets &amp; DataLoaders</li>
<li><input type="checkbox" disabled="" />
Transform</li>
<li><input type="checkbox" disabled="" checked="" />
Save and Load the Model</li>
</ul></li>
<li><input type="checkbox" disabled="" />
Learning PyTorch</li>
<li><input type="checkbox" disabled="" />
Image and Video</li>
<li><input type="checkbox" disabled="" />
Text</li>
<li><input type="checkbox" disabled="" />
Deploying PyTorch Model in Production</li>
<li><input type="checkbox" disabled="" />
Model Optimization</li>
<li><input type="checkbox" disabled="" />
Parallel and Distributed Training</li>
</ul></li>
<li>PyTorch Recipes
<ul class="task-list">
<li><input type="checkbox" disabled="" />
快速入门</li>
<li><input type="checkbox" disabled="" />
自定义层、训练循环</li>
<li><input type="checkbox" disabled="" />
分布式训练</li>
</ul></li>
<li>Additional Resources
<ul class="task-list">
<li><input type="checkbox" disabled="" />
Examples of PyTorch</li>
<li><input type="checkbox" disabled="" />
PyTorch Cheat Sheet</li>
<li><input type="checkbox" disabled="" />
Tutorials on GitHub</li>
<li><input type="checkbox" disabled="" />
Run Tutorials on Google Colab</li>
</ul></li>
</ul></li>
<li><input type="checkbox" disabled="" />
Docs<ul class="task-list">
<li><input type="checkbox" disabled="" />
PyTorch</li>
<li><input type="checkbox" disabled="" />
torchaudio</li>
<li><input type="checkbox" disabled="" />
torchtext</li>
<li><input type="checkbox" disabled="" />
torchvision</li>
<li><input type="checkbox" disabled="" />
TorchElastic</li>
<li><input type="checkbox" disabled="" />
TorchServe</li>
<li><input type="checkbox" disabled="" />
PyTorch on XLA Devices</li>
</ul></li>
<li><input type="checkbox" disabled="" />
GitHub<ul class="task-list">
<li><input type="checkbox" disabled="" />
<a href="https://github.com/pytorch/pytorch" class="uri">https://github.com/pytorch/pytorch</a></li>
<li><input type="checkbox" disabled="" />
<a href="https://github.com/onnx/tutorials" class="uri">https://github.com/onnx/tutorials</a></li>
</ul></li>
</ul>
</div>
<div id="pytorch-tensor-的创建" class="section level1">
<h1>1.PyTorch tensor 的创建</h1>
<ul>
<li>1.tensor 的简介及创建
<ul>
<li>tensor 是多维数组</li>
<li>tensor 的创建
<ul>
<li>直接创建
<ul>
<li><code>torch.tensor()</code></li>
<li><code>torch.from_numpy()</code></li>
</ul></li>
<li>依数值创建
<ul>
<li><code>torch.empty()</code></li>
<li><code>torch.ones()</code></li>
<li><code>torch.zeros()</code></li>
<li><code>torch.eye()</code></li>
<li><code>torch.full()</code></li>
<li><code>torch.arange()</code></li>
<li><code>torch.linspace()</code></li>
</ul></li>
<li>依概率分布创建
<ul>
<li><code>torch.normal()</code></li>
<li><code>torch.randn()</code></li>
<li><code>torch.rand()</code></li>
<li><code>torch.randint()</code></li>
<li><code>torch.randperm()</code></li>
</ul></li>
</ul></li>
</ul></li>
<li>2.tensor 的操作
<ul>
<li>tensor 的基本操作
<ul>
<li>tensor 的拼接
<ul>
<li><code>torch.cat()</code></li>
<li><code>torch.stack()</code></li>
</ul></li>
<li>tensor 的切分
<ul>
<li><code>torch.chunk()</code></li>
<li><code>torch.split()</code></li>
</ul></li>
<li>tensor 的索引
<ul>
<li><code>index_select()</code></li>
<li><code>masked_select()</code></li>
</ul></li>
<li>tensor 的变换
<ul>
<li><code>torch.reshape()</code></li>
<li><code>torch.transpose()</code></li>
<li><code>torch.t</code></li>
</ul></li>
</ul></li>
<li>tensor 的数学运算
<ul>
<li><code>add(input, aplha, other)</code></li>
</ul></li>
</ul></li>
</ul>
<div id="tensor-的介绍" class="section level2">
<h2>1.1 tensor 的介绍</h2>
<p>tensor 是 PyTorch 中最基本的概念,其参与了整个运算过程,
这里主要介绍 tensor 的概念和属性, 如 data, variable, device 等,
并且介绍 tensor 的基本创建方法, 如直接创建、依属主创建、依概率分布创建等</p>
<ul>
<li>tensor
<ul>
<li>tensor 其实是多维数组,它是标量、向量、矩阵的高维拓展</li>
</ul></li>
<li>tensor 与 variable
<ul>
<li>在 PyTorch 0.4.0 版本之后 variable 已经并入 tensor,
但是 variable 这个数据类型对于理解 tensor 来说很有帮助,
variable 是 <code>torch.autograd</code> 中的数据类型.
variable(<code>torch.autograd.variable</code>) 有 5 个属性,
这些属性都是为了 tensor 的自动求导而设置的:
<ul>
<li><code>data</code></li>
<li><code>grad</code></li>
<li><code>grad_fn</code></li>
<li><code>requires_grad</code></li>
<li><code>is_leaf</code></li>
</ul></li>
<li>tensor(<code>torch.tensor</code>) 有 8 个属性:
<ul>
<li>与数据本身相关
<ul>
<li><code>data</code>: 被包装的 tensor</li>
<li><code>dtype</code>: tensor 的数据类型, 如 <code>torch.floattensor</code>, <code>torch.cuda.floattensor</code>, <code>float32</code>, <code>int64(torch.long)</code></li>
<li><code>shape</code>: tensor 的形状</li>
<li><code>device</code>: tensor 所在的设备, gpu/cup, tensor 放在 gpu 上才能使用加速</li>
</ul></li>
<li>与梯度求导相关
<ul>
<li><code>requires_grad</code>: 是否需要梯度</li>
<li><code>grad</code>: <code>data</code> 的梯度</li>
<li><code>grad_fn</code>: fn 表示 function 的意思，记录创建 tensor 时用到的方法</li>
<li><code>is_leaf</code>: 是否是叶子节点(tensor)</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</div>
<div id="tensor-的创建" class="section level2">
<h2>1.2 tensor 的创建</h2>
<pre class="python"><code>from __future__ import print_function
import numpy as np
import torch</code></pre>
<div id="直接创建" class="section level3">
<h3>1.2.1 直接创建</h3>
<ol style="list-style-type: decimal">
<li>torch.tensor(): 从 data 创建 tensor api</li>
</ol>
<ul>
<li><p>API</p>
<pre class="python"><code>torch.tensor(
    data,                   # list, numpy
    dtype = none,
    device = none,
    requires_grad = false,
    pin_memory = false      # 是否存于锁页内存
)</code></pre></li>
<li><p>示例</p>
<pre class="python"><code>arr = np.ones((3, 3))

t = torch.tensor(arr, device = &quot;cuda&quot;)
print(t)</code></pre></li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>通过 numpy array 来创建 tensor api</li>
</ol>
<blockquote>
<p>创建的 tensor 与原 ndarray 共享内存，当修改其中一个数据的时候，另一个也会被改动</p>
</blockquote>
<ul>
<li><p>API</p>
<pre class="python"><code>torch.from_numpy(ndarray)</code></pre></li>
<li><p>示例</p>
<pre class="python"><code># np.ndarray
arr = np.array(
    [
        [1, 2, 3], 
        [4, 5, 6]
    ]
)
print(arr)

# torch.tensor
t = torch.from_numpy(arr)
print(t)

# 修改 arr    
arr[0, 0] = 0
print(arr, t)

# 修改 torch.tensor
t[1, 1] = 100
print(arr, t)</code></pre></li>
</ul>
</div>
<div id="依数值创建" class="section level3">
<h3>1.2.2 依数值创建</h3>
<ul>
<li><p>API</p>
<pre class="python"><code>torch.zeros(
    *size,
    out = none,             # 输出张量，就是把这个张量赋值给另一个张量，但这两个张量一样，指的是同一个内存地址
    dtype = none,
    layout = torch.strided, # 内存的布局形式
    device = none,
    requires_grad = false
)</code></pre></li>
<li><p>示例</p>
<pre class="python"><code>out_t = torch.tensor([1])
t = torch.zeros((3, 3), out = out_t)
print(out_t, t)
print(id(out_t), id(t), id(t) == id(out_t))</code></pre></li>
</ul>
</div>
</div>
<div id="tensor-的操作" class="section level2">
<h2>1.3 tensor 的操作</h2>
<blockquote>
<ul>
<li>相加
<ul>
<li><code>+</code></li>
<li><code>torch.add(, out)</code></li>
<li><code>.add_()</code></li>
</ul></li>
<li>index
<ul>
<li><code>[:, :]</code></li>
</ul></li>
<li>resize
<ul>
<li><code>.view()</code></li>
<li><code>.size()</code></li>
</ul></li>
<li>object trans
<ul>
<li><code>.items()</code></li>
</ul></li>
<li>numpy.array to torch.tensor
<ul>
<li><code>torch.from_numpy()</code></li>
</ul></li>
<li>torch.tensor to numpy.array
<ul>
<li><code>.numpy()</code></li>
</ul></li>
</ul>
</blockquote>
<ul>
<li><p>add</p>
<pre class="python"><code>import torch

x = torch.zeros(5, 3, dtype = torch.long)
y = torch.rand(5, 3)

# method 1
print(x + y)

# method 2
print(torch.add(x, y))

# method 3
result = torch.empty(5, 3)
torch.add(x, y, out = result)
print(result)

# method 4
y.add_(x)
print(y)</code></pre></li>
<li><p>index</p>
<pre class="python"><code>import torch

x = torch.zeros(5, 3, dtype = torch.long)
print(x[:, 1])</code></pre></li>
<li><p>resize</p>
<pre class="python"><code>import torch

x = torch.randn(4, 4)
y = x.view(16)
z = x.view(-1, 8)
print(x.size(), y.size(), z.size())</code></pre></li>
<li><p>object trans</p>
<pre class="python"><code>import torch

x = torch.randn(1)
print(x)
print(x.item()) # python number</code></pre></li>
<li><p>torch tensor 2 numpy array</p>
<pre class="python"><code>import torch

a = torch.ones(5)
b = a.numpy()
print(a)
print(b)

a.add_(1)
print(a)
print(b)</code></pre></li>
<li><p>numpy array 2 torch tensor</p>
<pre class="python"><code>import numpy as np

a = np.ones(5)
b = torch.from_numpy(a)
np.add(a, 1, out = a)

print(a)
print(b)</code></pre></li>
</ul>
</div>
<div id="cuda-tensor" class="section level2">
<h2>1.4 cuda tensor</h2>
<pre class="python"><code># let us run this cell only if cuda is available
# we will use `torch.device` objects to move tensors in and out of gpu
x = torch.tensor([1])
if torch.cuda.is_available():
   device = torch.device(&quot;cuda&quot;)            # a cuda device object

   y = torch.ones_like(x, device = device)  # directly create a tensor on gpu
   
   x = x.to(device)                         # or just use strings `.to(&quot;cuda&quot;)`
   
   z = x + y
   z.to(&quot;cpu&quot;, torch.double)                # `.to` can also change dtype together!</code></pre>
</div>
</div>
<div id="pytorch-数据读取与预处理" class="section level1">
<h1>2.PyTorch 数据读取与预处理</h1>
<div id="dataset-和-dataloader" class="section level2">
<h2>2.1 Dataset 和 DataLoader</h2>
<blockquote>
<ul>
<li>Dataset 保存了数据的样本和标签</li>
<li>DataLoader 将 Dataset 封装为可迭代对象, 便于访问样本</li>
</ul>
</blockquote>
<div id="使用-torchvision-torchtext-torchaudio-dataset" class="section level3">
<h3>2.1.1 使用 TorchVision, TorchText, TorchAudio dataset</h3>
<ul>
<li>加载相关库</li>
</ul>
<pre class="python"><code>from torch.utils.data import Dataset
from torch.utils.data import DataLoader
from torchvision import datasets
from torchvision.transforms import ToTensor</code></pre>
<ul>
<li>下载、读取数据</li>
</ul>
<pre class="python"><code>training_data = datasets.FashionMNIST(
    root = &quot;data&quot;,
    trian = True,
    download = True,
    transform = ToTensor(),
)

test_data = datasets.FashionMNIST(
    root = &quot;data&quot;,
    train = False,
    download = True,
    transform = ToTensor(),
)</code></pre>
<ul>
<li>生成数据加载器</li>
</ul>
<pre class="python"><code>batch_size = 64

# create data loaders
train_dataloader = DataLoader(trianing_data, batch_size = batch_size)
test_dataloader = DataLoader(test_data, batch_size = batch_size)

for X, y in test_dataloader:
    print(f&quot;Shape of X [N, C, H, W]: {X.shape}&quot;)
    print(f&quot;Shape of y: {y.shape} {y.dtype}&quot;)
    break

# Display image and label.
train_features, train_labels = next(iter(train_dataloader))
print(f&quot;Feature batch shape: {train_features.size()}&quot;)
print(f&quot;Labels batch shape: {train_labels.size()}&quot;)
img = train_features[0].squeeze()
label = train_labels[0]
plt.imshow(img, cmap=&quot;gray&quot;)
plt.show()
print(f&quot;Label: {label}&quot;)</code></pre>
</div>
<div id="使用文件创建自定义的-dataset" class="section level3">
<h3>2.1.2 使用文件创建自定义的 Dataset</h3>
<blockquote>
<ul>
<li><code>Dataset</code> 类需要实现以下方法
<ul>
<li><code>__init__</code></li>
<li><code>__len__</code></li>
<li><code>__getitem__</code></li>
</ul></li>
</ul>
</blockquote>
<ul>
<li>记载相关库</li>
</ul>
<pre class="python"><code>import os
import pandas as pd
import torchvision.io import read_image
# TODO import torchtext.iop import *
from torch.utils.data import Dataset
from torch.utils.data import DataLoader</code></pre>
<ul>
<li><p>创建 Dataset 类</p>
<ul>
<li>文件所在目录结构如下 TODO</li>
</ul>
<pre><code>- img_dir
    - 
    - labels.csv
        tshirt1.jpg, 0
        tshirt2.jpg, 0
        ...
        ankleboot999.jpg, 9</code></pre></li>
</ul>
<pre class="python"><code>class CustomImage(Dataset):

    def __init__(self, annotations_file, img_dir, transform = None, target_transform = None):
        self.img_labels = pd.read_csv(annotations_file)
        self.img_dir = img_dir
        self.transform = transform
        self.target_transform = target_transform

    def __len__(self):
        return len(self.img_labels)
    
    def __getitem__(self, idx):
        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])
        image = read_image(img_path)
        label = self.img_labels.iloc[idx, 1]
        if self.transform:
            image = self.transform(image)
        if self.target_transform:
            label = self.target_transform(label)
        return image, label</code></pre>
<ul>
<li>使用 DataLoader 准备训练数据集</li>
</ul>
<pre class="python"><code>training_data = CustomImage(
    annotations_file = &quot;&quot;,
    img_dir = &quot;&quot;,
    transform = ToTensor(),
    target_transform = ToTensor(),
)
test_data = CustomImage(
    annotations_file = &quot;&quot;,
    img_dir = &quot;&quot;,
    transform = ToTensor(),
    target_transform = ToTensor(),
)

train_dataloader = DataLoader(training_data, batch_size = 64, shuffle = True)
test_dataloader = DataLoader(test_data, batch_size = 64, shuffle = True)

# Display image and label.
train_features, train_labels = next(iter(train_dataloader))
print(f&quot;Feature batch shape: {train_features.size()}&quot;)
print(f&quot;Labels batch shape: {train_labels.size()}&quot;)
img = train_features[0].squeeze()
label = train_labels[0]
plt.imshow(img, cmap=&quot;gray&quot;)
plt.show()
print(f&quot;Label: {label}&quot;)</code></pre>
</div>
<div id="torch.utils.data-api" class="section level3">
<h3>2.1.3 torch.utils.data API</h3>
<ul>
<li>torch.utils
<ul>
<li>torch.utils.benchmark</li>
<li>torch.utils.bottleneck</li>
<li>torch.utils.checkpoint</li>
<li>torch.utils.cpp_extension</li>
<li><strong>torch.utils.data</strong></li>
<li>torch.utils.dlpack</li>
<li>torch.utils.mobile_optimizer</li>
<li>torch.utils.model_zoo</li>
<li>torch.utils.tensorboard</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="pytorch-数据预处理" class="section level1">
<h1>3.PyTorch 数据预处理</h1>
<ul>
<li>torchvision.transforms</li>
<li>torchtext.transforms</li>
<li>torchaudio.transforms</li>
</ul>
<div id="transforms-示例" class="section level2">
<h2>3.1 Transforms 示例</h2>
<pre class="python"><code>import torch
from torchvision import datasets
from torchvision import ToTensor, Lambda

training_data = datasets.FashionMNIST(
    root = &quot;data&quot;,
    train = True,
    download = True,
    transform = ToTensor(),
    target_transform = Lambda(
        lambda y: torch.zeros(10, dtype = torch.float)
                       .scatter_(dim = 0, torch.tensor(y), value = 1)
    )
)
test_data = datasets.FashionMNIST(
    root = &quot;data&quot;,
    train = False,
    download = True,
    transform = ToTensor(),
    target_transform = Lambda(
        lambda y: torch.zeros(10, dtype = torch.float)
                       .scatter_(dim = 0, torch.tensor(y), value = 1)
    )
)</code></pre>
</div>
<div id="torchvision-transforms" class="section level2">
<h2>3.2 torchvision transforms</h2>
<ul>
<li>Scriptable transforms</li>
<li>Compositions of transforms</li>
<li>Transforms on PIL Image and torch.*Tensor</li>
<li>Transforms on PIL Image only</li>
<li>Transforms on torch.*Tensor only</li>
<li>Conversion transforms</li>
<li>Generic transforms</li>
<li>Automatic Augmentation transforms</li>
<li>Functional transforms</li>
</ul>
</div>
<div id="torchtext-transforms" class="section level2">
<h2>3.3 torchtext transforms</h2>
</div>
<div id="torchaudio-transforms" class="section level2">
<h2>3.4 torchaudio transforms</h2>
</div>
<div id="pytorch-数据并行" class="section level2">
<h2>2.3 PyTorch 数据并行</h2>
<div id="让模型跑在-gpu-上" class="section level3">
<h3>2.3.1 让模型跑在 GPU 上</h3>
<pre class="python"><code>import torch

# 让模型在 GPU 上运行
device = torch.device(&quot;cuda:0&quot;)
model.to(device)

# 将 tensor 复制到 GPU 上
my_tensor = torch.ones(2, 2, dtype = torch.double)
mytensor = my_tensor.to(device)</code></pre>
</div>
<div id="让模型跑在多个-gpu-上" class="section level3">
<h3>2.3.2 让模型跑在多个 GPU 上</h3>
<blockquote>
<ul>
<li>PyTorch 默认使用单个 GPU 执行运算</li>
</ul>
</blockquote>
<pre class="python"><code>model = nn.DataParallel(model)</code></pre>
<pre class="python"><code>import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

# Parameters and DataLoaders
input_size = 5
output_size = 2

batch_size = 30
data_size = 100

# Device
device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)

class RandomDataset(Dataset):

    def __init__(self, size, length):
        self.len = length
        self.data = torch.randn(length, size)

    def __len__(self):
        return self.len
    
    def __getitem__(self, index):
        return self.data[index]

rand_loader = DataLoader(
    dataset = RandomDataset(input_size, data_size), 
    batch_size = batch_size, 
    shuffle = True
)

class Model(nn.Module):

    def __init__(self, input_size, output_size):
        super(Model, self)__init__()
        self.fc = nn.Linear(input_size, output_size)

    def forward(self, input):
        output = self.fc(input)
        print(&quot;\tIn Model: input size&quot;, input.size(),
                &quot;output size&quot;, output.size())

        return output

model = Model(input_size, output_size)
if torch.cuda.device_count() &gt; 1:
print(&quot;Let&#39;s use&quot;, torch.cuda.device_count(), &quot;GPUs!&quot;)
model = nn.DataParallel(model)

model.to(device)


for data in rand_loader:
    input = data.to(device)
    output = model(input)
    print(&quot;Outside: input size&quot;, input.size(),
        &quot;output_size&quot;, output.size())</code></pre>
</div>
</div>
</div>
<div id="pytorch-nn" class="section level1">
<h1>3.PyTorch nn</h1>
<p>A typical training procedure for a neural network is as follows:</p>
<ul>
<li>Define the neural network that has some learnable parameters (or
weights)</li>
<li>Iterate over a dataset of inputs</li>
<li>Process input through the network</li>
<li>Compute the loss (how far is the output from being correct)</li>
<li>Propagate gradients back into the network’s parameters</li>
<li>Update the weights of the network, typically using a simple update
rule: <code>weight = weight - learning_rate * gradient</code></li>
</ul>
<div id="定义神经网" class="section level2">
<h2>定义神经网</h2>
<pre class="python"><code>import torch
import torch.nn as nn
import torch.nn.functional as F


device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;
print(f&quot;Using {device} device&quot;)


class Net(nn.Module):

    def __init__(self):
        super(Net, self).__init__()
        # 1 input image channel, 6 output channels, 3x3 square convolution kernel
        self.conv1 = nn.Conv2d(1, 6, 3)
        self.conv2 = nn.Conv2d(6, 16, 3)
        # an affine operation: y = Wx + b
        self.fc1 = nn.Linear(16 * 6 * 6, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        # Max pooling over a (2, 2) window
        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
        # If the size is a square you can only specify a single number
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)
        x = x.view(-1, self.num_flat_features(x))
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)

        return x

    def num_flat_features(self, x):
        size = x.size()[1:]
        num_features = 1
        for s in size:
            num_features *= s

        return num_features


net = Net().to(device)
print(net)</code></pre>
<pre class="python"><code>params = list(net.parameters)
print(len(params))
print(params[0].size()) # conv1&#39;s .weight</code></pre>
<pre class="python"><code>net.zero_grad()
out.backward(torch.randn(1, 10))</code></pre>
</div>
</div>
<div id="pytorch-动态图自动求导" class="section level1">
<h1>4.PyTorch 动态图、自动求导</h1>
<ul>
<li><p>4.1 计算图</p>
<ul>
<li><p>描述运算的有向无环图</p>
<ul>
<li><p>Tensor 的 is_leaf 属性</p></li>
<li><p>Tensor 的 grad_fn 属性</p></li>
</ul></li>
</ul></li>
<li><p>4.2 PyTorch 动态图机制</p>
<ul>
<li>动态图与静态图</li>
</ul></li>
<li><p>4.3 PyTorch 自动求导机制</p>
<ul>
<li><p>torch.autograd.backward() 方法自动求取梯度</p></li>
<li><p>torch.autograd.grad() 方法可以高阶求导</p></li>
<li><p>note</p>
<ul>
<li>梯度不自动清零</li>
<li>依赖叶节点的节点, requires_grad 默认为 True</li>
<li>叶节点不能执行原位操作</li>
</ul></li>
</ul></li>
</ul>
<div id="计算图" class="section level2">
<h2>4.1 计算图</h2>
<p>计算图是用来描述运算的有向无环图。主要有两个因素:节点、边。
其中节点表示数据，如向量、矩阵、张量；而边表示运算，如加减乘除、卷积等。</p>
<p>使用计算图的好处不仅是让计算看起来更加简洁，还有个更大的优势是让梯度求导也变得更加方便。</p>
<ul>
<li>示例:</li>
</ul>
<pre class="python"><code>x = torch.tensor([2.], requires_grad = True)
w = torch.tensor([1.], requires_grad = True)

a = torch.add(w, x)
b = torch.add(w, 1)

y = torch.mul(a, b)

y.backward()
print(w.grad)</code></pre>
</div>
<div id="pytorch-动态图机制" class="section level2">
<h2>4.2 Pytorch 动态图机制</h2>
</div>
<div id="pytorch-自动求导机制" class="section level2">
<h2>4.3 PyTorch 自动求导机制</h2>
<ul>
<li>package <code>autograd</code></li>
<li>torch.Tensor</li>
<li>.requires_grad = True</li>
<li>.backward()</li>
<li>.grad</li>
<li>.detach()</li>
<li>with torch.no_grad(): pass</li>
<li>.grad_fn</li>
</ul>
<p>PyTorch 自动求导机制使用的是 <code>torch.autograd.backward</code> 方法，功能就是自动求取梯度。</p>
<ul>
<li>API:</li>
</ul>
<pre class="python"><code>torch.autograd.backward(
   tensors, 
   gard_tensors = None, 
   retain_graph = None, 
   create_graph = False
)</code></pre>
<p><code>autograd</code> 包提供了对所有 Tensor 的自动微分操作</p>
<pre class="python"><code>#!/usr/bin/env python
# -*- coding: utf-8 -*-

&quot;&quot;&quot;
`autograd` package:
-------------------
跟踪 torch.Tensor 上所有的操作:torch.Tensor(requires_grad = True)
自动计算所有的梯度:.backward()
torch.Tensor 上的梯度:.grad
torch.Tensor 是否被跟踪:.requires_grad
停止跟踪 torch.Tensor 上的跟踪历史、未来的跟踪:.detach()

with torch.no_grad():
      pass

Function
.grad_fn
&quot;&quot;&quot;

import torch

# --------------------
# 创建 Tensor 时设置 requires_grad 跟踪前向计算
# --------------------
x = torch.ones(2, 2, requires_grad = True)
print(&quot;x:&quot;, x)

y = x + 2
print(&quot;y:&quot;, y)
print(&quot;y.grad_fn:&quot;, y.grad_fn)

z = y * y * 3
print(&quot;z:&quot;, z)
print(&quot;z.grad_fn&quot;, z.grad_fn)

out = z.mean()
print(&quot;out:&quot;, out)
print(&quot;out.grad_fn:&quot;, out.grad_fn)
out.backward()
print(&quot;x.grad:&quot;, x.grad)

# --------------------
# .requires_grad_() 能够改变一个已经存在的 Tensor 的 `requires_grad`
# --------------------
a = torch.randn(2, 2)
a = ((a * 3) / (a - 1))
print(&quot;a.requires_grad&quot;, a.requires_grad)
a.requires_grad_(True)
print(&quot;a.requires_grad:&quot;, a.requires_grad)
b = (a * a).sum()
print(&quot;b.grad_fn&quot;, b.grad_fn)



# 梯度
x = torch.randn(3, requires_grad = True)
y = x * 2
while y.data.norm() &lt; 1000:
      y = y * 2
v = torch.tensor([0.1, 1.0, 0.0001], dtype = torch.float)
y.backward(v)
print(x.grad)

# .requires_grad
print(x.requires_grad)
print((x ** 2).requires_grad)

with torch.no_grad():
      print((x ** 2).requires_grad)

# .detach()
print(x.requires_grad)
y = x.detach()
print(y.requires_grad)
print(x.eq(y).all())</code></pre>
</div>
</div>
<div id="pytorch-模型" class="section level1">
<h1>5.PyTorch 模型</h1>
<div id="pytorch-模型的创建" class="section level2">
<h2>5.1 PyTorch 模型的创建</h2>
</div>
<div id="pytorch-模型容器" class="section level2">
<h2>5.2 PyTorch 模型容器</h2>
</div>
</div>
<div id="pytorch-权重初始化" class="section level1">
<h1>6.PyTorch 权重初始化</h1>
<div id="权重初始化方法" class="section level2">
<h2>6.1 权重初始化方法</h2>
<p>正确的权重初始化可以加速模型的收敛，不恰当的权重初始化导致输出层的输出过大或者过小，
最终导致梯度爆炸或者消失，使得模型无法训练</p>
<ul>
<li>使用与饱和激活函数 tanh 等的 Xavier 初始化方法</li>
<li>非饱和激活函数 relu 等的 Kaiming 初始化方法</li>
</ul>
</div>
</div>
<div id="pytorch-损失函数" class="section level1">
<h1>7.PyTorch 损失函数</h1>
</div>
<div id="pytorhc-优化器" class="section level1">
<h1>8.PyTorhc 优化器</h1>
</div>
<div id="pytorch-评价指标" class="section level1">
<h1>9.PyTorch 评价指标</h1>
</div>
<div id="pytorch-模型保存和加载" class="section level1">
<h1>10.PyTorch 模型保存和加载</h1>
<div id="保存和加载模型权重参数" class="section level2">
<h2>10.1 保存和加载模型权重参数</h2>
<p>PyTorch 将模型训练的学习到的权重参数保存在一个状态字典中, <code>state_dict</code></p>
<ul>
<li>模型保存</li>
</ul>
<pre class="python"><code>import torch
import torchvision.models as models

# 保存模型
model = models.vgg16(pretrained = True)
MODEL_PATH = &quot;models/model_weights.pth&quot;
torch.save(
    model.state_dict(), 
    MODEL_PATH
)</code></pre>
<ul>
<li>模型加载</li>
</ul>
<pre class="python"><code>import torch
import torchvision.models as models

# do not specify pretrained=True, i.e. do not load default weights
model = models.vgg16()
MODEL_PATH = &quot;models/model_weights.pth&quot;
model.load_state_dict(
    torch.load(MODEL_PATH)
)
model.eval()</code></pre>
</div>
<div id="保存和加载整个模型" class="section level2">
<h2>10.2 保存和加载整个模型</h2>
<ul>
<li>模型保存</li>
</ul>
<pre class="python"><code>import torch
import torchvision.models as models

model = models.vgg16(pretrained = True)
MODEL_PATH = &quot;models/model.pth&quot;
torch.save(the_model, MODEL_PATH)</code></pre>
<ul>
<li>模型加载</li>
</ul>
<pre class="python"><code>import torch

MODEL_PATH = &quot;models/model.pth&quot;
model = torch.load(MODEL_PATH)  # require pickle module</code></pre>
</div>
<div id="导出模型为-onnx" class="section level2">
<h2>10.3 导出模型为 ONNX</h2>
<ul>
<li>PyTorch 还具有原生 ONNX 导出支持。然而，鉴于 PyTorch 执行图的动态特性，
导出过程必须遍历执行图以生成持久的 ONNX 模型。出于这个原因，
应该将适当大小的测试变量传递给导出例程</li>
<li><a href="https://github.com/onnx/tutorials">ONNX 教程</a></li>
</ul>
<pre class="python"><code>import torch
import torch.onnx as onnx

input_image = torch.zeros((1, 3, 224, 224))</code></pre>
</div>
<div id="保存和加载-checkpoint" class="section level2">
<h2>10.4 保存和加载 checkpoint</h2>
<ul>
<li>加载相关库</li>
</ul>
<pre class="python"><code>import torch
import torch.nn as nn
import torch.optim as optim</code></pre>
<ul>
<li>定义和初始化模型</li>
</ul>
<pre class="python"><code>class Net(nn.Module):

    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

net = Net()
print(net)</code></pre>
<ul>
<li>初始化优化器</li>
</ul>
<pre class="python"><code>optimizer = optim.SGD(net.parameters(), lr = 0.001, momentum = 0.9)</code></pre>
<ul>
<li>保存 checkpoint</li>
</ul>
<pre class="python"><code>EPOCH = 5
PATH = &quot;model.pt&quot;
LOSS = 0.4

torch.save({
    &quot;epoch&quot;: EPOCH,
    &quot;model_state_dict&quot;: net.state_dict(),
    &quot;optimizer_state_dict&quot;: optimizer.state_dict(),
    &quot;loss&quot;: LOSS,
}, PATH)</code></pre>
<ul>
<li>加载 checkpoint</li>
</ul>
<pre class="python"><code>PATH = &quot;model.pt&quot;

# 初始化模型
model = Net()
# 初始化优化器
optimizer = optim.SGD(net.parameters(), lr = 0.001, momentum = 0.9)

# 加载 checkpoint
checkpoint = torch.load(PATH)

model.load_state_dict(checkpoint[&quot;model_state_dict&quot;])
optimizer.load_state_dict(checkpoint[&quot;optimizer_state_dict&quot;])

epoch = checkpoint[&quot;epoch&quot;]
loss = checkpoint[&quot;loss&quot;]

model.eval()
# - or - 
model.train()</code></pre>
</div>
</div>
<div id="pytorch-cheat-sheet" class="section level1">
<h1>11.PyTorch Cheat Sheet</h1>
<div id="imports" class="section level2">
<h2>11.1 Imports</h2>
<pre class="python"><code># General
import torch # root package
from torch.utils.data import Dataset, Dataloader # dataset representation and loading

# Neural Network API
from torch import Tensor
import torch.nn as nn
import torch.nn.functional as F
import torch.autograd as autograd
import torch.optim as optim
from torch.jit import script, trace

# Torchscript and JIT
torch.jit.trace()
@script

# ONNX
torch.onnx.export(model, dummy data, xxxx.proto)
model = onnx.load(&quot;alexnet.proto&quot;)
onnx.checker.check_model(model)
onnx.helper.printable_graph(model.graph)

# Vision
from torchvision import datasets, models, transforms
import torchvision.transforms as transforms

# Distributed Training
import torch.distributed as dist
from torch.multiprocessing import Process</code></pre>
</div>
<div id="tensors" class="section level2">
<h2>11.2 Tensors</h2>
<pre class="python"><code>import torch

# Creation
x = torch.randn(*size)
x = torch.[ones|zeros](*size)
x = torch.tensor(L)
y = x.clone()
with torch.no_grad():
requires_grad = True

# Dimensionality
x.size()
x = torch.cat(tensor_seq, dim = 0)
y = x.view(a, b, ...)
y - x.view(-1,a)
y = x.transpose(a, b)
y = x.permute(*dims)
y = x.unsqueeze(dim)
y = x.unsqueeze(dim = 2)
y = x.squeeze()
y = x.squeeze(dim = 1)

# Algebra
ret = A.mm(B)
ret = A.mv(x)
x = x.t()

# GUP Usage
torch.cuda.is_available
x = x.cuda()
x = x.cpu()
if not args.disable_cuda and torch.cuda.is_available():
   args.device = torch.device(&quot;cuda&quot;)
else:
   args.device = torch.device(&quot;cpu&quot;)
net.to(device)
x = x.to(device)</code></pre>
</div>
<div id="deep-learning" class="section level2">
<h2>11.3 Deep Learning</h2>
<pre class="python"><code>import torch.nn as nn

nn.Linear(m, n)
nn.ConvXd(m, n, s)
nn.MaxPoolXd(s)
nn.BatchNormXd
nn.RNN
nn.LSTM
nn.GRU
nn.Dropout(p = 0.5, inplace = False)
nn.Dropout2d(p = 0.5, inplace = False)
nn.Embedding(num_embeddings, embedding_dim)

# Loss Function
nn.X

# Activation Function
nn.X

# Optimizers
opt = optim.x(model.parameters(), ...)
opt.step()
optim.X

# Learning rate scheduling
scheduler = optim.X(optimizer, ...)
scheduler.step()
optim.lr_scheduler.X</code></pre>
</div>
<div id="data-utilities" class="section level2">
<h2>11.4 Data Utilities</h2>
<pre class="python"><code># Dataset
Dataset
TensorDataset
Concat Dataset

# Dataloaders and DataSamplers
DataLoader(dataset, batch_size = 1, ...)
sampler.Sampler(dataset, ...)
sampler.XSampler where ...</code></pre>
</div>
</div>
