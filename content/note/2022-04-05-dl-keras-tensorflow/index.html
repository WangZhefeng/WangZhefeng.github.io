---
title: DL Keras & TensorFlow
author: 王哲峰
date: '2017-04-05'
slug: dl-keras
categories:
  - deeplearning
tags:
  - tool
output:
  blogdown::html_page:
    toc: true
    fig_width: 6
    dev: "svg"
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#需要掌握的内容">需要掌握的内容</a>
<ul>
<li><a href="#tensorflow-新手">Tensorflow 新手</a></li>
<li><a href="#tensorflow-专家">Tensorflow 专家:</a></li>
<li><a href="#tensorflow-库和扩展程序">TensorFlow 库和扩展程序</a></li>
</ul></li>
<li><a href="#tensorflow-keras-数据">Tensorflow &amp; Keras 数据</a>
<ul>
<li><a href="#tensorflow-datasets-库">1.TensorFlow Datasets 库</a></li>
<li><a href="#tensorflow-dataset-介绍">2.TensorFlow Dataset 介绍</a>
<ul>
<li><a href="#tf.data">2.1 tf.data</a></li>
<li><a href="#tensorflow_datasets">2.2 tensorflow_datasets</a></li>
</ul></li>
<li><a href="#tensorflow-数据集-api">3.TensorFlow 数据集 API</a></li>
<li><a href="#tensorflow-dataset-建立">3.TensorFlow Dataset 建立</a></li>
<li><a href="#tensorflow-内置-dataset">4.TensorFlow 内置 Dataset</a></li>
<li><a href="#tensorflow-dataset-预处理">5.TensorFlow Dataset 预处理</a>
<ul>
<li><a href="#数据集预处理-api-介绍">5.1 数据集预处理 API 介绍</a></li>
<li><a href="#数据集处理示例">4.2 数据集处理示例</a></li>
<li><a href="#图像">4.2 图像</a></li>
<li><a href="#文本">4.3 文本</a></li>
<li><a href="#csv">4.4 CSV</a></li>
<li><a href="#numpy">4.5 Numpy</a></li>
<li><a href="#pandas.dataframe">4.6 pandas.DataFrame</a></li>
<li><a href="#unicode">4.7 Unicode</a></li>
<li><a href="#tf.text">4.8 TF.Text</a></li>
<li><a href="#tfrecord">4.9 TFRecord</a></li>
<li><a href="#tf.io-的其他格式">4.10 tf.io 的其他格式</a></li>
<li><a href="#tf.tensorarray">4.11 tf.TensorArray</a></li>
</ul></li>
<li><a href="#数据输入流水线">6.数据输入流水线</a>
<ul>
<li><a href="#tf.data-1">6.1 tf.data</a></li>
<li><a href="#优化流水线性能">6.2 优化流水线性能</a></li>
<li><a href="#分析流水线性能">6.3 分析流水线性能</a></li>
</ul></li>
</ul></li>
<li><a href="#模型构建-todo-完善">模型构建 [TODO 完善]</a>
<ul>
<li><a href="#模型共有的方法和属性">1.模型共有的方法和属性</a></li>
<li><a href="#sequential-api">2.Sequential API</a></li>
<li><a href="#functional-api">3.Functional API</a></li>
<li><a href="#subclassing-api">4.Subclassing API</a></li>
<li><a href="#回调函数-callbacks">5.回调函数-Callbacks</a></li>
</ul></li>
<li><a href="#applicationstodo-完善">Applications[TODO 完善]</a>
<ul>
<li><a href="#目前可用模型">1.目前可用模型</a></li>
<li><a href="#示例">2.示例</a></li>
</ul></li>
<li><a href="#utils">Utils</a>
<ul>
<li><a href="#模型可视化">1.模型可视化</a>
<ul>
<li><a href="#plot_model">1.1 <code>plot_model()</code></a></li>
<li><a href="#model_to_dot">1.2 <code>model_to_dot()</code></a></li>
</ul></li>
<li><a href="#序列化工具serialization-utilities">2.序列化工具(Serialization utilities)</a>
<ul>
<li><a href="#customobjectscope-class">2.1 <code>CustomObjectScope</code> class</a></li>
<li><a href="#get_custom_objects">2.2 get_custom_objects()</a></li>
<li><a href="#register_keras_serializable">2.3 register_keras_serializable()</a></li>
<li><a href="#serialize_keras_object">2.4 serialize_keras_object()</a></li>
<li><a href="#daserialize_keras_object">2.5 daserialize_keras_object()</a></li>
</ul></li>
<li><a href="#python-numpy-utilities">3.Python &amp; Numpy utilities</a>
<ul>
<li><a href="#to_categorical">3.1 <code>to_categorical()</code></a></li>
<li><a href="#normalize">3.2 <code>normalize()</code></a></li>
<li><a href="#get_file">3.3 <code>get_file()</code></a></li>
<li><a href="#progbar-class">3.4 <code>Progbar</code> class</a></li>
<li><a href="#sequence-class">3.5 <code>Sequence</code> class</a></li>
</ul></li>
</ul></li>
<li><a href="#模型编译">模型编译</a>
<ul>
<li><a href="#损失函数">1.损失函数</a>
<ul>
<li><a href="#常用损失函数">1.1 常用损失函数</a></li>
<li><a href="#损失函数的使用compile-fit">1.2 损失函数的使用——compile() &amp; fit()</a></li>
<li><a href="#损失函数的使用单独使用">1.3 损失函数的使用——单独使用</a></li>
<li><a href="#创建自定义损失函数">1.4 创建自定义损失函数</a></li>
</ul></li>
<li><a href="#评价指标">2.评价指标</a>
<ul>
<li><a href="#metrics">2.1 metrics</a></li>
<li><a href="#accuracy-metrics">2.2 Accuracy metrics</a></li>
<li><a href="#probabilistic-metrics">2.3 Probabilistic metrics</a></li>
<li><a href="#regression-metrics">2.4 Regression metrics</a></li>
<li><a href="#classification-metrics-based-on-truefalse-positives-negatives">2.5 Classification metrics based on True/False positives &amp; negatives</a></li>
<li><a href="#image-segmentation-metrics">2.6 image segmentation metrics</a></li>
<li><a href="#hinge-metrics-for-maximum-margin-classification">2.7 Hinge metrics for “maximum-margin” Classification</a></li>
<li><a href="#评价指标的使用compile-fit">2.8 评价指标的使用——compile() &amp; fit()</a></li>
<li><a href="#评价指标的使用单独使用">2.9 评价指标的使用——单独使用</a></li>
<li><a href="#自定义评估指标">2.10 自定义评估指标</a></li>
</ul></li>
<li><a href="#优化器">3.优化器</a>
<ul>
<li><a href="#optimizers">3.1 Optimizers</a></li>
<li><a href="#optimizder-的使用方式">3.2 optimizder 的使用方式</a></li>
<li><a href="#optimizers-的共有参数">3.3 optimizers 的共有参数</a></li>
<li><a href="#优化器的使用">3.4 优化器的使用</a></li>
<li><a href="#优化算法核心-api">3.5 优化算法核心 API</a></li>
</ul></li>
</ul></li>
<li><a href="#keras-网络层">Keras 网络层</a>
<ul>
<li><a href="#自定义层">1.自定义层</a></li>
<li><a href="#keras-layers-共有的方法">1.Keras Layers 共有的方法:</a></li>
<li><a href="#keras-layers">2.Keras Layers</a></li>
<li><a href="#keras-layers-配置">3.Keras Layers 配置</a>
<ul>
<li><a href="#activation-function">3.1 Activation Function</a></li>
<li><a href="#可用的-activations">3.2 可用的 activations</a></li>
<li><a href="#keras-参数初始化initializers">3.3 Keras 参数初始化(Initializers)</a></li>
<li><a href="#keras-正则化regularizers">3.4 Keras 正则化(Regularizers)</a></li>
<li><a href="#keras-约束constraints">3.5 Keras 约束(Constraints)</a></li>
</ul></li>
</ul></li>
<li><a href="#tensorflow-tensorboard">TensorFlow TensorBoard</a>
<ul>
<li><a href="#实时查看参数变化情况">1.实时查看参数变化情况</a>
<ul>
<li><a href="#tensorboard-使用介绍">1.1 TensorBoard 使用介绍</a></li>
<li><a href="#tensorboard-代码框架">1.2 TensorBoard 代码框架</a></li>
</ul></li>
<li><a href="#查看-graph-和-profile-信息">2.查看 Graph 和 Profile 信息</a></li>
</ul></li>
<li><a href="#tensorflow-serving">TensorFLow Serving</a>
<ul>
<li><a href="#tensorflow-serving-安装">1.TensorFLow Serving 安装</a></li>
<li><a href="#tensorflow-serving-模型部署">2.TensorFLow Serving 模型部署</a></li>
<li><a href="#在客户端调用以-tensorflow-serving-部署的模型">3.在客户端调用以 TensorFLow Serving 部署的模型</a></li>
</ul></li>
<li><a href="#tensorflow-savemodel">TensorFlow SaveModel</a>
<ul>
<li><a href="#tf.train.checkpoint-变量的保存与恢复">1.tf.train.Checkpoint: 变量的保存与恢复</a>
<ul>
<li><a href="#tf.train.checkpoint-介绍">1.1 tf.train.Checkpoint 介绍</a></li>
<li><a href="#tf.train.checkpoint-代码框架">1.2 tf.train.Checkpoint 代码框架</a></li>
</ul></li>
<li><a href="#使用-savemodel-完整导出模型">2.使用 SaveModel 完整导出模型</a></li>
<li><a href="#keras-自有的模型导出格式">3.Keras 自有的模型导出格式</a></li>
</ul></li>
<li><a href="#tensorflow-performance">TensorFlow Performance</a>
<ul>
<li><a href="#使用-tf.function-提升性能">1.使用 tf.function 提升性能</a>
<ul>
<li><a href="#tf.funciton-图执行模式">1.1 <span class="citation">@tf.funciton</span>: 图执行模式</a></li>
<li><a href="#tf.function-基础使用方法">1.2 <span class="citation">@tf.function</span> 基础使用方法</a></li>
<li><a href="#tf.function-内在机制">1.3 <span class="citation">@tf.function</span> 内在机制</a></li>
<li><a href="#autograph-将-python-控制流转化为-tensorflow-计算图">1.4 AutoGraph: 将 Python 控制流转化为 TensorFlow 计算图</a></li>
<li><a href="#使用传统的-tf.session">1.5 使用传统的 tf.Session</a></li>
</ul></li>
<li><a href="#分析-tenforflow-的性能">2.分析 TenforFlow 的性能</a></li>
<li><a href="#图优化">3.图优化</a></li>
<li><a href="#混合精度">4.混合精度</a></li>
</ul></li>
<li><a href="#tensorflow-estimator">TensorFlow Estimator</a>
<ul>
<li><a href="#预创建的estimator">1.预创建的Estimator</a></li>
<li><a href="#自定义的estimator">2.自定义的Estimator</a></li>
<li><a href="#从-keras-模型创建-estimator">3.从 Keras 模型创建 Estimator</a></li>
</ul></li>
<li><a href="#todo">TODO</a>
<ul>
<li><a href="#多输入模型">1.多输入模型</a></li>
<li><a href="#多输出模型">2.多输出模型</a></li>
<li><a href="#经验总结">3.经验总结</a>
<ul>
<li><a href="#机器深度学习任务问题">3.1 机器、深度学习任务问题</a></li>
<li><a href="#回归问题">3.2 回归问题</a></li>
<li><a href="#二分类问题">3.3 二分类问题</a></li>
<li><a href="#数据预处理问题">3.4 数据预处理问题</a></li>
<li><a href="#样本量问题">3.5 样本量问题</a></li>
<li><a href="#网络结构选择问题">3.6 网络结构选择问题</a></li>
<li><a href="#优化器-1">3.7 优化器</a></li>
</ul></li>
</ul></li>
<li><a href="#tensorflow-keras-后端">TensorFlow Keras 后端</a>
<ul>
<li><a href="#什么是-keras-后端">1.什么是 Keras 后端？</a></li>
<li><a href="#从一个后端切换到另一个后端">2.从一个后端切换到另一个后端</a></li>
<li><a href="#keras.json-详细配置">3.keras.json 详细配置</a></li>
<li><a href="#backend-api">5.Backend API</a></li>
</ul></li>
<li><a href="#相关资料">相关资料</a></li>
</ul>
</div>

<div id="需要掌握的内容" class="section level1">
<h1>需要掌握的内容</h1>
<div id="tensorflow-新手" class="section level2">
<h2>Tensorflow 新手</h2>
<ul class="task-list">
<li><input type="checkbox" disabled="" checked="" />
快速入门</li>
<li><input type="checkbox" disabled="" />
Keras 机器学习基础知识</li>
<li><input type="checkbox" disabled="" />
加载和预处理数据</li>
</ul>
</div>
<div id="tensorflow-专家" class="section level2">
<h2>Tensorflow 专家:</h2>
<ul class="task-list">
<li><input type="checkbox" disabled="" checked="" />
快速入门</li>
<li><input type="checkbox" disabled="" />
自定义层、训练循环</li>
<li><input type="checkbox" disabled="" />
分布式训练</li>
</ul>
</div>
<div id="tensorflow-库和扩展程序" class="section level2">
<h2>TensorFlow 库和扩展程序</h2>
<ul class="task-list">
<li><input type="checkbox" disabled="" />
TensorBoard</li>
<li><input type="checkbox" disabled="" />
TensorFLow Hub</li>
<li><input type="checkbox" disabled="" />
数据集</li>
<li><input type="checkbox" disabled="" />
模型优化</li>
<li><input type="checkbox" disabled="" />
概率</li>
<li><input type="checkbox" disabled="" />
XLA</li>
<li><input type="checkbox" disabled="" />
TFX</li>
<li><input type="checkbox" disabled="" />
…</li>
</ul>
</div>
</div>
<div id="tensorflow-keras-数据" class="section level1">
<h1>Tensorflow &amp; Keras 数据</h1>
<div id="tensorflow-datasets-库" class="section level2">
<h2>1.TensorFlow Datasets 库</h2>
<ul>
<li>库安装:</li>
</ul>
<pre class="bash"><code>$ pip install tensorflow
$ pip install tensorflow-datasets</code></pre>
<ul>
<li>库导入:</li>
</ul>
<pre class="python"><code># tf.data, tf.data.Dataset
import tensorflow as tf

# tf.keras.datasets.&lt;dataset_name&gt;.load_data
from tensorflow.keras import datasets

# tfds.load
import tensorflow_datasets as tfds</code></pre>
</div>
<div id="tensorflow-dataset-介绍" class="section level2">
<h2>2.TensorFlow Dataset 介绍</h2>
<div id="tf.data" class="section level3">
<h3>2.1 tf.data</h3>
<p>TensorFlow 提供了 <code>tf.data</code> 模块, 它包括了一套灵活的数据集构建 API,
能够帮助快速、高效地构建数据输入的流水线, 尤其适用于数据量巨大的情景
<code>tf.data</code> API 在 TensorFlow 中引入了两个新的抽象类:</p>
<ul>
<li><code>tf.data.Dataset</code>
<ul>
<li><code>tf.data.Dataset</code>: 提供了对数据集的高层封装。<code>tf.data.Dataset</code> 由一系列可迭代访问的元素(element)组成,
其中每个元素包含一个或多个 <code>Tensor</code> 对象。<code>tf.data.Dataset</code> 可以通过两种方式来创建数据集:
<ul>
<li><strong>创建来源</strong>: 通过一个或多个 <code>tf.Tensor</code> 对象构建数据集
<ul>
<li><code>tf.data.Dataset.from_tensors()</code></li>
<li><code>tf.data.Dataset.from_tensor_slices()</code></li>
</ul></li>
<li><strong>应用转换</strong>: 通过一个或多个 <code>tf.data.Dataset</code> 对象构建数据集
<ul>
<li><code>tf.data.Dataset.map()</code></li>
<li><code>tf.data.Dataset.batch()</code></li>
</ul></li>
</ul></li>
</ul></li>
<li><code>tf.data.Iterator</code>
<ul>
<li><code>tf.data.Iterator</code>: 提供了从数据集中提取元素的主要方法</li>
<li><code>tf.data.Iterator.get_next()</code>
<ul>
<li>返回的操作会在执行时生成Dataset的下一个元素, 并且此操作通常当输入管道和模型之间的接口</li>
</ul></li>
<li><code>tf.data.Iterator.initializer</code>
<ul>
<li>使用不同的数据集重新初始化和参数化迭代器</li>
</ul></li>
</ul></li>
</ul>
</div>
<div id="tensorflow_datasets" class="section level3">
<h3>2.2 tensorflow_datasets</h3>
<p>TensorFlow Datasets(<code>tensorflow_datasets</code>) 是可用于 TensorFlow
或其他 Python 机器学习框架(例如 Jax) 的一系列数据集。
所有数据集都作为 <code>tf.data.Dataset</code> 提供, 实现易用且高性能的输入流水线。</p>
</div>
</div>
<div id="tensorflow-数据集-api" class="section level2">
<h2>3.TensorFlow 数据集 API</h2>
<ul>
<li><code>tf.data</code>
<ul>
<li><code>tf.data.Dataset</code></li>
<li><code>tf.data.Dataset.from_tensor_slices</code></li>
</ul></li>
<li><code>tensorflow_datasets</code>
<ul>
<li><code>tensorflow_datasets.load(data, split, shuffle_files, as_supervised)</code></li>
</ul></li>
<li><code>tf.keras.datasets</code>
<ul>
<li><code>tf.keras.datasets.mnist.load_data()</code></li>
</ul></li>
</ul>
</div>
<div id="tensorflow-dataset-建立" class="section level2">
<h2>3.TensorFlow Dataset 建立</h2>
<ol style="list-style-type: decimal">
<li><p>建立 <code>tf.data.Dataset</code> 的最基本的方法是使用 <code>tf.data.Dataset.from_tensor_slices()</code></p>
<ul>
<li>适用于数据量较小(能够将数据全部装进内存)的情况</li>
<li>如果数据集中的所有元素通过张量的第 0 维拼接成一个大的张量</li>
</ul></li>
</ol>
<pre class="python"><code>import tensorflow as tf
import numpy as np

X = tf.constant([2013, 2014, 2015, 2016, 2017])
Y = tf.constant([12000, 14000, 15000, 16500, 17500])

dataset = tf.data.Dataset.from_tensor_slices((X, Y))
for x, y in dataset:
    print(x.numpy(), y.numpy())</code></pre>
<p>2.使用 <code>tf.data.Dataset.from_tensor_slices()</code>、<code>tf.keras.datasets.mnist.load_data()</code></p>
<pre class="python"><code>import tensorflow as tf
import matplotlib.pyplot as plt

(train_data, train_label), (_, _) = tf.keras.datasets.mnist.load_data()
train_data = np.expand_dim(train_data.astype(np.float32) / 255, axis = -1)
mnist_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_label))

for image, label in mnist_dataset.take(1):
    plt.title(label.numpy())
    plt.imshow(image.numpy())
    plt.show()</code></pre>
<p>3.TensorFlow Datasets 提供了一系列可以和 Tensorflow 配合使用的数据集, 它负责下载和准备数据, 以及构建 <code>tf.data.Dataset</code></p>
<ul>
<li>每一个数据集(dataset) 都实现了抽象基类 tfds.core.DatasetBuilder 来构建</li>
</ul>
<pre class="python"><code>
   import tensorflow_datasets as tfds

   # 构建 tf.data.Dataset
   dataset1 = tfds.load(&quot;mnist&quot;, split = &quot;train&quot;, shuffle_files = True)
   dataset2 = tfds.load(&quot;mnist&quot;, split = tfds.Split.TRAIN, as_supervised = True)

   # 构建输入数据 Pipeline
   dataset1 = dataset1 \
      .shuffle(1024) \
      .batch(32) \
      .prefetch(tf.data.experimential.AUTOTUNE)
   
   for example in dataset1.take(1):
      image, label = example[&quot;image&quot;], example[&quot;label&quot;]</code></pre>
<p>.. note::</p>
<ul>
<li>对于特别巨大而无法完整载入内存的数据集, 可以先将数据集处理为 <code>TFRecord</code> 格式,
然后使用 <code>tf.data.TFRecordDataset()</code> 进行载入</li>
</ul>
</div>
<div id="tensorflow-内置-dataset" class="section level2">
<h2>4.TensorFlow 内置 Dataset</h2>
<ul>
<li><p>TensorFlow Datasets 提供了一系列可以和 Tensorflow 配合使用的数据集, 它负责下载和准备数据, 以及构建 <code>tf.data.Dataset</code></p></li>
<li><p>每一个数据集(dataset) 都实现了抽象基类 tfds.core.DatasetBuilder 来构建</p></li>
<li><p>官方文档</p></li>
<li><p><a href="https://github.com/tensorflow/datasets" class="uri">https://github.com/tensorflow/datasets</a></p></li>
<li><p><a href="https://www.tensorflow.org/datasets/overview" class="uri">https://www.tensorflow.org/datasets/overview</a></p></li>
<li><p><a href="https://www.tensorflow.org/datasets/catalog/overview#all_datasets" class="uri">https://www.tensorflow.org/datasets/catalog/overview#all_datasets</a></p></li>
<li><p><a href="https://www.tensorflow.org/datasets/api_docs/python/tfds" class="uri">https://www.tensorflow.org/datasets/api_docs/python/tfds</a></p></li>
<li><p><a href="https://blog.tensorflow.org/2019/02/introducing-tensorflow-datasets.html?hl=zh-CN" class="uri">https://blog.tensorflow.org/2019/02/introducing-tensorflow-datasets.html?hl=zh-CN</a></p></li>
</ul>
<ol style="list-style-type: decimal">
<li>查看可用的数据集</li>
</ol>
<pre class="python"><code>import tensorflow as tf
import tensorflow_datasets as tfds

# 所有可用的数据集
print(tfds.list_builders()) 

[&#39;abstract_reasoning&#39;, &#39;aflw2k3d&#39;, &#39;amazon_us_reviews&#39;, 
&#39;bair_robot_pushing_small&#39;, &#39;bigearthnet&#39;, &#39;binarized_mnist&#39;, &#39;binary_alpha_digits&#39;, 
&#39;caltech101&#39;, &#39;caltech_birds2010&#39;, &#39;caltech_birds2011&#39;, &#39;cats_vs_dogs&#39;, &#39;celeb_a&#39;, &#39;celeb_a_hq&#39;, &#39;chexpert&#39;, &#39;cifar10&#39;, &#39;cifar100&#39;, &#39;cifar10_corrupted&#39;, &#39;clevr&#39;, &#39;cnn_dailymail&#39;, &#39;coco&#39;, &#39;coco2014&#39;, &#39;coil100&#39;, &#39;colorectal_histology&#39;, &#39;colorectal_histology_large&#39;, &#39;curated_breast_imaging_ddsm&#39;, &#39;cycle_gan&#39;, 
&#39;deep_weeds&#39;, &#39;definite_pronoun_resolution&#39;, &#39;diabetic_retinopathy_detection&#39;, &#39;downsampled_imagenet&#39;, &#39;dsprites&#39;, &#39;dtd&#39;, &#39;dummy_dataset_shared_generator&#39;, &#39;dummy_mnist&#39;, 
&#39;emnist&#39;, &#39;eurosat&#39;, 
&#39;fashion_mnist&#39;, &#39;flores&#39;, &#39;food101&#39;, 
&#39;gap&#39;, &#39;glue&#39;, &#39;groove&#39;, 
&#39;higgs&#39;, &#39;horses_or_humans&#39;, 
&#39;image_label_folder&#39;, &#39;imagenet2012&#39;, &#39;imagenet2012_corrupted&#39;, &#39;imdb_reviews&#39;, &#39;iris&#39;, &#39;kitti&#39;, 
&#39;kmnist&#39;, 
&#39;lfw&#39;, &#39;lm1b&#39;, &#39;lsun&#39;, 
&#39;mnist&#39;, &#39;mnist_corrupted&#39;, &#39;moving_mnist&#39;, &#39;multi_nli&#39;, 
&#39;nsynth&#39;, 
&#39;omniglot&#39;, &#39;open_images_v4&#39;, &#39;oxford_flowers102&#39;, &#39;oxford_iiit_pet&#39;, 
&#39;para_crawl&#39;, &#39;patch_camelyon&#39;, &#39;pet_finder&#39;, &#39;quickdraw_bitmap&#39;, 
&#39;resisc45&#39;, &#39;rock_paper_scissors&#39;, &#39;rock_you&#39;, 
&#39;scene_parse150&#39;, &#39;shapes3d&#39;, &#39;smallnorb&#39;, &#39;snli&#39;, &#39;so2sat&#39;, &#39;squad&#39;, &#39;stanford_dogs&#39;, &#39;stanford_online_products&#39;, &#39;starcraft_video&#39;, &#39;sun397&#39;, &#39;super_glue&#39;, &#39;svhn_cropped&#39;, 
&#39;ted_hrlr_translate&#39;, &#39;ted_multi_translate&#39;, &#39;tf_flowers&#39;, &#39;titanic&#39;, &#39;trivia_qa&#39;, 
&#39;uc_merced&#39;, &#39;ucf101&#39;, 
&#39;visual_domain_decathlon&#39;, &#39;voc2007&#39;, 
&#39;wikipedia&#39;, &#39;wmt14_translate&#39;, &#39;wmt15_translate&#39;, &#39;wmt16_translate&#39;, &#39;wmt17_translate&#39;, &#39;wmt18_translate&#39;, &#39;wmt19_translate&#39;, &#39;wmt_t2t_translate&#39;, &#39;wmt_translate&#39;, 
&#39;xnli&#39;]</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>内置数据集分类</li>
</ol>
<ul>
<li><p>Audio</p>
<ul>
<li>groove</li>
<li>nsynth</li>
</ul></li>
<li><p>Image</p>
<ul>
<li>abstract_reasoning</li>
<li>aflw2k3d</li>
<li>bigearthnet</li>
<li>binarized_mnist</li>
<li>binaryalphadigits</li>
<li>caltech101</li>
<li>caltech_birds2010</li>
<li>caltech_birds2011</li>
<li>catsvsdogs</li>
<li>celeb_a</li>
<li>celebahq</li>
<li>cifar10</li>
<li>cifar100</li>
<li>cifar10_corrupted</li>
<li>clevr</li>
<li>coco</li>
<li>coco2014</li>
<li>coil100</li>
<li>colorectal_histology</li>
<li>colorectalhistologylarge</li>
<li>curatedbreastimaging_ddsm</li>
<li>cycle_gan</li>
<li>deep_weeds</li>
<li>diabeticretinopathydetection</li>
<li>downsampled_imagenet</li>
<li>dsprites</li>
<li>dtd</li>
<li>emnist</li>
<li>eurosat</li>
<li>fashion_mnist</li>
<li>food101</li>
<li>horsesorhumans</li>
<li>imagelabelfolder</li>
<li>imagenet2012</li>
<li>imagenet2012_corrupted</li>
<li>kitti</li>
<li>kmnist</li>
<li>lfw</li>
<li>lsun</li>
<li>mnist</li>
<li>mnist_corrupted</li>
<li>omniglot</li>
<li>openimagesv4</li>
<li>oxford_flowers102</li>
<li>oxfordiiitpet</li>
<li>patch_camelyon</li>
<li>pet_finder</li>
<li>quickdraw_bitmap</li>
<li>resisc45</li>
<li>rockpaperscissors</li>
<li>scene_parse150</li>
<li>shapes3d</li>
<li>smallnorb</li>
<li>so2sat</li>
<li>stanford_dogs</li>
<li>stanfordonlineproducts</li>
<li>sun397</li>
<li>svhn_cropped</li>
<li>tf_flowers</li>
<li>uc_merced</li>
<li>visualdomaindecathlon</li>
<li>voc2007</li>
</ul></li>
<li><p>Structured</p>
<ul>
<li>amazonusreviews</li>
<li>higgs</li>
<li>iris</li>
<li>rock_you</li>
<li>titanic</li>
</ul></li>
<li><p>Text</p>
<ul>
<li>cnn_dailymail</li>
<li>definitepronounresolution</li>
<li>gap</li>
<li>glue</li>
<li>imdb_reviews</li>
<li>lm1b</li>
<li>multi_nli</li>
<li>snli</li>
<li>squad</li>
<li>super_glue</li>
<li>trivia_qa</li>
<li>wikipedia</li>
<li>xnli</li>
</ul></li>
<li><p>Translate</p>
<ul>
<li>flores</li>
<li>para_crawl</li>
<li>tedhrlrtranslate</li>
<li>tedmultitranslate</li>
<li>wmt14_translate</li>
<li>wmt15_translate</li>
<li>wmt16_translate</li>
<li>wmt17_translate</li>
<li>wmt18_translate</li>
<li>wmt19_translate</li>
<li>wmtt2ttranslate</li>
</ul></li>
<li><p>Video</p>
<ul>
<li>bairrobotpushing_small</li>
<li>moving_mnist</li>
<li>starcraft_video</li>
<li>ucf101</li>
</ul></li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>构建并加载内置数据集</li>
</ol>
<ul>
<li><code>tfds.load</code> 是构建并加载 <code>tf.data.Dataset</code> 最简单的方式</li>
<li><code>tf.data.Dataset</code> 是构建输入流水线的标准 TensorFlow 接口</li>
<li>加载数据集时, 默认使用规范的版本, 但是可以指定要使用的数据集的主版本, 并在结果中表明使用了哪个版本的数据集</li>
</ul>
<p>示例1:</p>
<pre class="python"><code>mnist_train = tfds.load(&quot;mnist&quot;, split = &quot;train&quot;, download = False, data_dir = &quot;~/.tensorflow_datasets/&quot;)
assert isinstance(mnist_train, tf.data.Dataset)
print(mnist_train)</code></pre>
<p>示例2: 版本控制</p>
<pre class="python"><code>mnist = tfds.load(&quot;mnist:1.*.*&quot;)</code></pre>
<ol start="4" style="list-style-type: decimal">
<li>内置数据集特征字典</li>
</ol>
<ul>
<li>所有 <code>tensorflow_datasets, tfds</code> 数据集都包含将特征名称映射到 Tensor 值的特征字典。典型的数据集将具有 2 个键:
<ul>
<li><code>"image"</code></li>
<li><code>"label"</code></li>
</ul></li>
</ul>
<p>示例:</p>
<pre class="python"><code>mnist_train = tfds.load(&quot;mnist&quot;, split = &quot;train&quot;, download = False, data_dir = &quot;~/.tensorflow_datasets/&quot;)
for mnist_example in mnist_train.take(1):
   image, label = mnist_example[&quot;image&quot;], mnist_example[&quot;label&quot;]
   plt.imshow(
      image.numpy()[:, :, 0].astype(np.float32),
      cma = plt.get_cmap(&quot;gray&quot;)
   )
   print(&quot;Label: %d&quot; % label.numpy())
   plt.show()</code></pre>
<ol start="5" style="list-style-type: decimal">
<li>DatasetBuilder</li>
</ol>
<ul>
<li><code>tensorflow_datasets.load</code> 实际上是一个基于 <code>DatasetBuilder</code> 的简单方便的包装器</li>
</ul>
<p>示例:</p>
<pre class="python"><code>mnist_builder = tfds.builder(&quot;mnist&quot;)
mnsit_builder.download_and_prepare()
mnist_train = mnist_builder.as_dataset(split = &quot;train&quot;)
mnist_train</code></pre>
<ol start="6" style="list-style-type: decimal">
<li>内置数据集输入流水线</li>
</ol>
<ul>
<li>一旦有了 <code>tf.data.Dataset</code> 对象, 就可以使用 <code>tf.data</code> 接口定义适合模型训练的输入流水线的其余部分.</li>
</ul>
<p>示例:</p>
<pre class="python"><code>mnist_train = mnist_train.repeat().shuffle(1024).batch(32)

# prefetch 将使输入流水线可以在模型训练时一步获取批处理
mnist_train = mnist_train \
               .repeat() \
               .shuffle(1024) \
               .batch(32) \
               .prefetch(tf.data.experimental.AUTOTUNE)</code></pre>
<ol start="7" style="list-style-type: decimal">
<li>内置数据集信息</li>
</ol>
<p>示例:</p>
<pre class="python"><code># method 1
info = mnist_builder.info
print(info)
print(info.features)
print(info.features[&quot;label&quot;].num_classes)
print(info.features[&quot;label&quot;].names)

# method 2
mnist_test, info = tfds.load(&quot;mnist&quot;, split = &quot;test&quot;, with_info = True)
print(info)</code></pre>
<ol start="8" style="list-style-type: decimal">
<li>内置数据集可视化</li>
</ol>
<p>示例:</p>
<pre class="python"><code>fig = tfds.show_examples(info, mnist_test)</code></pre>
</div>
<div id="tensorflow-dataset-预处理" class="section level2">
<h2>5.TensorFlow Dataset 预处理</h2>
<div id="数据集预处理-api-介绍" class="section level3">
<h3>5.1 数据集预处理 API 介绍</h3>
<ul>
<li>Sequence Preprocessing
<ul>
<li>TimeseriesGenerator</li>
<li>pad_sequences</li>
<li>skipgrams</li>
<li>makesamplingtable</li>
</ul></li>
<li>Text Preprocessing
<ul>
<li>Tokenizer</li>
<li>hashing_trick
<ul>
<li>将文本转换为固定大小的散列空间中的索引序列</li>
</ul></li>
<li>one_hot
<ul>
<li>One-hot将文本编码为大小为n的单词索引列表</li>
</ul></li>
<li>texttoword_sequence
<ul>
<li>将文本转换为单词(或标记)序列</li>
</ul></li>
</ul></li>
<li>Image Preprocessing
<ul>
<li><code>class</code> ImageDataGenerator</li>
<li><code>method</code>
<ul>
<li>.apply_transform()</li>
<li>.fit ()</li>
<li>.flow()</li>
<li>采用数据和标签数组, 生成批量增强数据</li>
<li>.flowfromdataframe()</li>
<li>获取数据帧和目录路径, 并生成批量的扩充/规范化数据</li>
<li>.flowfromdirectory()</li>
<li>获取目录的路径并生成批量的增强数据</li>
<li>.getrandomtransform()</li>
<li>为转换生成随机参数</li>
<li>.random_transform()</li>
<li>随机转换</li>
<li>.standardize()</li>
<li>标准化</li>
</ul></li>
</ul></li>
<li><code>tf.data.Dataset</code> 类提供了多种数据集预处理方法:
<ul>
<li><code>tf.data.Dataset.map(f)</code>:
<ul>
<li>对数据集中的每个元素应用函数 <code>f</code>, 得到一个新的数据集</li>
<li>结合 <code>tf.io</code> 对文件进行读写和解码</li>
<li>结合 <code>tf.image</code> 进行图像处理</li>
</ul></li>
<li><code>tf.data.Dataset.shuffle(buffer_size)</code>:
<ul>
<li>将数据集打乱</li>
<li>设定一个固定大小的缓冲区(buffer), 取出前 buffer_size 个元素放入, 并从缓冲区中随机采样, 采样后的数据用后续数据替换</li>
</ul></li>
<li><code>tf.data.Dataset.batch(batch_size)</code>:
<ul>
<li>将数据集分成批次</li>
<li>对每 <code>batch_size</code> 个元素, 使用 <code>tf.stack()</code> 在第 0 维合并, 成为一个元素</li>
</ul></li>
<li><code>tf.data.Dataset.repeat()</code>:
<ul>
<li>重复数据集的元素</li>
</ul></li>
<li><code>tf.data.Dataset.reduce()</code>:
<ul>
<li>与 Map 相对的聚合操作</li>
</ul></li>
<li><code>tf.data.Dataset.take()</code>:
<ul>
<li>截取数据集中的前若干个元素</li>
</ul></li>
<li><code>tf.data.Dataset.prefetch()</code>:
<ul>
<li>并行化策略提高训练流程效率</li>
<li>获取与使用 <code>tf.data.Dataset</code> 数据集元素</li>
</ul></li>
<li><code>tf.data.Dataset</code> 是一个 Python 的可迭代对象</li>
</ul></li>
</ul>
</div>
<div id="数据集处理示例" class="section level3">
<h3>4.2 数据集处理示例</h3>
<ul>
<li>(1)使用 <code>tf.data.Dataset.map()</code> 将所有图片旋转 90 度</li>
</ul>
<pre class="python"><code>import tensorflow as tf

# data preprocessing function
def rot90(image, label):
   image = tf.image.rot90(image)
   return image, label

# data
mnist_dataset = tf.keras.datasets.mnist.load_data()

# data preprocessing
mnist_dataset = mnist_dataset.map(rot90)

# data visual
for image, label in mnist_dataset:
   plt.title(label.numpy())
   plt.imshow(image.numpy()[:, :, 0])
   plt.show()</code></pre>
<ul>
<li>(2)使用 <code>tf.data.Dataset.batch()</code> 将数据集划分为批次, 每个批次的大小为 4</li>
</ul>
<pre class="python"><code>import tensorflow as tf

# data
mnist_dataset = tf.keras.datasets.mnist.load_data()

# data preprocessing
mnist_dataset = mnist_dataset.batch(4)

# data visual
for images, labels in mnist_dataset: # image: [4, 28, 28, 1], labels: [4]
   fig, axs = plt.subplots(1, 4)
   for i in range(4):
      axs[i].set_title(label.numpy()[i])
      axs[i].imshow(images.numpy()[i, :, :, 0])
   plt.show()</code></pre>
<ul>
<li>(3)使用 <code>tf.data.Dataset.shuffle()</code> 将数据打散后再设置批次, 缓存大小设置为 10000</li>
</ul>
<pre class="python"><code>import tensorflow as tf

# data
mnist_dataset = tf.keras.datasets.mnist.load_data()

# data preprocessing
mnist_dataset = mnist_dataset.shuffle(buffer_size = 10000).batch(4)

# data visual
for i in range(2):
   for images, labels in mnist_dataset: # image: [4, 28, 28, 1], labels: [4]
      fig, axs = plt.subplots(1, 4)
      for i in range(4):
         axs[i].set_title(label.numpy()[i])
         axs[i].imshow(images.numpy()[i, :, :, 0])
      plt.show()</code></pre>
<p>.. note::</p>
<ul>
<li><p>一般而言, 若数据集的顺序分布较为随机, 则缓冲区的大小可较小, 否则需要设置较大的缓冲区</p></li>
<li><p>(4)使用 <code>tf.data.Dataset.prefetch()</code> 并行化策略提高训练流程效率</p></li>
<li><p>常规的训练流程</p></li>
<li><p>当训练模型时, 希望充分利用计算资源, 减少 CPU/GPU 的空载时间, 然而, 有时数据集的准备处理非常耗时,
使得在每进行一次训练前都需要花费大量的时间准备带训练的数据, GPU 只能空载等待数据, 造成了计算资源的浪费</p></li>
<li><p>使用 <code>tf.data.Dataset.prefetch()</code> 方法进行数据预加载后的训练流程</p></li>
<li><p><code>tf.data.Dataset.prefetch()</code> 可以让数据集对象 <code>Dataset</code> 在训练时预先取出若干个元素,
使得在 GPU 训练的同时 CPU 可以准备数据, 从而提升训练流程的效率</p></li>
</ul>
<pre class="python"><code>import tensorflow as tf

# data preprocessing function
def rot90(image, label):
   image = tf.image.rot90(image)
   return image, label

# data
mnist_dataset = tf.keras.datasets.mnist.load_data()

# data preprocessing
# 开启数据预加载功能
mnist_dataset = mnist_dataset.prefetch(buffer_size = tf.data.experimental.AUTOTUNE)
# 利用多 GPU 资源, 并行化地对数据进行变换
mnist_dataset = mnist_dataset.map(map_func = rot90, num_parallel_calls = 2)
mnist_dataset = mnist_dataset.map(map_func = rot90, num_parallel_calls = tf.data.experimental.AUTOTUNE)</code></pre>
<ul>
<li><p>(5)获取与使用 <code>tf.data.Dataset</code> 数据集元素</p></li>
<li><p>构建好数据并预处理后, 需要从中迭代获取数据用于训练</p></li>
</ul>
<pre class="python"><code>dataset = tf.data.Dataset.from_tensor_slices((A, B, C, ...))
for a, b, c ... in dataset:
   pass</code></pre>
<pre class="python"><code>dataset = tf.data.Dataset.from_tensor_slices((A, B, C, ...))
it = iter(dataset)
a_0, b_0, c_0, ... = next(it)
a_1, b_1, c_1, ... = next(it)</code></pre>
</div>
<div id="图像" class="section level3">
<h3>4.2 图像</h3>
<p>keras.preprocessing.imgae.ImageDataGenerator 通过实时数据增强生成批量张量图像数据</p>
<pre class="python"><code>keras.preprocessing.image.ImageDataGenerator(featurewise_center = False, # 将数据的特征均值设定为0
      samplewise_center = False,  # 将数据的样本均值设定为0
      featurewise_std_normalization = False, # 是否将特征除以特征的标准差进行归一化
      samplewise_std_normalization = False,  # 是否将样本除以样本的标准差进行归一化
      zca_whitening = False, # 是否进行 ZCA 白化
      zca_epsilon = 1e-06,   # 进行 ZCA 白化的epsilon参数
      rotation_range = 0,      # 随机旋转的角度范围
      width_shift_range = 0.0, # 宽度调整的范围
      height_shift_range = 0.0,# 高度调整的范围
      brightness_range = None, # 亮度范围 
      shear_range = 0.0,         # 剪切范围
      zoom_range = 0.0,          # 缩放范围
      channel_shift_range = 0.0, # 通道调整范围
      fill_mode = &#39;nearest&#39;,     # 填充边界之外点的方式:
      cval=0.0, 
      horizontal_flip=False,  # 水平翻转
      vertical_flip=False,    # 垂直翻转
      rescale=None,           # 
      preprocessing_function=None, 
      data_format=None, 
      validation_split=0.0,
      dtype=None)</code></pre>
<p><strong>用法:</strong></p>
<pre class="python"><code>from keras.datasets import cifar10
from keras import utils
from keras.preprocessing.image import ImageDataGenerator

# model training parameters
num_classes = 10
data_augmentation = True
batch_size = 32
epochs = 20

# data
(x_train, y_train), (x_test, y_test) = cifar10.load_data()
x_train = x_train.astype(&quot;float32&quot;)
x_test = x_test.astype(&quot;float32&quot;)
x_train /= 255
x_test /= 255
y_train = utils.to_categorical(y_train, num_classes = num_classes)
y_test = utils.to_categorical(y_test, num_classes = num_classes)

# model training
if not data_augmentation:
      print(&quot;Not using data augmentation.&quot;)
      model.fit(x_train, y_train,
               batch_size = batch_size,
               epochs = epochs,
               validation_data = (x_test, y_test),
               shuffle = True)
else:
      print(&quot;Using real-time data augmentation.&quot;)
      # This will do preprocessing and realtime data augmentation:
      datagen = ImageDataGenerator(
         featurewise_center = False,
         samplewise_center = False,
         featurewise_std_normalization = False,
         samplewise_std_normalization = False,
         zca_whitening = False,
         zca_epsilon = 1e-6,
         rotation_range = 0,
         width_shift_range = 0.1,
         height_shift_range = 0.1,
         shear_range = 0.,
         zoom_range = 0.,
         channel_shift_range = 0,
         fill_mode = &quot;nearest&quot;,
         cval = 0.,
         horizontal_flip = True,
         vertical_flip = False,
         rescale = None,
         preprocessing_function = None,
         data_format = None,
         validation_split = 0.0
      )
      datagen.fit(x_train)
      model.fit_generator(datagen.flow(x_train,
                                       y_train,
                                       batch_size = batch_size,
                                       epochs = epochs,
                                       validation_data = (x_test, y_test),
                                       workers = 4))</code></pre>
<pre class="python"><code>from keras.datasets import cifar10
from keras import utils


# data
(x_train, y_train), (x_test, y_test) = cifar10.load_data()
x_train = x_train.astype(&quot;float32&quot;)
x_test = x_test.astype(&quot;float32&quot;)
x_train /= 255
x_test /= 255
y_train = utils.to_categorical(y_train, num_classes = num_classes)
y_test = utils.to_categorical(y_test, num_classes = num_classes)


# model training parameters
batch_size = 32
epochs = 20
num_classes = 10
data_augmentation = True

# model training
datagen = ImageDataGenerator(featurewise_center = True,
                              featurewise_std_normalization = True,
                              rotation_range = 20,
                              width_shift_range = 0.2,
                              height_shift_range = 0.2,
                              horizontal_flip = True)

for e in range(epochs):
      print(&quot;Epoch&quot;, e)
      batches = 0
      for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size = batch_size):
         model.fit(x_batchd, y_batch)
         batches += 1
         if batches &gt;= len(x_train) / 32:
            break</code></pre>
<pre class="python"><code>train_datagen = ImageDataGenerator(rescale = 1. / 255,
                                    shear_range = 0.2,
                                    zoom_range = 0.2,
                                    horizontal_flip = True)
test_datagen = ImageDataGenerator(rescale = 1.0 / 255)

train_generator = train_datagen \
      .flow_from_directory(&quot;data/train&quot;,
                           target_size = (150, 150),
                           batch_size = 32,
                           class_mode = &quot;binary&quot;)
validation_generator = test_datagen \
      .flow_from_directory(&quot;data/validation&quot;,
                           target_size = (150, 150),
                           batch_size = 32,
                           class_mode = &quot;binary&quot;)

model.fit_generator(train_generator,
                     steps_per_epoch = 2000,
                     epochs = 50,
                     validation_data = validation_generator,
                     validation_steps = 800)</code></pre>
<pre class="python"><code># we create two instances with the same arguments
data_gen_args = dict(featurewise_center=True,
                     featurewise_std_normalization=True,
                     rotation_range=90,
                     width_shift_range=0.1,
                     height_shift_range=0.1,
                     zoom_range=0.2)
image_datagen = ImageDataGenerator(**data_gen_args)
mask_datagen = ImageDataGenerator(**data_gen_args)

# Provide the same seed and keyword arguments to the fit and flow methods
seed = 1
image_datagen.fit(images, augment=True, seed=seed)
mask_datagen.fit(masks, augment=True, seed=seed)

image_generator = image_datagen.flow_from_directory(
      &#39;data/images&#39;,
      class_mode=None,
      seed=seed)

mask_generator = mask_datagen.flow_from_directory(
      &#39;data/masks&#39;,
      class_mode=None,
      seed=seed)

# combine generators into one which yields image and masks
train_generator = zip(image_generator, mask_generator)

model.fit_generator(
      train_generator,
      steps_per_epoch=2000,
      epochs=50)</code></pre>
</div>
<div id="文本" class="section level3">
<h3>4.3 文本</h3>
<p><code>tf.data.TextLineDataset</code> 通常被用来以文本文件构建数据集(原文件中的一行为一个样本)。
这适用于大多数的基于行的文本数据(例如, 诗歌或错误日志)。</p>
<ul>
<li>删除文档的页眉、页脚、行号、章节标题</li>
</ul>
<pre class="python"><code>import os
import tensorflow as tf
import tensorflow_datasets as tfds

DIRECTORY_URL = &quot;https://storage.googleapis.com/download.tensorflow.org/data/illiad/&quot;
FILE_NAMES = [&quot;cowper.txt&quot;, &quot;derby.txt&quot;, &quot;butler.txt&quot;]
for name in FILE_NAMES:
   text_dir = tf.keras.utils.get_file(name, origin = DIRECTORY_URL + name)

def labeler(example, index):
   return example, tf.cast(index, tf.int64)

parent_dir = os.path.dirname(text_dir)
labeled_data_sets = []
for i, file_name in enumerate(FILE_NAMES):
   lines_dataset = tf.data.TextLineDataset(os.path.join(parent_dir, file_name))
   labeled_dataset = lines_dataset.map(lambda ex: labeler(ex, i))
   labeled_data_sets.append(labeled_dataset)

BUFFER_SIZE = 50000
BATCH_SIZE = 64
TAKE_SIZE = 5000

all_labeled_data = labeled_data_sets[0]
for labeled_dataset in labeled_data_sets[1:]:
   all_labeled_data = all_labeled_data.concatenate(labeled_dataset)

all_labeled_data = all_labeled_data.shuffle(BUFFER_SIZE, reshuffle_each_iteration = False)

for ex in all_labeled_data.take(5):
   print(ex)</code></pre>
</div>
<div id="csv" class="section level3">
<h3>4.4 CSV</h3>
</div>
<div id="numpy" class="section level3">
<h3>4.5 Numpy</h3>
</div>
<div id="pandas.dataframe" class="section level3">
<h3>4.6 pandas.DataFrame</h3>
</div>
<div id="unicode" class="section level3">
<h3>4.7 Unicode</h3>
</div>
<div id="tf.text" class="section level3">
<h3>4.8 TF.Text</h3>
<pre class="python"><code>from keras.preprocessing.text import Tokenizer
from keras.preprocessing.text import hashing_trick
from keras.preprocessing.text import one_hot
from keras.preprocessing.text import text_to_word_sequence</code></pre>
</div>
<div id="tfrecord" class="section level3">
<h3>4.9 TFRecord</h3>
<ol style="list-style-type: decimal">
<li>TFRecord 数据文件介绍</li>
</ol>
<p>TFRecord 是 TensorFlow 中的数据集存储格式。当将数据集整理成 TFRecord 格式后,
TensorFlow 就可以高效地读取和处理这些数据集了。从而帮助更高效地进行大规模模型训练。</p>
<p>TFRecord 可以理解为一系列序列化的 <code>tf.train.Example</code> 元素所组成的列表文件,
而每一个 <code>tf.train.Example</code> 又由若干个 <code>tf.train.Feature</code> 的字典组成:</p>
<pre class="python"><code>
   # dataset.tfrecords
   [
      {  # example 1 (tf.train.Example)
         &#39;feature_1&#39;: tf.train.Feature,
         ...
         &#39;feature_k&#39;: tf.train.Feature,
      },
      ...
      {  # example N (tf.train.Example)
         &#39;feature_1&#39;: tf.train.Feature,
         ...
         &#39;feature_k&#39;: tf.train.Feature,
      }, 
   ]</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>TFRecord 文件保存</li>
</ol>
<ul>
<li>TFRecord 文件保存步骤</li>
</ul>
<p>为了将形式各样的数据集整理为 TFRecord 格式, 可以对数据集中的每个元素进行以下步骤:</p>
<ul>
<li><ol style="list-style-type: decimal">
<li>读取该数据元素到内存</li>
</ol></li>
<li><ol start="2" style="list-style-type: decimal">
<li>将该元素转换为 <code>tf.train.Example</code> 对象</li>
</ol>
<ul>
<li>每个 <code>tf.train.Example</code> 对象由若干个 <code>tf.train.Feature</code> 的字典组成, 因此需要先建立 Feature 的子典</li>
</ul></li>
<li><ol start="3" style="list-style-type: decimal">
<li>将 <code>tf.train.Example</code> 对象序列化为字符串, 并通过一个预先定义的 <code>tf.io.TFRecordWriter</code> 写入 <code>TFRecord</code> 文件</li>
</ol></li>
<li>TFRecord 文件保存示例</li>
</ul>
<pre class="python"><code>import tensorflow as tf
import os

# root
root_dir = &quot;/Users/zfwang/project/machinelearning/deeplearning&quot;
# project
project_path = os.path.join(root_dir, &quot;deeplearning/src/tensorflow_src&quot;)
# model save
models_path = os.path.join(project_path, &quot;save&quot;)
# data
cats_and_dogs_dir = os.path.join(root_dir, &quot;datasets/cats_vs_dogs&quot;)
data_dir = os.path.join(root_dir, &quot;datasets/cats_vs_dogs/cats_and_dogs_small&quot;)
# train data
train_dir = os.path.join(data_dir, &quot;train&quot;)
train_cats_dir = os.path.join(train_dir, &quot;cat&quot;)
train_dogs_dir = os.path.join(train_dir, &quot;dog&quot;)
# tfrecord
tfrecord_file = os.path.join(cats_and_dogs_dir, &quot;train.tfrecord&quot;)

# 训练数据
train_cat_filenames = [os.path.join(train_cats_dir, filename) for filename in os.listdir(train_cats_dir)]
train_dog_filenames = [os.path.join(train_dogs_dir, filename) for filename in os.listdir(train_dogs_dir)]
train_filenames = train_cat_filenames + train_dog_filenames
train_labels = [0] * len(train_cat_filenames) + [1] * len(train_dog_filenames)

# 迭代读取每张图片, 建立 tf.train.Feature 字典和 tf.train.Example 对象, 序列化并写入 TFRecord
with tf.io.TFRecordWriter(tfrecord_file) as writer:
   for filename, label in zip(train_filenames, train_labels):
      # 读取数据集图片到内存, image 为一个 Byte 类型的字符串
      image = open(filename, &quot;rb&quot;).read()
      # 建立 tf.train.Feature 字典
      feature = {
            # 图片是一个 Byte 对象
            &quot;image&quot;: tf.train.Feature(bytes_list = tf.train.BytesList(value = [image])),
            &quot;label&quot;: tf.train.Feature(int64_list = tf.train.Int64List(value = [label]))
      }
      # 通过字典建立 Example
      example = tf.train.Example(features = tf.train.Features(feature = feature))
      # 将 Example 序列化并写入 TFRecord 文件
      writer.write(example.SerializeToString())</code></pre>
<ol start="3" style="list-style-type: decimal">
<li>TFRecord 文件读取</li>
</ol>
<ul>
<li>TFRecord 数据文件读取步骤
<ul>
<li>(1)通过 <code>tf.data.TFRecordDataset</code> 读入原始的 TFRecord 文件, 获得一个 <code>tf.data.Dataset</code> 数据集对象</li>
<li>此时文件中的 <code>tf.train.Example</code> 对象尚未被反序列化</li>
<li>(2)通过 <code>tf.data.Dataset.map</code> 方法, 对该数据集对象中的每个序列化的 <code>tf.train.Example</code> 字符串
执行 <code>tf.io.parse_single_example</code> 函数, 从而实现反序列化</li>
</ul></li>
<li>TFRecord 数据文件读取示例</li>
</ul>
<pre class="python"><code>import tensorflow as tf
import os
import matplotlib.pyplot as plt

# root
root_dir = &quot;/Users/zfwang/project/machinelearning/deeplearning&quot;
# data
cats_and_dogs_dir = os.path.join(root_dir, &quot;datasets/cats_vs_dogs&quot;)
# tfrecord
tfrecord_file = os.path.join(cats_and_dogs_dir, &quot;train.tfrecord&quot;)

def _parse_example(example_string):
   &quot;&quot;&quot;
   将 TFRecord 文件中的每一个序列化的 tf.train.Example 解码
   &quot;&quot;&quot;
   # 定义 Feature 结构, 告诉解码器每个 Feature 的类型是什么
   feature_description = {
      &quot;image&quot;: tf.io.FixedLenFeature([], tf.string),
      &quot;label&quot;: tf.io.FixedLenFeature([], tf.int64)
   }
   feature_dict = tf.io.parse_single_example(example_string, feature_description)
   # 解码 JPEG 图片
   feature_dict[&quot;image&quot;] = tf.io.decode_jpeg(feature_dict[&quot;image&quot;])
   return feature_dict[&quot;image&quot;], feature_dict[&quot;label&quot;]

# 读取 TFRecord 文件
raw_dataset = tf.data.TFRecordDataset(tfrecord_file)
dataset = raw_dataset.map(_parse_example)

for image, label in dataset:
   plt.title(&quot;cat&quot; if label == 0 else &quot;dog&quot;)
   plt.imshow(image.numpy())
   plt.show()</code></pre>
</div>
<div id="tf.io-的其他格式" class="section level3">
<h3>4.10 tf.io 的其他格式</h3>
</div>
<div id="tf.tensorarray" class="section level3">
<h3>4.11 tf.TensorArray</h3>
<ol style="list-style-type: decimal">
<li>tf.TensorArray 介绍</li>
</ol>
<p>在部分网络结构中, 尤其是涉及时间序列的结构中, 可能需要将一系列张量以数组的方式依次存放起来, 以供进一步处理。</p>
<ul>
<li>在即时执行模式下, 可以直接使用一个 Python 列表存放数组</li>
<li>如果需要基于计算图的特性, 例如使用 <span class="citation">@tf.function</span> 加速模型运行或者使用 SaveModel 导出模型, 就无法使用 Python 列表了</li>
</ul>
<p>TensorFlow 提供了 <code>tf.TensorArray</code> (TensorFlow 动态数组) 支持计算图特性的 TensorFlow 动态数组.</p>
<ul>
<li><p>声明方式如下:</p>
<ul>
<li><p><code>arr = tf.TensorArray(dtype, size, dynamic_size = False)</code>:</p>
<ul>
<li>声明一个大小为 <code>size</code>, 类型为 <code>dtype</code> 的 <code>TensorArray arr</code></li>
<li>如果将 <code>dynamic_size</code> 参数设置为 True, 则该数组会自动增长空间</li>
</ul></li>
</ul></li>
<li><p>读取和写入的方法如下:</p>
<ul>
<li><code>write(index, value)</code>: 将 value 写入数组的第 index 个位置</li>
<li><code>read(index)</code>: 读取数组的第 index 个值</li>
<li><code>stack()</code></li>
<li><code>unstack()</code></li>
</ul></li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>tf.TensorArray 介绍</li>
</ol>
<pre class="python"><code>import tensorflow as tf

@tf.function
def array_write_and_read():
   arr = tf.TensorArray(dtype = tf.float32, size = 3)
   arr = arr.write(0, tf.constant(0.0))
   arr = arr.write(1, tf.constant(1.0))
   arr = arr.write(2, tf.constant(2.0))
   arr_0 = arr.read(0)
   arr_1 = arr.read(1)
   arr_2 = arr.read(2)
   return arr_0, arr_1, arr_2

a, b, c = array_write_and_read()
print(a, b, c)</code></pre>
<p>.. note::</p>
<ul>
<li>由于需要支持计算图, <code>tf.TensorArray</code> 的 <code>write()</code> 是不可以忽略左值的,
也就是说, 在图执行模式下, 必须按照以下的形式写入数组, 才可以正常生成一个计算图操作,
并将该操作返回给 <code>arr</code>:</li>
</ul>
<pre class="python"><code>arr.write(index, value)</code></pre>
<ul>
<li>不可以写成</li>
</ul>
<pre class="python"><code>arr.write(index, value)</code></pre>
</div>
</div>
<div id="数据输入流水线" class="section level2">
<h2>6.数据输入流水线</h2>
<div id="tf.data-1" class="section level3">
<h3>6.1 tf.data</h3>
</div>
<div id="优化流水线性能" class="section level3">
<h3>6.2 优化流水线性能</h3>
</div>
<div id="分析流水线性能" class="section level3">
<h3>6.3 分析流水线性能</h3>
</div>
</div>
</div>
<div id="模型构建-todo-完善" class="section level1">
<h1>模型构建 [TODO 完善]</h1>
<ul>
<li>Sequential API</li>
<li>Functional API</li>
<li>Subclassing API</li>
</ul>
<div id="模型共有的方法和属性" class="section level2">
<h2>1.模型共有的方法和属性</h2>
<pre class="python"><code>from tf.keras.model import Model
from tf.keras.model import model_from_json, model_from_yaml</code></pre>
<ul>
<li>model.layers</li>
<li>model.inputs</li>
<li>model.outputs</li>
<li>model.summary()</li>
<li>Config
<ul>
<li>model.get_config()
<ul>
<li>Model.from_config()</li>
<li>Sequential.from_config()</li>
</ul></li>
</ul></li>
<li>Weights
<ul>
<li>model.get_weights()
<ul>
<li><em>to Numpy arrays</em></li>
</ul></li>
<li>model.set_weights(weights)
<ul>
<li><em>from Numpy arrays</em></li>
</ul></li>
<li>model.save_weights(filepath)
<ul>
<li><em>to HDF5 file</em></li>
</ul></li>
<li>model.loadweights(filepath, byname = False)
<ul>
<li><em>from HDF5 file</em></li>
</ul></li>
</ul></li>
<li>Save or Load
<ul>
<li>model.to_json()
<ul>
<li>modelfromjson()</li>
</ul></li>
<li>modeltoyaml()
<ul>
<li>modelfromyaml()</li>
</ul></li>
</ul></li>
</ul>
</div>
<div id="sequential-api" class="section level2">
<h2>2.Sequential API</h2>
<p>Sequential 模型是层(layers)的线性堆叠</p>
<pre class="python"><code>from tensorflow import keras
from tensorflow.keras import layers, models
from tensorflow.keras.datasets import mnist

# data
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

# model
model = models.Sequential()
model.add(layers.Dense(units = 64, activation = &quot;relu&quot;))
model.add(layers.Dense(units = 10, activation = &quot;softmax&quot;))
model.compile(
   loss = &quot;categorical_crossentropy&quot;,
   optimizer = &quot;sgd&quot;,
   metrics = [&quot;accuracy&quot;]
)
model.fit(x_train, y_train, epochs = 5, batch_size = 32)
loss_and_metrics = model.evaluate(x_test, y_test, batch_size = 128)
classes = model.predict(x_test, batch_size = 128)</code></pre>
</div>
<div id="functional-api" class="section level2">
<h2>3.Functional API</h2>
<ul>
<li>Keras 函数式 API 是定义复杂模型的方法</li>
<li>Keras 函数式 API 可以重用经过训练的模型, 可以通过在张量上调用任何模型并将其视为一个层(layers)
<ul>
<li>调用模型的结构</li>
<li>调用模型的权重</li>
</ul></li>
</ul>
<p>函数式 API 特点</p>
<ul>
<li>所有模型都像层(layer)一样可以调用</li>
<li>多输入和多输出模型</li>
<li>共享图层</li>
<li>“层节点”概念</li>
</ul>
<pre class="python"><code>inputs = tf.keras.Input(shape = (28, 28, 1))
x = tf.keras.layers.Flatten()(inputs)
x = tf.keras.layers.Dense(units = 100, activation = tf.nn.relu)(x)
x = tf.keras.layers.Dense(units = 10)(x)
outputs = tf.keras.layers.Softmax()(x)
model = tf.keras.Model(inputs = inputs, outputs = outputs)</code></pre>
</div>
<div id="subclassing-api" class="section level2">
<h2>4.Subclassing API</h2>
<ul>
<li>使用 Subclassing API 建立模型, 即对 <code>tf.keras.Model</code> 类进行扩展以定义自己的新模型</li>
<li>实现 forward pass in the <code>call</code> method</li>
<li>模型的 layers 定义在 <code>__init__(self, ...)</code> 中</li>
<li>模型的前向传播定义在 <code>call(self, inputs)</code> 中</li>
<li>可以通过调用制定的自定义损失函数 <code>self.add_loss(loss_tensor)</code></li>
<li>在 subclassing 模型中, 模型的拓扑结构被定义为 Python 代码, 而不是 layers 的静态图,
因此无法检查或序列化模型的拓扑结构, 即以下方法不适用于 subclassing 模型:
<ul>
<li>model.inputs</li>
<li>model.outputs</li>
<li>model.to_yaml()</li>
<li>model.to_json()</li>
<li>model.get_config()</li>
<li>model.save()</li>
</ul></li>
<li>模型(keras.model.Model)子类的 API 可以为实现更加复杂的模型提供了灵活性, 但是是有代价的, 除了以上的功能不能使用, 并且模型更复杂, 更容易出错</li>
</ul>
<pre class="python"><code>import tensorflow as tf

class MyModel(tf.keras.Model):
    
    def __init__(self):
        super(MyModel, self).__init__()
        
        # 此处添加初始化的代码(包含call方法中会用到的层)例如:
        self.layer1 = tf.keras.layers.BuildInLayer()
        self.layer2 = MyCustomLayer(...)

    def call(self, input):
        # 此处添加模型调用的代码(处理输入并返回输出), 例如:
        x = layer1(input)
        self.output = layer2(x)
        return output

model = MyModel()

with tf.GradientTape() as tape:
    logits = model(images)
    loss_value = loss(logits, labels)
grads = tape.gradient(loss_value, model.trainable_variables)
optimizer.apply(zip(grads, model.trainable_variables))</code></pre>
</div>
<div id="回调函数-callbacks" class="section level2">
<h2>5.回调函数-Callbacks</h2>
<ul>
<li>回调函数是一个函数的集合, 会在训练的阶段使用</li>
<li>可以使用回调函数查看训练模型的内在状态和统计。
也可以传递一个列表的回调函数(作为 <code>callbacks</code> 关键字参数)到 <code>Sequential</code> 或 <code>Model</code> 类型的 <code>.fit()</code> 方法。
在训练时, 相应的回调函数的方法会被在各自的阶段被调用</li>
</ul>
<p>回调函数API:</p>
<ul>
<li>keras.callbacks.Callback()
<ul>
<li>用来创建新的回调函数的抽象基类</li>
<li><code>.params</code></li>
<li><code>.model</code></li>
</ul></li>
<li>keras.callbacks.BaseLogger(stateful_metrics = None)
<ul>
<li>基类训练 epoch 评估值的均值</li>
</ul></li>
<li>keras.callbacks.TerminateOnNaN()
<ul>
<li>当遇到损失为 <code>NaN</code> 停止训练</li>
</ul></li>
<li>keras.callbacks.ProgbarLogger()</li>
<li>keras.callbacks.History()
<ul>
<li>所有事件都记录到 History 对象</li>
</ul></li>
<li>keras.callbacks.ModelCheckpoint()
<ul>
<li>在每个训练期之后保存模型</li>
</ul></li>
<li>keras.callbacks.EarlyStopping()</li>
<li>keras.callbacks.RemoteMonitor()</li>
<li>keras.callbacks.LearningRateScheduler(schedule, verbose = 0)</li>
<li>keras.callbacks.TensorBoard()</li>
<li>keras.callbacks.ReduceLROnPlateau()</li>
<li>keras.callbacks.CSVLogger()</li>
<li>keras.callbacks.LambdaCallback()</li>
</ul>
<p>创建回调函数:</p>
<pre class="python"><code>from keras.layers import Dense, Activation
from keras.models import Sequential
from keras.callbacks import ModelCheckpoint

# 模型建立
model = Sequenital()
model.add(Dense(10, input_dim = 784, kernel_initializer = &quot;uniform&quot;))
model.add(Activation(&quot;softmax&quot;))

# 模型编译
model.compile(loss = &quot;categorical_crossentropy&quot;, optimizer = &quot;rmsporp&quot;)

# 模型训练
# 在训练时, 保存批量损失值
class LossHistory(keras.callbacks.Callback):
      def on_train_begin(self, logs = {}):
         self.losses = []

      def on_batch_end(self, batch, logs = {}):
         self.losses.append(logs.get(&quot;loss&quot;))
history = LossHistory()

# 如果验证集损失下降, 在每个训练 epoch 后保存模型
checkpointer = ModelCheckpoint(filepath = &quot;/tmp/weight.hdf5&quot;,
                               verbose = 1,
                               save_best_only = True)
model.fit(x_train, 
         y_train, 
         batch_size = 128, 
         epochs = 20, 
         verbose = 0,
         validation_data = (x_test, y_test), 
         callbacks = [history, checkpointer]
)

# 模型结果输出
print(history.losses)</code></pre>
</div>
</div>
<div id="applicationstodo-完善" class="section level1">
<h1>Applications[TODO 完善]</h1>
<div id="目前可用模型" class="section level2">
<h2>1.目前可用模型</h2>
<p>Keras Applications(<code>keras.applications</code>) 提供了预训练好的深度学习模型,
这些模型可以用于预测、特征提取等. 当初始化一个模型时就会自动下载,
默认下载的路径是: <code>~/.keras.models/</code>.</p>
<p>在 ImageNet 数据上预训练过的用于图像分类的模型</p>
<ul>
<li>Xception</li>
<li>VGG16</li>
<li>VGG19</li>
<li>ResNet, ResNetV2, ResNeXt</li>
<li>InceptionV3</li>
<li>InceptionResNet2</li>
<li>MobileNet</li>
<li>MobileNetV2</li>
<li>DenseNet</li>
<li>NASNet</li>
</ul>
<pre class="python"><code>from keras.applications.xception import Xception
from keras.applications.vgg16 import VGG16
from keras.applications.vgg19 import VGG19
from keras.applications.resnet50 import ResNet50
from keras.applications.inception_v3 import InceptionV3
from keras.applications.inception_resnet_v2 import InceptionResNetV2
from keras.applications.mobilenet import MobileNet
from keras.applications.densenet import DenseNet121
from keras.applications.densenet import DenseNet169
from keras.applications.densenet import DenseNet201
from keras.applications.nasnet import NASNetLarge
from keras.applications.nasnet import NASNetMobile
from keras.applications.mobilenet_v2 import MobileNetV2

# channels_last only; 299x299
xception_model = Xception(include_top = True,
                           weights = &quot;imagenet&quot;,
                           input_tensor = None, 
                           input_shape = None,
                           pooling = None,
                           classes = 1000)
# channels_first and channels_last; 224x224
vgg16_model = VGG16(include_top = True,
                     weights = &quot;imagenet&quot;,
                     input_tensor = None, 
                     input_shape = None,
                     pooling = None,
                     classes = 1000)
vgg19_model = VGG19(include_top = True, 
                     weights = &#39;imagenet&#39;,
                     input_tensor = None, 
                     input_shape = None, 
                     pooling = None, 
                     classes = 1000)
resnet50_model = ResNet50(include_top = True, 
                           weights = &#39;imagenet&#39;, 
                           input_tensor = None, 
                           input_shape = None, 
                           pooling = None, 
                           classes = 1000)
inception_v3_model = InceptionV3(include_top = True, 
                                 weights = &#39;imagenet&#39;, 
                                 input_tensor = None, 
                                 input_shape = None, 
                                 pooling = None, 
                                 classes = 1000)
inception_resnet_v2_model = InceptionResNetV2(include_top = True, 
                                                weights = &#39;imagenet&#39;, 
                                                input_tensor = None, 
                                                input_shape = None, 
                                                pooling = None, 
                                                classes = 1000)
mobilenet_model = MobileNet(input_shape = None, 
                              alpha = 1.0, 
                              depth_multiplier = 1, 
                              dropout = 1e-3, 
                              include_top = True, 
                              weights = &#39;imagenet&#39;, 
                              input_tensor = None, 
                              pooling = None, 
                              classes = 1000)
densenet_model = DenseNet121(include_top = True, 
                              weights = &#39;imagenet&#39;, 
                              input_tensor = None, 
                              input_shape = None, 
                              pooling = None, 
                              classes = 1000)
densenet_model = DenseNet169(include_top = True, 
                              weights = &#39;imagenet&#39;, 
                              input_tensor = None, 
                              input_shape = None, 
                              pooling = None, 
                              classes = 1000)
densenet_model = DenseNet201(include_top = True, 
                              weights = &#39;imagenet&#39;, 
                              input_tensor = None, 
                              input_shape = None, 
                              pooling = None, 
                              classes = 1000)
nasnet_model = NASNetLarge(input_shape = None, 
                           include_top = True, 
                           weights = &#39;imagenet&#39;, 
                           input_tensor = None, 
                           pooling = None, 
                           classes = 1000)
nasnet_model = NASNetMobile(input_shape = None, 
                              include_top = True, 
                              weights = &#39;imagenet&#39;, 
                              input_tensor = None, 
                              pooling = None, 
                              classes = 1000)
mobilenet_v2_model = MobileNetV2(input_shape = None, 
                                 alpha = 1.0, 
                                 depth_multiplier = 1, 
                                 include_top = True, 
                                 weights = &#39;imagenet&#39;, 
                                 input_tensor = None, 
                                 pooling = None, 
                                 classes = 1000)</code></pre>
</div>
<div id="示例" class="section level2">
<h2>2.示例</h2>
<ul>
<li>图像分类模型使用示例</li>
</ul>
<pre class="python"><code>from keras.preprocessing import image
from keras.applications.resnet50 import ResNet50
from keras.applications.resnet50 import preprocess_input, decode_prediction
import numpy as np

# Load model
model = ResNet50(weights = &quot;imagenet&quot;)

# Image data
img_path = &quot;elephant.jpg&quot;
img = image.load_img(img_path, target_size = (224, 224))
x = image.img_to_array(img)
x = np.expand_dims(x, axis = 0)
x = preprocess_input(x)

preds = model.predict(x)
print(&quot;Predicted:&quot;, decode_prediction(preds, top = 3)[0])</code></pre>
</div>
</div>
<div id="utils" class="section level1">
<h1>Utils</h1>
<div id="模型可视化" class="section level2">
<h2>1.模型可视化</h2>
<div id="plot_model" class="section level3">
<h3>1.1 <code>plot_model()</code></h3>
<ul>
<li>Converts a Keras model to dot format and save to a file.</li>
</ul>
<pre class="python"><code>
import tensorflow as tf

tf.keras.utils.plot_model(
    model,
    to_file = &quot;model.png&quot;,
    show_shapes = False,
    show_dtype = False,
    show_layer_names = True,
    rankdir = &quot;TB&quot;,
    expand_nested = False,
    dpi = 96,
)</code></pre>
</div>
<div id="model_to_dot" class="section level3">
<h3>1.2 <code>model_to_dot()</code></h3>
<ul>
<li>Convert a Keras model to dot format.</li>
</ul>
<pre class="python"><code>import tensorflow as tf

tf.keras.utils.model_to_dot(
    model,
    show_shapes = False,
    show_dtype = False,
    show_layer_names = True,
    rankdir = &quot;TB&quot;,             # &quot;TB&quot;: a vertical plot; &quot;LR&quot;: a horizontal plot
    expand_nested = False,
    dpi = 96,
    subgraph = False,
)</code></pre>
</div>
</div>
<div id="序列化工具serialization-utilities" class="section level2">
<h2>2.序列化工具(Serialization utilities)</h2>
<ul>
<li>custom_object_scope()</li>
<li>get_custom_objects()</li>
<li>register_keras_serializable()</li>
<li>serialize_keras_object()</li>
<li>daserialize_keras_object()</li>
</ul>
<div id="customobjectscope-class" class="section level3">
<h3>2.1 <code>CustomObjectScope</code> class</h3>
<ul>
<li><p>作用</p>
<ul>
<li>将自定义类/函数 暴露给 Keras 反序列化内部组件</li>
<li>在范围 <code>with custom_object_scope(object_dict)</code>, Keras 方法将能够反序列化已保存的配置引用的任何自定义对象</li>
</ul></li>
<li><p>语法</p></li>
</ul>
<pre class="python"><code>import tensorflow as tf

tf.keras.utils.custom_object_scope(*args)</code></pre>
<ul>
<li>示例</li>
</ul>
<pre class="python"><code># 一个自定义的正则化器 `my_regularizer`
my_regularizer = None

# a layer
layer = Dense(3, kernel_regularizer = my_regularizer)

# Config contains a reference to &quot;my_regularizer&quot;
config = layer.get_config()
...

# Later
with custom_object_scope({&quot;my_regularizer&quot;: my_regularizer}):
    layer = Dense.from_config(config)</code></pre>
</div>
<div id="get_custom_objects" class="section level3">
<h3>2.2 get_custom_objects()</h3>
<ul>
<li><p>作用</p>
<ul>
<li>额, 下次一定</li>
</ul></li>
<li><p>语法</p></li>
</ul>
<pre class="python"><code>import tensorflow as tf

tf.keras.utils.get_custom_objects()</code></pre>
<ul>
<li>示例</li>
</ul>
<pre class="python"><code>get_custom_objects().clear()
get_custom_objects()[&quot;MyObject&quot;] = MyObject</code></pre>
</div>
<div id="register_keras_serializable" class="section level3">
<h3>2.3 register_keras_serializable()</h3>
<ul>
<li><p>作用</p>
<ul>
<li>额, 下次一定</li>
</ul></li>
<li><p>语法</p></li>
</ul>
<pre class="python"><code>import tensorflow as tf

tf.keras.utils.register_keras.serializable(package = &quot;Custom&quot;, name = None)</code></pre>
</div>
<div id="serialize_keras_object" class="section level3">
<h3>2.4 serialize_keras_object()</h3>
<ul>
<li><p>作用</p>
<ul>
<li>将 Keras 对象序列化为 Json 兼容的表示形式</li>
</ul></li>
<li><p>语法</p></li>
</ul>
<pre class="python"><code>import tensorflow as tf

tf.keras.utils.serialize_keras_object(instance)</code></pre>
</div>
<div id="daserialize_keras_object" class="section level3">
<h3>2.5 daserialize_keras_object()</h3>
<ul>
<li><p>作用</p>
<ul>
<li>将 Keras 对象的序列化形式转换回实际对象</li>
</ul></li>
<li><p>语法</p></li>
</ul>
<pre class="python"><code>import tensorflow as tf

tf.keras.utils.deserialize_keras_object(
    identifier, 
    module_objects = None,
    custom_objects = None,
    printable_module_name = &quot;object&quot;
)</code></pre>
</div>
</div>
<div id="python-numpy-utilities" class="section level2">
<h2>3.Python &amp; Numpy utilities</h2>
<div id="to_categorical" class="section level3">
<h3>3.1 <code>to_categorical()</code></h3>
<ul>
<li><p>作用</p>
<ul>
<li>将一个类别型向量(整数)转换为 二元类别矩阵</li>
<li>类似于 one-hot</li>
</ul></li>
<li><p>语法</p></li>
</ul>
<pre class="python"><code>import tensorflow as tf

utils.to_categorical(y,
                    num_classes = None,
                    dtypes = &quot;float32&quot;)</code></pre>
<ul>
<li>示例</li>
</ul>
<pre class="python"><code># example 1
a = tf.keras.utils.to_categorical([0, 1, 2, 3], num_classes = 4)
a = tf.constant(a, shape = [4, 4])
print(a)

# example 2
b = tf.constant([.9, .04, .03, .03,
                    .3, .45, .15, .13,
                    .04, .01, .94, .05,
                    .12, .21, .5, .17],
                    shape = [4, 4])
loss = tf.keras.backend.categorical_crossentropy(a, b)
print(np.around(loss, 5))

# example 3
loss = tf.keras.backend.categorical_crossentropy(a, a)
print(np.around(loss, 5))</code></pre>
</div>
<div id="normalize" class="section level3">
<h3>3.2 <code>normalize()</code></h3>
<ul>
<li><p>作用</p>
<ul>
<li>标准化一个 Numpy 数组</li>
</ul></li>
<li><p>语法</p></li>
</ul>
<pre class="python"><code>import tensorflow as tf

tf.keras.utils.normalize(x, axis = -1, order = 2)</code></pre>
</div>
<div id="get_file" class="section level3">
<h3>3.3 <code>get_file()</code></h3>
<ul>
<li><p>作用</p>
<ul>
<li>Downloads a file from a URL if it not already in the cache.</li>
<li>By default the file at the url <code>origin</code> is downloaded to the cache_dir <code>~/.keras</code>,
placed in the cache_subdir datasets, and given the filename <code>fname</code>.
The final location of a file <code>example.txt</code> would therefore be <code>~/.keras/datasets/example.txt</code>.</li>
<li>Files in tar, tar.gz, tar.bz, and zip formats can also be extracted.
Passing a hash will verify the file after download.
The command line programs shasum and sha256sum can compute the hash.</li>
</ul></li>
<li><p>语法</p></li>
</ul>
<pre class="python"><code>
tf.keras.utils.get_file(
    fname,
    origin,
    untar=False,
    md5_hash=None,
    file_hash=None,
    cache_subdir=&quot;datasets&quot;,
    hash_algorithm=&quot;auto&quot;,
    extract=False,
    archive_format=&quot;auto&quot;,
    cache_dir=None,
)</code></pre>
<ul>
<li>示例</li>
</ul>
<pre class="python"><code>
import tensorflow

path_to_downloaded_file = tf.keras.utils.get_file(
    &quot;flower_photos&quot;,
    &quot;https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz&quot;,
    untar = True
)</code></pre>
</div>
<div id="progbar-class" class="section level3">
<h3>3.4 <code>Progbar</code> class</h3>
<ul>
<li><p>作用</p>
<ul>
<li>显示进度条</li>
</ul></li>
<li><p>语法</p></li>
</ul>
<pre class="python"><code>import tensorflow as tf

tf.keras.utils.Progbar(
    target, 
    width = 30, 
    verbose = 1, 
    interval = 0.05, 
    stateful_metrics = None, 
    unit_name = &quot;step&quot;
)</code></pre>
</div>
<div id="sequence-class" class="section level3">
<h3>3.5 <code>Sequence</code> class</h3>
<ul>
<li><p>作用</p>
<ul>
<li>用于拟合数据序列(如数据集)的基础对象</li>
<li>每个人都Sequence必须实现__getitem__和__len__方法。如果您想在各个时期之间修改数据集, 则可以实现 on_epoch_end。该方法__getitem__应返回完整的批次</li>
<li>Sequence是进行多处理的更安全方法。这种结构保证了网络在每个时期的每个样本上只会训练一次, 而生成器则不会</li>
</ul></li>
<li><p>语法</p></li>
</ul>
<pre class="python"><code>import tensorflow as tf
tf.keras.utils.Sequence()</code></pre>
<ul>
<li>示例</li>
</ul>
<pre class="python"><code>from skimage.io import imread
from skimage.transform import resize
import numpy as np
import math

# Here, `x_set` is list of path to the images
# and `y_set` are the associated classes.

class CIFAR10Sequence(Sequence):

    def __init__(self, x_set, y_set, batch_size):
        self.x, self.y = x_set, y_set
        self.batch_size = batch_size

    def __len__(self):
        return math.ceil(len(self.x) / self.batch_size)

    def __getitem__(self, idx):
        batch_x = self.x[idx * self.batch_size:(idx + 1) *
        self.batch_size]
        batch_y = self.y[idx * self.batch_size:(idx + 1) *
        self.batch_size]

        return np.array([
            resize(imread(file_name), (200, 200))
            for file_name in batch_x]), np.array(batch_y)</code></pre>
</div>
</div>
</div>
<div id="模型编译" class="section level1">
<h1>模型编译</h1>
<pre class="python"><code>model.compile(loss, optimizer, metrics)</code></pre>
<div id="损失函数" class="section level2">
<h2>1.损失函数</h2>
<ul>
<li>Loss Function</li>
<li>Objective Function</li>
<li>Optimization score Function</li>
</ul>
<p><strong>回归:</strong></p>
<pre class="python"><code>from keras import losses

# 回归 
from keras.losses import mean_squared_error
from keras.losses import mean_absolute_error
from keras.losses import mean_absolute_percentage_error
from keras.losses import mean_squared_logarithmic_error
from keras.losses import squared_hinge
from keras.losses import hinge
from keras.losses import categorical_hinge
from keras.losses import logcosh

model.Compile(loss = [&quot;mse&quot;, &quot;MSE&quot;, mean_squared_error], 
            optimizer, 
            metircs)
model.Compile(loss = [&quot;mae&quot;, &quot;MAE&quot;, mean_absolute_error], 
            optimizer, 
            metircs)
model.Compile(loss = [&quot;mape&quot;, &quot;MAPE&quot;, mean_absolute_percentage_error], 
            optimizer, 
            metircs)
model.Compile(loss = [&quot;msle&quot;, &quot;MLSE&quot;, mean_squared_logarithmic_error], 
            optimizer, 
            metircs)</code></pre>
<p><strong>分类:</strong></p>
<pre class="python"><code># 分类
from keras.losses import categorical_crossentropy
from keras.losses import sparse_categorical_crossentropy
from keras.losses import binary_crossentropy
from keras.losses import kullback_leibler_divergence
from keras.losses import poisson
from keras.losses import cosine_proximity

model.Compile(loss = [&quot;kld&quot;, &quot;KLD&quot;, kullback_leibler_divergence], 
            optimizer, 
            metircs)
model.Compile(loss = [&quot;cosine&quot;, cosine_proximity], 
            optimizer, 
            metircs)</code></pre>
<p>The purpose of loss functions is to compute the quantity that a model
should seek to minimize during training.</p>
<div id="常用损失函数" class="section level3">
<h3>1.1 常用损失函数</h3>
<ul>
<li>class handle
<ul>
<li>可以传递配置参数</li>
</ul></li>
<li>function handle</li>
</ul>
<ol style="list-style-type: decimal">
<li>概率损失(Probabilistic losses)</li>
</ol>
<ul>
<li><code>BinaryCrossentropy</code> class
<ul>
<li><code>binary_crossentropy()</code> function</li>
</ul></li>
<li><code>CategoricalCrossentropy</code> class
<ul>
<li><code>categorical_crossentropy()</code> function</li>
</ul></li>
<li><code>SparseCategoricalCrossentropy</code> class
<ul>
<li><code>sparse_categorical_crossentropy()</code> function</li>
</ul></li>
<li><code>Possion</code> class
<ul>
<li><code>possion()</code> function</li>
</ul></li>
<li><code>KLDivergence</code> class
<ul>
<li><code>kl_divergence()</code> function</li>
</ul></li>
</ul>
<p>class &amp; function() 使用方法</p>
<ul>
<li>作用</li>
<li>二分类损失函数
<ul>
<li>BinaryCrossentropy &amp; binary_crossentropy</li>
<li>Computes the cross-entropy loss between true labels and predicted labels.</li>
</ul></li>
<li>二分类、多分类
<ul>
<li>CategoricalCrossentropy &amp; categorical_crossentropy</li>
<li>SparseCategoricalCrossentropy &amp; sparse_categorical_crossentropy</li>
</ul></li>
<li>其他</li>
<li>语法</li>
</ul>
<pre class="python"><code>tf.keras.losses.Class(
    from_loits = False, 
    label_smoothing = 0, 
    reduction = &quot;auto&quot;, 
    name = &quot;&quot;
)</code></pre>
<ul>
<li>示例</li>
</ul>
<pre class="python"><code># data
y_ture = [[0., 1.], [0., 0.]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]

# reduction=&quot;auto&quot; or &quot;sum_over_batch_size&quot;
bce = tf.keras.losses.BinaryCrossentropy()
bce(y_true, y_pred).numpy()

# reduction=sample_weight
bce = tf.keras.losses.BinaryCrossentropy()
bce(y_true, y_pred, sample_weight = [1, 0]).numpy()

# reduction=sum
bce = tf.keras.losses.BinaryCrossentropy(reduction = tf.keras.losses.Reduction.SUM)
bce(y_true, y_pred).numpy()

# reduction=none
bce = tf.keras.losses.BinaryCrossentropy(reduction = tf.keras.losses.Reduction.NONE)
bce(y_true, y_pred).numpy()</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>回归损失(Regression losses)</li>
</ol>
<ul>
<li><code>MeanSquaredError</code> class
<ul>
<li><code>mean_squared_error</code> function</li>
</ul></li>
<li><code>MeanAbsoluteError</code> class
<ul>
<li><code>mean_absolute_error</code> function</li>
</ul></li>
<li><code>MeanAbsolutePercentageError</code> class
<ul>
<li><code>mean_absolute_percentage_error</code> function</li>
</ul></li>
<li><code>MeanSquaredLogarithmicError</code> class
<ul>
<li><code>mean_squared_logarithmic_error</code> function</li>
</ul></li>
<li><code>CosineSimilarity</code> class
<ul>
<li><code>cosine_similarity</code> function</li>
</ul></li>
<li><code>Huber</code> class
<ul>
<li><code>huber</code> function</li>
</ul></li>
<li><code>LogCosh</code> class
<ul>
<li><code>log_cosh</code> function</li>
</ul></li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>Hinge losses for “maximum-margin” classification</li>
</ol>
<ul>
<li><code>Hinge</code> class
<ul>
<li><code>hinge</code> function</li>
</ul></li>
<li><code>SquaredHinge</code> class
<ul>
<li><code>squared_hinge</code> function</li>
</ul></li>
<li><code>CategoricalHinge</code> class
<ul>
<li><code>categorical_hinge</code> function</li>
</ul></li>
</ul>
</div>
<div id="损失函数的使用compile-fit" class="section level3">
<h3>1.2 损失函数的使用——compile() &amp; fit()</h3>
<ul>
<li>通过实例化一个损失类创建损失函数, 可以传递配置参数</li>
</ul>
<pre class="python"><code>from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential()
model.add(layers.Dense(64, kernel_initializer = &quot;uniform&quot;, input_shape = (10,)))
model.add(layers.Activation(&quot;softmax&quot;))

model.compile(
    loss = keras.losses.SparseCategoricalCrossentropy(from_logits = True), 
    optimizer = &quot;adam&quot;, 
    metrics = [&quot;acc&quot;]
)</code></pre>
<ul>
<li>直接使用损失函数</li>
</ul>
<pre class="python"><code>from tensorflow.keras.losses import sparse_categorical_crossentropy

model.compile(
    loss = &quot;sparse_categorical_crossentropy&quot;, 
    optimizer = &quot;adam&quot;, 
    metrics = [&quot;acc&quot;]
)</code></pre>
</div>
<div id="损失函数的使用单独使用" class="section level3">
<h3>1.3 损失函数的使用——单独使用</h3>
<pre class="python"><code>tf.keras.losses.mean_squared_error(tf.ones((2, 2)), tf.zeros((2, 2)))
loss_fn = tf.keras.losses.MeanSquaredError(resuction = &quot;sum_over_batch_size&quot;)
loss_fn(tf.ones((2, 2)), tf.zeros((2, 2)))

loss_fn = tf.keras.losses.MeanSquaredError(reduction = &quot;sum&quot;)
loss_fn(tf.ones((2, 2)), tf.zeros((2, 2)))

loss_fn = tf.keras.losses.MeanSquaredError(reduction = &quot;none&quot;)
loss_fn(tf.ones((2, 2)), tf.zeros((2, 2)))

loss_fn = tf.keras.losses.mean_squared_error
loss_fn(tf.ones((2, 2,)), tf.zeros((2, 2)))

loss_fn = tf.keras.losses.MeanSquaredError()
loss_fn(tf.ones((2, 2)), tf.zeros((2, 2)))</code></pre>
</div>
<div id="创建自定义损失函数" class="section level3">
<h3>1.4 创建自定义损失函数</h3>
<ul>
<li>Any callable with the signature <code>loss_fn(y_true, y_pred)</code> that returns an array of
losses (one of sample in the input batch) can be passed to compile() as a loss.</li>
<li>Note that sample weighting is automatically supported for any such loss.</li>
</ul>
<p>示例:</p>
<pre class="python"><code>def my_loss_fn(y_true, y_pred):
    squared_difference = tf.square(y_true - y_pred)
    return tf.reduce_mean(squared_difference, axis = -1)

model.compile(optimizer = &quot;adam&quot;, loss = my_loss_fn)</code></pre>
<ol style="list-style-type: decimal">
<li><code>add_loss()</code> API</li>
</ol>
<pre class="python"><code>from tensorflow.keras.layers import Layer

class MyActivityRegularizer(Layer):
    &quot;&quot;&quot;Layer that creates an activity sparsity regularization loss.&quot;&quot;&quot;

    def __init__(self, rate = 1e-2):
        super(MyActivityRegularizer, self).__init__()
        self.rate = rate

    def call(self, inputs):
        self.add_loss(self.rate * tf.reduce_sum(tf.square(inputs)))

        return inputs

from tensorflow.keras import layers

class SparseMLP(Layer):
    &quot;&quot;&quot;Stack of Linear layers with a sparsity regularization loss.&quot;&quot;&quot;

    def __init__(self, output_dim):
        super(SparseMLP, self).__init__()
        self.dense_1 = layers.Dense(32, activation=tf.nn.relu)
        self.regularization = MyActivityRegularizer(1e-2)
        self.dense_2 = layers.Dense(output_dim)

    def call(self, inputs):
        x = self.dense_1(inputs)
        x = self.regularization(x)
        return self.dense_2(x)

mlp = SparseMLP(1)
y = mlp(tf.ones((10, 10)))

print(mlp.losses)  # List containing one float32 scalar

mlp = SparseMLP(1)
mlp(tf.ones((10, 10)))
assert len(mlp.losses) == 1
mlp(tf.ones((10, 10)))
assert len(mlp.losses) == 1  # No accumulation.</code></pre>
<ul>
<li>自定义损失函数需要继承 <code>tf.keras.losses.Loss</code> 类, 重写 <code>call</code> 方法即可,
输入真实值 <code>y_true</code> 和模型预测值 <code>y_pred</code>, 输出模型预测值和真实值之间通
过自定义的损失函数计算出的损失值</li>
</ul>
<pre class="python"><code>import numpy as np
import tensorflow as tf

class MeanSquaredError(tf.keras.losses.Loss):
    def call(self, y_true, y_pred):
        return tf.reduce_mean(tf.square(y_pred - y_true))</code></pre>
</div>
</div>
<div id="评价指标" class="section level2">
<h2>2.评价指标</h2>
<ul>
<li>Metric 是一个评估模型表现的函数</li>
<li>Metric 函数类似于一个损失函数, 只不过模型评估返回的 metric
不用来训练模型, 因此, 可以使用任何损失函数当做一个 metric 函数使用</li>
</ul>
<div id="metrics" class="section level3">
<h3>2.1 metrics</h3>
<p>API:</p>
<pre class="python"><code>from keras import metrics
from keras.metrics import binary_accuracy
from keras.metrics import categorical_accuracy
from keras.metrics import sparse_categorical_accuracy
from keras.metrics import top_k_categorical_accuracy
from keras.metrics import sparse_top_k_categorical_accuracy
from keras.metrics import mae

from keras.losses import mean_squared_error
from keras.losses import mean_absolute_error
from keras.losses import mean_absolute_percentage_error
from keras.losses import mean_squared_logarithmic_error
from keras.losses import squared_hinge
from keras.losses import hinge
from keras.losses import categorical_hinge
from keras.losses import logcosh
from keras.losses import categorical_crossentropy
from keras.losses import sparse_categorical_crossentropy
from keras.losses import binary_crossentropy
from keras.losses import kullback_leibler_divergence
from keras.losses import poisson
from keras.losses import cosine_proximity</code></pre>
<p>Metrics Name:</p>
<pre class="python"><code>metrics = [&quot;acc&quot;, &quot;accuracy&quot;]</code></pre>
</div>
<div id="accuracy-metrics" class="section level3">
<h3>2.2 Accuracy metrics</h3>
<ul>
<li>Accuracy class</li>
<li>BinaryAccuracy class</li>
<li>CategoricalAccuracy class</li>
<li>TopKCategoricalAccuracy class</li>
<li>SparseTopKCategoricalAccuracy class</li>
</ul>
</div>
<div id="probabilistic-metrics" class="section level3">
<h3>2.3 Probabilistic metrics</h3>
<ul>
<li>BinaryCrossentropy class</li>
<li>CategoricalCrossentropy class</li>
<li>SparseCategoricalCrossentropy class</li>
<li>KLDivergence class</li>
<li>Poisson class</li>
</ul>
</div>
<div id="regression-metrics" class="section level3">
<h3>2.4 Regression metrics</h3>
<ul>
<li>MeanSquaredError class</li>
<li>RootMeanSquaredError class</li>
<li>MeanAbsoluteError class</li>
<li>MeanAbsolutePercentageError class</li>
<li>CosineSimilarity class</li>
<li>LogCoshError class</li>
</ul>
</div>
<div id="classification-metrics-based-on-truefalse-positives-negatives" class="section level3">
<h3>2.5 Classification metrics based on True/False positives &amp; negatives</h3>
<ul>
<li>AUC class</li>
<li>Precision class</li>
<li>Recall class</li>
<li>TurePositives class</li>
<li>TrueNegatives class</li>
<li>FalsePositives class</li>
<li>FalseNegatives class</li>
<li>PrecisionAtRecall class</li>
<li>SensitivityAtSpecificity class</li>
<li>SpecificityAtSensitivity class</li>
</ul>
</div>
<div id="image-segmentation-metrics" class="section level3">
<h3>2.6 image segmentation metrics</h3>
<ul>
<li>MeanIoU class</li>
</ul>
</div>
<div id="hinge-metrics-for-maximum-margin-classification" class="section level3">
<h3>2.7 Hinge metrics for “maximum-margin” Classification</h3>
<ul>
<li>Hinge class</li>
<li>SquaredHinge class</li>
<li>CategoricalHinge class</li>
</ul>
</div>
<div id="评价指标的使用compile-fit" class="section level3">
<h3>2.8 评价指标的使用——compile() &amp; fit()</h3>
</div>
<div id="评价指标的使用单独使用" class="section level3">
<h3>2.9 评价指标的使用——单独使用</h3>
</div>
<div id="自定义评估指标" class="section level3">
<h3>2.10 自定义评估指标</h3>
<ul>
<li>自定义评估指标需要继承 <code>tf.keras.metrics.Metric</code> 类,
并重写 <code>__init__</code>、<code>update_state</code>、<code>result</code> 三个方法</li>
</ul>
<pre class="python"><code>import numpy as np
import tensorflow as tf

class SparseCategoricalAccuracy(tf.keras.metrics.Metric):
    def __init__(self):
        super().__init__()
        self.total = self.add_weight(name = &quot;total&quot;, dtype = tf.int32, initializer = tf.zeros_initializer())
        self.count = self.add_weight(name = &quot;total&quot;, dtype = tf.int32, initializer = tf.zeros_initializer())

    def update_state(self, y_true, y_pred, sample_weight = None):
        values = tf.cast(tf.equal(y_true, tf.argmax(y_pred, axis = 1, output_type = tf.int32)), tf.int32)
        self.total.assign_add(tf.shape(y_true)[0])
        self.count.assign_add(tf.reduce_sum(values))

    def result(self):
        return self.count / self.total</code></pre>
<pre class="python"><code>
import keras.backend as K

def mean_pred(y_true, y_pred):
   return K.mean(y_pred)

model.compile(optimizers = &quot;rmsprop&quot;,
            loss = &quot;binary_accuracy&quot;,
            metrics = [&quot;accuracy&quot;, mean_pred])</code></pre>
</div>
</div>
<div id="优化器" class="section level2">
<h2>3.优化器</h2>
<div id="optimizers" class="section level3">
<h3>3.1 Optimizers</h3>
<ul>
<li>SGD</li>
<li>RMSprop</li>
<li>Adagrad</li>
<li>Adadelta</li>
<li>Adam</li>
<li>Adamax</li>
<li>Nadam</li>
</ul>
<pre class="python"><code>from keras import optimizers

sgd = optimizers.SGD(lr = 0.01)
model.compile(loss, optimizer = sgd)
# or
model.compile(loss, optimizer = &quot;sgd&quot;)

rmsprop = optimizers.RMSprop(lr = 0.001)
model.compile(loss, optimizer = rmsprop)
# or
model.compile(loss, optimizer = &quot;rmsprop&quot;)

adagrad = optimizers.Adagrad(lr = 0.01)
model.compile(loss, optimizer = adagrad)
# or
model.compile(loss, optimizer = &quot;adagrad&quot;)

adadelta = optimizers.Adadelta(lr = 1.0)
model.compile(loss, optimizer = adadelta)
# or
model.compile(loss, optimizer = &quot;adadelta&quot;)

adam = optimizers.Adam(lr = 0.001)
model.compile(loss, optimizer = adam)
# or
model.compile(loss, optimizer = &quot;adam&quot;)

adamax = optimizers.Adamax(lr = 0.02)
model.compile(loss, optimizer = adamax)
# or
model.compile(loss, optimizer = &quot;adamax&quot;)

nadam = optimizers.Nadam(lr = 0.002)
model.compile(loss, optimizer = nadam)
# or
model.compile(loss, optimizer = &quot;nadam&quot;)</code></pre>
</div>
<div id="optimizder-的使用方式" class="section level3">
<h3>3.2 optimizder 的使用方式</h3>
<ol style="list-style-type: decimal">
<li><code>keras.optimizers</code> 和 <code>optimizer</code> 参数</li>
</ol>
<pre class="python"><code>from keras import optimizers

# 编译模型
sgd = optimizers.SGD(lr = 0.01, decay = 1e-6, momentum = 0.9, nesterov = True)
model.compile(loss = &quot;mean_squared_error&quot;, optimizer = sgd)</code></pre>
<ol start="2" style="list-style-type: decimal">
<li><code>optimizer</code> 参数</li>
</ol>
<pre class="python"><code># 编译模型
model.compile(loss = &quot;mean_squared_error&quot;, optimizer = &quot;sgd&quot;)</code></pre>
</div>
<div id="optimizers-的共有参数" class="section level3">
<h3>3.3 optimizers 的共有参数</h3>
<ul>
<li>control gradient clipping
<ul>
<li><code>clipnorm</code></li>
<li><code>clipvalue</code></li>
</ul></li>
</ul>
<pre class="python"><code>from keras import optimizers

# All parameter gradients will be clipped to
# a maximum norm of 1.
sgd = optimizers.SGD(lr = 0.01, clipnorm = 1)

# All parameter gradients will be clipped to
# a maximum value of 0.5 and
# a minimum value of -0.5.
sgd = optimizers.SGD(lr = 0.01, clipvalue = 0.5)</code></pre>
</div>
<div id="优化器的使用" class="section level3">
<h3>3.4 优化器的使用</h3>
<ol style="list-style-type: decimal">
<li>模型编译(compile)和拟合(fit)</li>
</ol>
<pre class="python"><code>from tensorflow import keras
from tensorflow.keras import layers

# model
model = keras.Sequential()
model.add(layers.Dense(64, kernel_initializer = &quot;uniform&quot;, input_shape = (10,)))
model.add(layers.Activate(&quot;softmax&quot;))
# model compile
opt = keras.optimizers.Adam(learning_rate = 0.01)
model.compile(loss = &quot;categorical_crossentropy&quot;, optimizer = opt)
# model.compile(loss = &quot;categorical_crossentropy&quot;, optimizer = &quot;adam&quot;)</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>自定义迭代训练</li>
</ol>
<pre class="python"><code># Instantiate an optimizer
optimizer = tf.keras.optimizer.Adam()

# Iterate over the batches of a dataset.
for x, y in dataset:
# open a GradientTape
with tf.GradientTape() as tape:
    # Forward pass.
    logits = model(x)
    
    # Loss value for this batch
    loss_value = loss_fn(y, logits)

# Get gradients of loss wrt the weights
gradients = tape.gradient(loss_value, model.trainable_weights)

# Update the weights of the model
optimizer.apply_gradients(zip(gradients, model.trainable_weights))</code></pre>
<ol start="3" style="list-style-type: decimal">
<li>学习率衰减(decay)、调度(sheduling)</li>
</ol>
<ul>
<li>可以使用学习率时间表来调整优化器的学习率如何随时间变化</li>
<li>ExponentialDecay: 指数衰减</li>
<li>PiecewiseConstantDecay:</li>
<li>PolynomialDecay: 多项式衰减</li>
<li>InverseTimeDecay: 逆时间衰减</li>
</ul>
<pre class="python"><code>lr_schedule = keras.optimizers.schedules.ExponentialDecay(
initial_learning_rate = 1e-2,
decay_steps = 10000,
decay_rate = 0.9
)
optimizer = keras.optimizers.SGD(learning_rate = lr_schedule)</code></pre>
</div>
<div id="优化算法核心-api" class="section level3">
<h3>3.5 优化算法核心 API</h3>
<ul>
<li>apply_gradients</li>
<li>weights_property</li>
<li>get_weights</li>
<li>set_weights</li>
</ul>
<ol style="list-style-type: decimal">
<li>apply_gradients</li>
</ol>
<ul>
<li>语法</li>
</ul>
<pre class="python"><code>Optimizer.apply_gradients(
    grads_and_vars, name=None, experimental_aggregate_gradients=True
)</code></pre>
<ul>
<li>参数
<ul>
<li>grads_and_vars: 梯度、变量对的列表</li>
<li>name: 返回的操作的名称</li>
<li>experimental_aggregate_gradients:</li>
</ul></li>
<li>示例</li>
</ul>
<pre class="python"><code>grads = tape.gradient(loss, vars)
grads = tf.distribute.get_replica_context().all_reduce(&quot;sum&quot;, grads)

# Processing aggregated gradients.
optimizer.apply_gradients(zip(grad, vars), experimental_aggregate_gradients = False)</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>weights_property</li>
</ol>
<ul>
<li>语法</li>
</ul>
<pre class="python"><code>import tensorflow as tf

tf.keras.optimizers.Optimizer.weights</code></pre>
<ol start="3" style="list-style-type: decimal">
<li>get_weights</li>
</ol>
<ul>
<li>语法</li>
</ul>
<pre class="python"><code>Optimizer.get_weights()</code></pre>
<ul>
<li>示例</li>
</ul>
<pre class="python"><code># 模型优化器
opt = tf.keras.optimizers.RMSprop()

# 模型构建、编译
m = tf.keras.models.Sequential()
m.add(tf.keras.layers.Dense(10))
m.compile(opt, loss = &quot;mse&quot;)

# 数据
data = np.arange(100).reshape(5, 20)
labels = np.zeros(5)

# 模型训练
print(&quot;Training&quot;)
results = m.fit(data, labels)
print(opt.get_weights)</code></pre>
<ol start="4" style="list-style-type: decimal">
<li>set_weights</li>
</ol>
<ul>
<li>语法</li>
</ul>
<pre class="python"><code>Optimizer.set_weights(weights)</code></pre>
<ul>
<li>示例</li>
</ul>
<pre class="python"><code># 模型优化器
opt = tf.keras.optimizers.RMSprop()

# 模型构建、编译
m = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])
m.compile(opt, loss = &quot;mse&quot;)

# 数据        
data = np.arange(100).reshape(5, 20)
labels = np.zeros(5)

# 模型训练
print(&quot;Training&quot;)
results = m.fit(data, labels)

# 优化器新权重
new_weights = [
    np.array(10),       # 优化器的迭代次数
    np.ones([20, 10]),  # 优化器的状态变量
    np.zeros([10])      # 优化器的状态变量
]
opt.set_weights(new_weights)
opt.iteration</code></pre>
</div>
</div>
</div>
<div id="keras-网络层" class="section level1">
<h1>Keras 网络层</h1>
<div id="自定义层" class="section level2">
<h2>1.自定义层</h2>
<ul>
<li>自定义层需要继承 <code>tf.keras.layers.Layers</code> 类, 并重写 <code>__init__</code>、<code>build</code>、<code>call</code> 三个方法</li>
</ul>
<pre class="python"><code>import numpy as np
import tensorflow as tf

class MyLayer(tf.keras.layers.Layer):
    def __init__(self):
        super().__init__()
        # 初始化代码
    
    def build(self, input_shape): # input_shape 是一个 TensorShape 类型对象, 提供输入的形状
        # 在第一次使用该层的时候调用该部分代码, 在这里创建变量可以使得变量的形状自适应输入的形状
        # 而不需要使用者额外指定变量形状
        # 如果已经可以完全确定变量的形状, 也可以在 __init__ 部分创建变量
        self.variable_0 = self.add_weight(...)
        self.variable_1 = self.add_weight(...)
    
    def call(self, inputs):
        # 模型调用的代码(处理输入并返回输出)
        return output</code></pre>
<ul>
<li>线性层示例</li>
</ul>
<pre class="python"><code>import numpy as np
import tensorflow as tf

class LinearLayer(tf.keras.layers.Layer):
    def __init__(self, units):
        super.__init__()
        self.units = units
    
    def build(self, input_shape):
        self.w = self.add_variable(
            name = &quot;w&quot;, 
            shape = [input_shape[-1], self.units],  # [n, 1]
            initializer = tf.zeros_initializer()
        )
        self.b = self.add_variable(
            name = &quot;b&quot;,
            shape = [self.units],                   # [1]
            initializer = tf.zeros_initializer()
        )
    
    def call(self, inputs):
        y_pred = tf.matmul(inputs, self.w) + self.b
        return y_pred

class LinearModel(tf.keras.Model):
    def __init__(self):
        super().__init__()
        self.layer = LinearLayer(untis = 1)
    
    def call(self, inputs):
        output = self.layer(inputs)
        return output</code></pre>
</div>
<div id="keras-layers-共有的方法" class="section level2">
<h2>1.Keras Layers 共有的方法:</h2>
<pre class="python"><code>from keras import layers</code></pre>
<ul>
<li>layer.get_weights()</li>
<li>layer.set_weights(weights)</li>
<li>layer.get_config()
<ul>
<li>keras.layer.Dense.from_config(config)</li>
<li>keras.layer.deserialize({“class_name”: , “config”: config})</li>
</ul></li>
<li>如果 Layer 是单个节点(不是共享 layer), 可以使用以下方式获取 layer
的属性:
<ul>
<li>layer.input</li>
<li>layer.output</li>
<li>layer.input_shape</li>
<li>layer.output_shape</li>
</ul></li>
<li>如果 Layer 具有多个节点(共享 layer), 可以使用以下方式获取 layer
的属性:
<ul>
<li>layer.getinputat(note_index)</li>
<li>layer.getoutputat(note_index)</li>
<li>layer.getinputshapeat(noteindex)</li>
<li>layer.getoutputshaepat(noteindex)</li>
</ul></li>
</ul>
</div>
<div id="keras-layers" class="section level2">
<h2>2.Keras Layers</h2>
<ul>
<li><p><strong>Core Layers</strong></p>
<ul>
<li>Dense</li>
<li>Activation</li>
<li>Drop</li>
<li>Flatten</li>
<li>Input</li>
<li>Reshape
<ul>
<li><code>keras.layers.Reshape(target_shape)</code></li>
</ul></li>
<li>Permute</li>
<li>RepeatVector</li>
<li>Lambda</li>
<li>ActivityRegularization</li>
<li>Masking</li>
<li>SpatialDropout1D</li>
<li>SpatialDropout2D</li>
<li>SpatialDropout3D`</li>
</ul></li>
<li><p><strong>Convolutional Layers</strong></p>
<ul>
<li>卷积层
<ul>
<li>Conv1D</li>
<li>Conv2D</li>
<li>Conv3D</li>
<li>SeparableConv1D
<ul>
<li><code>keras.layers.SeparableConv1D(rate)</code></li>
</ul></li>
<li>SeparableConv2D</li>
<li>DepthwiseConv3D</li>
</ul></li>
<li>Transpose
<ul>
<li>Conv2DTranspose</li>
<li>Conv3DTranspose</li>
</ul></li>
<li>Cropping
<ul>
<li>Cropping1D</li>
<li>Cropping2D</li>
<li>Cropping3D</li>
</ul></li>
<li>UnSampling
<ul>
<li>UnSampling1D</li>
<li>UnSampling2D</li>
<li>UnSampling3D</li>
</ul></li>
<li>ZeroPadding
<ul>
<li>ZeroPadding1D</li>
<li>ZeroPadding2D</li>
<li>ZeroPadding3D</li>
</ul></li>
</ul></li>
<li><p><strong>Pooling Layers</strong></p>
<ul>
<li>最大池化
<ul>
<li><code>MaxPolling1D()</code></li>
<li><code>MaxPolling2D()</code></li>
<li><code>MaxPolling3D()</code></li>
<li><code>GlobalMaxPolling1D()</code></li>
<li><code>GlobalMaxPolling2D()</code></li>
<li><code>GlobalMaxPolling3D()</code></li>
</ul></li>
<li>平均池化
<ul>
<li><code>AveragePolling1D()</code></li>
<li><code>AveragePolling2D()</code></li>
<li><code>AveragePolling3D()</code></li>
<li><code>GlobalAveragePolling1D()</code></li>
<li><code>GlobalAveragePolling2D()</code></li>
<li><code>GlobalAveragePolling3D()</code></li>
</ul></li>
</ul></li>
<li><p><strong>Locally-connected Layers</strong></p>
<ul>
<li><code>LocallyConnected1D()</code></li>
<li><code>LocallyConnected2D()</code></li>
</ul></li>
<li><p><strong>Recurrent Layers</strong></p>
<ul>
<li>RNN
<ul>
<li><code>RNN()</code></li>
<li><code>SimpleRNN()</code></li>
<li><code>SimpleRNNCell()</code></li>
</ul></li>
<li>GRU
<ul>
<li><code>GRU()</code></li>
<li><code>GRUCell()</code></li>
</ul></li>
<li>LSTM
<ul>
<li><code>LSTM()</code></li>
<li><code>LSTMCell()</code></li>
<li><code>ConvLSTM2D()</code></li>
<li><code>ConvLSTM2DCell()</code></li>
</ul></li>
<li>CuDNN
<ul>
<li><code>CuDNNGRU()</code></li>
<li><code>CuDNNLSTM()</code></li>
</ul></li>
</ul></li>
<li><p><strong>Embedding Layers</strong></p>
<ul>
<li><code>Embedding()</code></li>
</ul></li>
<li><p><strong>Merge Layers</strong></p>
<ul>
<li><code>Add()</code></li>
<li><code>Subtract()</code></li>
<li><code>Multiply()</code></li>
<li><code>Average()</code></li>
<li><code>Maximum()</code></li>
<li><code>Minimum()</code></li>
<li><code>Concatenate()</code></li>
<li><code>Dot()</code></li>
<li><code>add()</code></li>
<li><code>subtract()</code></li>
<li><code>multiply()</code></li>
<li><code>average()</code></li>
<li><code>maximum()</code></li>
<li><code>minimum()</code></li>
<li><code>concatenate()</code></li>
<li><code>dot()</code></li>
</ul></li>
<li><p><strong>Advanced Activations Layers</strong></p>
<ul>
<li><code>LeakyReLU()</code></li>
<li><code>PReLU()</code></li>
<li><code>ELU()</code></li>
<li><code>ThresholdedReLU()</code></li>
<li><code>Softmax()</code></li>
<li><code>ReLU()</code></li>
<li>Activation Functions</li>
</ul></li>
<li><p><strong>Normalization Layers</strong></p>
<ul>
<li><code>BatchNormalization()</code></li>
</ul></li>
<li><p><strong>Nosise Layers</strong></p>
<ul>
<li><code>GaussianNoise()</code></li>
<li><code>GaussianDropout()</code></li>
<li><code>AlphaDropout()</code></li>
</ul></li>
<li><p><strong>Others</strong></p>
<ul>
<li>Layer wrapper
<ul>
<li><code>TimeDistributed()</code></li>
<li><code>Bidirectional()</code></li>
</ul></li>
<li>Writting Customilize Keras Layers
<ul>
<li><code>build(input_shape)</code></li>
<li><code>call(x)</code></li>
<li><code>compute_output_shape(input_shape)</code></li>
</ul></li>
</ul></li>
</ul>
</div>
<div id="keras-layers-配置" class="section level2">
<h2>3.Keras Layers 配置</h2>
<pre class="python"><code>model.add(Layer(
      # 输出、输出
      output_dim,
      input_dim,
      # 参数初始化
      kernel_initializer,
      bias_initializer,
      # 参数正则化
      kernel_regularizer,
      activity_regularizer,
      # 参数约束
      kernel_constraint,
      bias_constraint,
      # 层激活函数
      activation,
))
# 输出
input_s = Input()
# 激活函数
model.add(Activation)</code></pre>
<div id="activation-function" class="section level3">
<h3>3.1 Activation Function</h3>
<ul>
<li>Keras Activations
<ul>
<li><code>Activation</code> layer</li>
<li><code>activation</code> argument supported by all forward layers</li>
</ul></li>
</ul>
<p><strong>调用方法:</strong></p>
<pre class="python"><code>from keras.layers import Activation, Dense
from keras import backend as K

# method 1
model.add(Dense(64))
model.add(Activation(&quot;tanh&quot;))

# method 2
model.add(Dense(64, activation = &quot;tanh&quot;))

# method 3
model.add(Dense(64, activation = K.tanh))</code></pre>
</div>
<div id="可用的-activations" class="section level3">
<h3>3.2 可用的 activations</h3>
<ul>
<li>softmax: Softmax activation function
<ul>
<li>x =&gt;</li>
<li><code>keras.activatons.softmax(x, axis = 1)</code></li>
</ul></li>
<li>relu: Rectified Linear Unit
<ul>
<li>x =&gt; max(x, 0)</li>
<li><code>keras.activations.relu(x, alpha = 0.0, max_value = None, threshold = 0.0)</code></li>
</ul></li>
<li>tanh: Hyperbolic tangent activation function
<ul>
<li><code>keras.activations.tanh(x)</code></li>
</ul></li>
<li>sigmoid: Sigmoid activation function
<ul>
<li>x =&gt; 1/(1 + exp(-x))</li>
<li><code>keras.activations.sigmoid(x)</code></li>
</ul></li>
<li>linear: Linear activation function
<ul>
<li>x =&gt; x</li>
<li><code>keras.activations.linear(x)</code></li>
</ul></li>
</ul>
</div>
<div id="keras-参数初始化initializers" class="section level3">
<h3>3.3 Keras 参数初始化(Initializers)</h3>
<p><strong>Initializers 的使用方法:</strong></p>
<p>初始化定义了设置 Keras Layer 权重随机初始的方法</p>
<ul>
<li><p><code>kernel_initializer</code> param</p>
<ul>
<li>“random_uniform”</li>
</ul></li>
<li><p><code>bias_initializer</code> param</p></li>
</ul>
<p><strong>可用的 Initializers:</strong></p>
<ul>
<li>keras.initializers.Initializer()
<ul>
<li>基类</li>
</ul></li>
<li>keras.initializers.Zeros()
<ul>
<li><code>0</code></li>
</ul></li>
<li>keras.initializers.Ones()
<ul>
<li><code>1</code></li>
</ul></li>
<li>keras.initializers.Constant()
<ul>
<li>keras.initializers.Constant(value = 0)
<ul>
<li><code>0</code></li>
</ul></li>
<li>keras.initializers.Constant(value = 1)
<ul>
<li><code>1</code></li>
</ul></li>
</ul></li>
<li>keras.initializers.RandomNormal(mean = 0.0, stddev = 0.05, seed =
None)
<ul>
<li>正态分布</li>
</ul></li>
<li>keras.initializers.RandomUniform(minval = 0.05, maxval = 0.05, seed =
None)
<ul>
<li>均匀分布</li>
</ul></li>
<li>keras.initializers.TruncatedNormal(mean = 0.0, stddev = 0.05, seed =
None)
<ul>
<li>截尾正态分布:生成的随机值与 <code>RandomNormal</code>
生成的类似, 但是在距离平均值两个标准差之外的随机值将被丢弃并重新生成。这是用来生成神经网络权重和滤波器的推荐初始化器</li>
</ul></li>
<li>keras.initializers.VarianveScaling(scale = 1.0, mode = “fan_in”,
distribution = “normal”, seed = None)
<ul>
<li>根据权值的尺寸调整其规模</li>
</ul></li>
<li>keras.initializers.Orthogonal(gain = 1.0, seed = None)
<ul>
<li><code>随机正交矩阵 &lt;http://arxiv.org/abs/1312.6120&gt;</code>__</li>
</ul></li>
<li>keras.initializers.Identity(gain = 1.0)
<ul>
<li>生成单位矩阵的初始化器。仅用于 2D 方阵</li>
</ul></li>
<li>keras.initializers.lecun_normal()
<ul>
<li>LeCun 正态分布初始化器</li>
<li>它从以 0 为中心, 标准差为 stddev = sqrt(1 / fanin)
的截断正态分布中抽取样本, 其中 fanin
是权值张量中的输入单位的数量</li>
</ul></li>
<li>keras.initializers.lecun_uniform()
<ul>
<li>LeCun 均匀初始化器</li>
<li>它从 [-limit, limit] 中的均匀分布中抽取样本, 其中 limit 是 sqrt(3
/ fanin), fanin 是权值张量中的输入单位的数量</li>
</ul></li>
<li>keras.initializers.glorot_normal()
<ul>
<li>Glorot 正态分布初始化器, 也称为 Xavier 正态分布初始化器</li>
<li>它从以 0 为中心, 标准差为 stddev = sqrt(2 / (fan*in + fanout))
的截断正态分布中抽取样本, 其中 fanin
是权值张量中的输入单位的数量, fanout
是权值张量中的输出单位的数量</li>
</ul></li>
<li>keras.initializers.glorot_uniform()
<ul>
<li>Glorot 均匀分布初始化器, 也称为 Xavier 均匀分布初始化器</li>
<li>它从 [-limit, limit] 中的均匀分布中抽取样本, 其中 limit 是 sqrt(6
/ (fan*in + fanout)), fanin 是权值张量中的输入单位的数量,
fanout 是权值张量中的输出单位的数量</li>
</ul></li>
<li>keras.initializers.he_normal()
<ul>
<li>He 正态分布初始化器</li>
<li>它从以 0 为中心, 标准差为 stddev = sqrt(2 / fanin)
的截断正态分布中抽取样本, 其中 fanin
是权值张量中的输入单位的数量</li>
</ul></li>
<li>keras.initializers.he_uniform()
<ul>
<li>He 均匀分布方差缩放初始化器</li>
<li>它从 :math:<code>[-limit, limit]</code> 中的均匀分布中抽取样本, 其中
:math:<code>limit</code> 是 :math:<code>sqrt(6 / fan_in)</code> , 其中 fan_in
是权值张量中的输入单位的数量</li>
</ul></li>
<li>自定义 Initializer</li>
</ul>
<pre class="python"><code>from keras import backend as K

def my_init(shape, dtype = None):
      return K.random_normal(shape, dtype = dtype)

model.add(Dense(64, kernel_initializer = my_init))</code></pre>
</div>
<div id="keras-正则化regularizers" class="section level3">
<h3>3.4 Keras 正则化(Regularizers)</h3>
<p>正则化器允许在优化过程中对 <code>层的参数</code> 或 <code>层的激活函数</code> 情况进行惩罚, 并且神经网络优化的损失函数的惩罚项也可以使用</p>
<p>惩罚是以层为对象进行的。具体的 API 因层而异, 但 Dense, Conv1D, Conv2D 和
Conv3D 这些层具有统一的 API</p>
<p><strong>Regularizers 的使用方法:</strong></p>
<ul>
<li>[class] keras.regularizers.Regularizer
<ul>
<li>[instance] <code>kernel_regularizer</code> param</li>
<li>[instance] <code>bias_regularizer</code> param</li>
<li>[instance] <code>activity_regularizer</code> param</li>
</ul></li>
</ul>
<p><strong>可用的 Regularizers:</strong></p>
<ul>
<li>keras.regularizers.l1(0.)</li>
<li>keras.regularizers.l2(0.)</li>
<li>keras.regularizers.l1_l2(l1 = 0.01, l2 = 0.01)</li>
<li>自定义的 Regularizer:
<ul>
<li><code>def l1_reg: pass</code></li>
</ul></li>
</ul>
</div>
<div id="keras-约束constraints" class="section level3">
<h3>3.5 Keras 约束(Constraints)</h3>
<p><code>constraints</code> 模块的函数允许在优化期间对网络参数设置约束(例如非负性)。</p>
<p>约束是以层为对象进行的。具体的 API 因层而异, 但 Dense, Conv1D, Conv2D 和
Conv3D 这些层具有统一的 API</p>
<p><strong>Constraints 的使用方法:</strong></p>
<ul>
<li>kernel_constraint</li>
<li>bias_constraint</li>
</ul>
<p><strong>可用的 Constraints:</strong></p>
<ul>
<li>keras.constraints.MaxNorm(max_value = 2, axis = 0)
<ul>
<li>最大范数权值约束</li>
</ul></li>
<li>keras.constraints.NonNeg()
<ul>
<li>权重非负的约束</li>
</ul></li>
<li>keras.constraints.UnitNorm()
<ul>
<li>映射到每个隐藏单元的权值的约束, 使其具有单位范数</li>
</ul></li>
<li>keras.constraints.MinMaxNorm(minvalue = 0, maxvalue = 1.0, rate
= 1.0, axis = 0)
<ul>
<li>最小/最大范数权值约束:映射到每个隐藏单元的权值的约束, 使其范数在上下界之间</li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="tensorflow-tensorboard" class="section level1">
<h1>TensorFlow TensorBoard</h1>
<div id="实时查看参数变化情况" class="section level2">
<h2>1.实时查看参数变化情况</h2>
<div id="tensorboard-使用介绍" class="section level3">
<h3>1.1 TensorBoard 使用介绍</h3>
<p>1.首先, 在代码目录下建立一个文件夹, 存放 TensorBoard 的记录文件</p>
<pre class="bash"><code>$ mkdir tensorboard</code></pre>
<p>2.在代码中实例化一个记录器</p>
<pre class="python"><code>
      summary_writer =  tf.summary.create_file_writer(&quot;./tensorboard&quot;)

3.当需要记录训练过程中的参数时, 通过 `with` 语句指定希望使用的记录器, 并对需要记录的参数(一般是标量)运行:

```python
with summary_writer.as_default():
   tf.summary.scalar(name, tensor, step = batch_index)</code></pre>
<p>4.当要对训练过程可视化时, 在代码目录打开终端</p>
<pre class="bash"><code>$ tensorboard --logdir=./tensorboard</code></pre>
<p>5.使用浏览器访问命令行程序所输出的网址, 即可访问 TensorBoard 的可视化界面</p>
<ul>
<li><code>http://计算机名称:6006</code></li>
</ul>
<p>.. note::</p>
<ul>
<li>每运行一次 <code>tf.summary.scalar()</code>, 记录器就会向记录文件中写入一条记录</li>
<li>除了最简单的标量以外, TensorBoard 还可以对其他类型的数据, 如:图像、音频等进行可视化</li>
<li>默认情况下, TensorBoard 每 30 秒更新一次数据, 可以点击右上角的刷新按钮手动刷新</li>
<li>TensorBoard 的使用有以下注意事项:
<ul>
<li>如果需要重新训练, 那么删除掉记录文件夹内的信息并重启 TensorBoard,
或者建立一个新的记录文件夹并开启 TensorBoard, 将 <code>--logdir</code> 参数设置为新建里的文件夹</li>
<li>记录文件夹目录许保持全英文</li>
</ul></li>
</ul>
</div>
<div id="tensorboard-代码框架" class="section level3">
<h3>1.2 TensorBoard 代码框架</h3>
<pre class="python"><code>
# (1)实例化一个记录器
summary_writer =  tf.summary.create_file_writer(&quot;./tensorboard&quot;)

# (2)开始训练模型
for batch_index in range(num_batches):
# ...(训练代码, 将当前 batch 的损失值放入变量 loss 中)

# (3)指定记录器
with summary_writer.as_default():
   tf.summary.scalar(&quot;loss&quot;, loss, step = batch_index)
   tf.summary.scalar(&quot;MyScalar&quot;, my_scalar, step = batch_index)</code></pre>
</div>
</div>
<div id="查看-graph-和-profile-信息" class="section level2">
<h2>2.查看 Graph 和 Profile 信息</h2>
<p>在训练时使用 <code>tf.summary.trace_on</code> 开启 Trace, 此时 TensorFlow 会将训练时的大量信息,
如:计算图的结构、每个操作所耗费的时间等, 记录下来。</p>
<p>在训练完成后, 使用 <code>tf.summary.trace_export</code> 将记录结果输出到文件。</p>
<p>1.使用 TensorBoard 代码框架对模型信息进行跟踪记录</p>
<pre class="python"><code>
# (1)实例化一个记录器
summary_writer =  tf.summary.create_file_writer(&quot;./tensorboard&quot;)

# (2)开启 Trace, 可以记录图结构和 profile 信息
tf.summary.trace_on(graph = True, profiler = True)

# (3)开始训练模型
for batch_index in range(num_batches):
   # (4)...(训练代码, 将当前 batch 的损失值放入变量 loss 中)
   
   # (5)指定记录器, 将当前指标值写入记录器
   with summary_writer.as_default():
      tf.summary.scalar(&quot;loss&quot;, loss, step = batch_index)
      tf.summary.scalar(&quot;MyScalar&quot;, my_scalar, step = batch_index)

# (6)保存 Trace 信息到文件
with summary_writer.as_default():
   tf.summary.trace_export(name = &quot;model_trace&quot;, step = 0, profiler_outdir = log_dir)</code></pre>
<p>2.在 TensorBoard 的菜单中选择 <code>PROFILE</code>, 以时间轴方式查看各操作的耗时情况,
如果使用了 <code>@tf.function</code> 建立计算图, 也可以点击 <code>GRAPHS</code> 查看图结构</p>
</div>
</div>
<div id="tensorflow-serving" class="section level1">
<h1>TensorFLow Serving</h1>
<div id="tensorflow-serving-安装" class="section level2">
<h2>1.TensorFLow Serving 安装</h2>
</div>
<div id="tensorflow-serving-模型部署" class="section level2">
<h2>2.TensorFLow Serving 模型部署</h2>
</div>
<div id="在客户端调用以-tensorflow-serving-部署的模型" class="section level2">
<h2>3.在客户端调用以 TensorFLow Serving 部署的模型</h2>
<p>TensorFLow Serving 支持使用 gRPC 方法和 RESTful API 方法调用以
TensorFLow Serving 部署的模型。</p>
<p>RESTful API 以标准的 HTTP POST 方法进行交互, 请求和回复均为 JSON 对象。为了调用服务器端的模型, 在客户端向服务器发送以下格式的请求.</p>
<ul>
<li>服务器 URI: <code>http://服务器地址:端口号/v1/models/模型名:predict</code></li>
<li>请求内容</li>
</ul>
<pre class="json"><code>{
    &quot;signature_name&quot;: &quot;需要调用的函数签名(Sequential模式不需要)&quot;,
    &quot;instances&quot;: &quot;输入数据&quot;
}</code></pre>
<ul>
<li>回复:</li>
</ul>
<pre class="json"><code>{
    &quot;predictions&quot;: &quot;返回值&quot;
}</code></pre>
</div>
</div>
<div id="tensorflow-savemodel" class="section level1">
<h1>TensorFlow SaveModel</h1>
<p>为了将训练好的机器学习模型部署到各个目标平台(如服务器、移动端、嵌入式设备和浏览器等),
我们的第一步往往是将训练好的整个模型完整导出(序列化)为一系列标准格式的文件。在此基础上,
我们才可以在不同的平台上使用相对应的部署工具来部署模型文件。</p>
<p>TensorFlow 提供了统一模型导出格式 <code>SaveModel</code>, 这是我们在 TensorFlow 2 中主要使用的导出格式。
这样我们可以以这一格式为中介, 将训练好的模型部署到多种平台上.</p>
<p>同时, 基于历史原因, Keras 的 Sequential 和 Functional 模式也有自有的模型导出格式。</p>
<div id="tf.train.checkpoint-变量的保存与恢复" class="section level2">
<h2>1.tf.train.Checkpoint: 变量的保存与恢复</h2>
<p>很多时候, 希望在模型训练完成后能将训练好的参数(变量)保存起来, 这样在需要使用模型的其他地方载入模型和参数,
就能直接得到训练好的模型, 保存模型有很多中方式:</p>
<ul>
<li><p>Python 的序列化模块 <code>pickle</code> 存储 <code>model.variables</code></p>
<ul>
<li>然而, TensorFlow 的变量类型 <code>ResourceVariable</code> 并不能被序列化</li>
<li>语法:</li>
</ul></li>
</ul>
<pre class="python"><code>import pickle</code></pre>
<div id="tf.train.checkpoint-介绍" class="section level3">
<h3>1.1 tf.train.Checkpoint 介绍</h3>
<ul>
<li><code>tf.train.Checkpoint</code> 简介</li>
</ul>
<p>TensorFlow 提供了 <code>tf.train.Checkpoint</code> 这一强大的变量保存与恢复类, 提供的方法可以保存和恢复 TensorFlow 中的大部分对象,
比如下面类的实例都可以被保存:</p>
<ul>
<li><p><code>tf.keras.optimizer</code></p></li>
<li><p><code>tf.Variable</code></p></li>
<li><p><code>tf.keras.Layer</code></p></li>
<li><p><code>tf.keras.Model</code></p></li>
<li><p>Checkpointable State 的对象</p></li>
<li><p><code>tf.train.Checkpoint</code> 使用方法</p></li>
<li><p>方法:</p>
<ul>
<li><code>save()</code></li>
<li><code>restore()</code></li>
</ul></li>
<li><p>语法:</p></li>
</ul>
<pre class="python"><code># 保存训练好的模型, 先声明一个 Checkpoint
model = TrainedModel()
checkpoint = tf.train.Checkpoint(myAwesomeModel = model, myAwesomeOptimizer = optimizer)
checkpoint.save(save_path_with_prefix)

# 载入保存的训练模型
model_to_be_restored = MyModel()  # 待恢复参数的同一模型
checkpoint = tf.train.Checkpoint(myAwesomeModel = model_to_be_restored)
checkpoint.restore(save_path_with_prefix_and_index)

# 为了载入最近的一个模型文件, 返回目录下最近一次检查点的文件名
tf.train.latest_checkpoint(save_path)</code></pre>
<p>.. note::</p>
<ul>
<li><p>参数:</p>
<ul>
<li><code>myAwesomeModel</code>: 待保存的模型 model 所取的任意键名, 在恢复变量时还将使用这一键名</li>
<li><code>myAwesomeOptimizer</code>: 待保存的模型 optimizer 所取的任意键名, 在恢复变量时还将使用这一键名</li>
<li><code>save_path_with_prefix</code>: 保存文件的目录+前缀</li>
<li><code>save_path_with_prefix_and_index</code>: 之前保存的文件目录+前缀+序号</li>
</ul></li>
<li><p><code>checkpoint.save("./model_save/model.ckpt")</code>: 会在模型保存的文件夹中生成三个文件:</p>
<ul>
<li><code>checkpoint</code></li>
<li><code>model.ckpt-1.index</code></li>
<li><code>model.ckpt-1.data-00000-of-00001</code></li>
</ul></li>
<li><p><code>checkpoint.restore("./model/save/model.ckpt-1")</code></p>
<ul>
<li>载入前缀为 <code>model.ckpt</code>、序号为 <code>1</code> 的文件来恢复模型</li>
</ul></li>
</ul>
</div>
<div id="tf.train.checkpoint-代码框架" class="section level3">
<h3>1.2 tf.train.Checkpoint 代码框架</h3>
<p>1.train.py 模型训练阶段</p>
<pre class="python"><code>
# 训练好的模型
model = MyModel()

# 实例化 Checkpoint, 指定保存对象为 model(如果需要保存 Optimizer 的参数也可以加入)
checkpoint = tf.train.Checkpoint(myModel = model)
manager = tf.train.CheckpointManager(checkpoint, directory = &quot;./save&quot;, checkpoint_name = &quot;model.ckpt&quot;, max_to_keep = 10)

# ...(模型训练代码)

# 模型训练完毕后将参数保存到文件(也可以在模型训练过程中每隔一段时间就保存一次)
if manager:
    manager.save(checkpoint_number = 100)
else:
    checkpoint.save(&quot;./save/model.ckpt&quot;)</code></pre>
<p>2.test.py 模型使用阶段</p>
<pre class="python"><code>
# 要使用的模型
model = MyModel()

# 实例化 Checkpoint, 指定恢复对象为 model
checkpoint = tf.train.Checkpoint(myModel = model)

# 从文件恢复模型参数
checkpoint.restore(tf.train.latest_checkpoint(&quot;./save))

# ...(模型使用代码)</code></pre>
<p>.. note::</p>
<ul>
<li><code>tf.train.Checkpoint</code> (检查点)只保存模型的参数, 不保存模型的计算过程,
因此一般用于在具有的模型源码时恢复之前训练好的模型参数。如果需要导出模型(无须源代码也能运行模型)。</li>
</ul>
</div>
</div>
<div id="使用-savemodel-完整导出模型" class="section level2">
<h2>2.使用 SaveModel 完整导出模型</h2>
<p>作为模型导出格式的 <code>SaveModel</code> 包含了一个 TensorFlow 程序的完整信息: 不仅包含参数的权值, 还包含计算的流程(计算图)。
当模型导出为 SaveModel 文件时, 无须模型的源代码即可再次运行模型, 这使得 <code>SaveModel</code> 尤其适用于模型的分享和部署。</p>
<p>Keras 模型均可以方便地导出为 <code>SaveModel</code> 格式。不过需要注意的是, 因为 <code>SaveModel</code> 基于计算图,
所以对于通过继承 <code>tf.keras.Model</code> 类建立的 Keras 模型来说, 需要导出为 <code>SaveModel</code> 格式的方法(比如 call) 都需要
使用 <code>@tf.function</code> 修饰。</p>
<p>语法:</p>
<pre class="python"><code># 保存
tf.saved_model.save(model, &quot;保存的目标文件夹名称&quot;)

# 载入
model = tf.saved_model.load(&quot;保存的目标文件夹名称&quot;)</code></pre>
<p>示例:</p>
<pre class="python"><code></code></pre>
</div>
<div id="keras-自有的模型导出格式" class="section level2">
<h2>3.Keras 自有的模型导出格式</h2>
<p>示例:</p>
<pre class="bash"><code>curl -LO https://raw.githubcontent.com/keras-team/keras/master/examples/mnist_cnn.py</code></pre>
<pre class="python"><code>model.save(&quot;mnist_cnn.h5&quot;)</code></pre>
<pre class="python"><code>
import keras

keras.models.load_model(&quot;mnist_cnn.h5&quot;)</code></pre>
</div>
</div>
<div id="tensorflow-performance" class="section level1">
<h1>TensorFlow Performance</h1>
<div id="使用-tf.function-提升性能" class="section level2">
<h2>1.使用 tf.function 提升性能</h2>
<div id="tf.funciton-图执行模式" class="section level3">
<h3>1.1 <span class="citation">@tf.funciton</span>: 图执行模式</h3>
<p>虽然目前 TensorFlow 默认的即时执行模式具有灵活及易调试的特性, 但在特定的场合,
例如追求高性能或部署模型时, 依然希望使用图执行模式, 将模型转换为高效的 TensorFlow 图模型。</p>
<p>TensorFlow 2 提供了 `<code>bashtf.function</code> 模块, 结合 AutoGraph 机制, 使得我们仅需加入一个简单的
<code>@tf.function</code> 修饰符, 就能轻松将模型以图执行模式运行。</p>
</div>
<div id="tf.function-基础使用方法" class="section level3">
<h3>1.2 <span class="citation">@tf.function</span> 基础使用方法</h3>
<p><code>@tf.function</code> 的基础使用非常简单, 只需要将我们希望以图执行模式运行的代码封装在一个函数内,
并在函数前面加上 <code>@tf.function</code> 即可.</p>
</div>
<div id="tf.function-内在机制" class="section level3">
<h3>1.3 <span class="citation">@tf.function</span> 内在机制</h3>
</div>
<div id="autograph-将-python-控制流转化为-tensorflow-计算图" class="section level3">
<h3>1.4 AutoGraph: 将 Python 控制流转化为 TensorFlow 计算图</h3>
</div>
<div id="使用传统的-tf.session" class="section level3">
<h3>1.5 使用传统的 tf.Session</h3>
</div>
</div>
<div id="分析-tenforflow-的性能" class="section level2">
<h2>2.分析 TenforFlow 的性能</h2>
</div>
<div id="图优化" class="section level2">
<h2>3.图优化</h2>
</div>
<div id="混合精度" class="section level2">
<h2>4.混合精度</h2>
</div>
</div>
<div id="tensorflow-estimator" class="section level1">
<h1>TensorFlow Estimator</h1>
<ul>
<li>一种可极大地简化机器学习编程的高阶TensorFlow API;</li>
<li>Estimator封装的操作:
<ul>
<li>训练</li>
<li>评估</li>
<li>预测</li>
<li>导出以使用</li>
</ul></li>
<li>Estimator优势:
<ul>
<li>可以在本地主机上或分布式多服务器环境中运行基于 Estimator
的模型, 而无需更改模型。此外, 可以在 CPU、GPU 或 TPU 上运行基于
Estimator 的模型, 而无需重新编码模型</li>
<li>Estimator 简化了在模型开发者之间共享实现的过程</li>
<li>可以使用高级直观代码开发先进的模型。简言之, 采用 Estimator
创建模型通常比采用低阶 TensorFlow API 更简单</li>
<li>Estimator 本身在 tf.layers 之上构建而成, 可以简化自定义过程</li>
<li>Estimator 会为您构建图</li>
<li>Estimator 提供安全的分布式训练循环, 可以控制如何以及何时:
<ul>
<li>构建图</li>
<li>初始化变量</li>
<li>开始排队</li>
<li>处理异常</li>
<li>创建检查点文件并从故障中恢复</li>
<li>保存 TensorBoard 的摘要</li>
</ul></li>
</ul></li>
</ul>
<div id="预创建的estimator" class="section level2">
<h2>1.预创建的Estimator</h2>
<p><strong>预创建的 Estimator 程序的结构</strong></p>
<p><strong>依赖预创建的Estimator的TensorFlow程序通常包含下列四个步骤:</strong></p>
<ol style="list-style-type: decimal">
<li>编写一个或多个数据集导入函数
<ul>
<li>创建一个函数来导入训练集, 并创建另一个函数来导入测试集。每个数据集导入函数都必须返回两个对象:
<ul>
<li>一个字典, 其中键是特征名称, 值是包含相应特征数据的张量(or Sparse Tensro);</li>
<li>一个包含一个或多个标签的张量;</li>
</ul></li>
</ul></li>
<li>定义特征列
<ul>
<li>每个 <code>tf.feature_column</code> 都标识了特征名称、特征类型和任何输入预处理操作</li>
</ul></li>
<li>实例化相关的预创建的Estimator
<ul>
<li>LinearClassifier</li>
</ul></li>
<li>调用训练、评估或推理方法
<ul>
<li>所有Estimator都提供训练模型的 <code>train</code> 方法</li>
</ul></li>
</ol>
<p><strong>上面步骤实现举例:</strong></p>
<pre class="python"><code>def input_fn_train(dataset):
   # manipulate dataset, extracting the feature dict and the label
   
   return feature_dict, label

def input_fn_test(dataset):
   # manipulate dataset, extracting the feature dict and the label
   
   return feature_dict, label


my_training_set = input_fn_train()
my_testing_set = input_fn_test()

population = tf.feature_column.numeric_column(&#39;population&#39;)
crime_rate = tf.feature_column.numeric_column(&#39;crime_rate&#39;)
median_education = tf.feature_column.numeric_column(&#39;median_education&#39;, 
                                                   normalizer_fn = lambda x: x - global_education_mean)

estimator = tf.estimator.LinearClassifier(
   feature_columns = [population, crime_rate, median_education],
)

estimator.train(input_fn = my_training_set, setps = 2000)</code></pre>
<p><strong>预创建的 Estimator 的优势</strong></p>
<ul>
<li>预创建的 Estimator 会编码最佳做法, 从而具有下列优势:
<ul>
<li>确定计算图不同部分的运行位置以及在单台机器或多台机器上实现策略的最佳做法。</li>
<li>事件(汇总)编写和普遍有用的汇总的最佳做法。</li>
</ul></li>
</ul>
</div>
<div id="自定义的estimator" class="section level2">
<h2>2.自定义的Estimator</h2>
<ul>
<li><p>每个
Estimator(无论是预创建还是自定义)的核心都是其模型函数, 这是一种为训练、评估和预测构建图的方法。如果您使用预创建的
Estimator, 则有人已经实现了模型函数。如果您使用自定义
Estimator, 则必须自行编写模型函数。</p></li>
<li><p>推荐的工作流程:</p>
<ul>
<li>1.假设存在合适的预创建的Estimator, 使用它构建第一个模型并使用其结果确定基准;</li>
<li>2.使用此预创建的Estimator构建和测试整体管道, 包括数据的完整性和可靠性;</li>
<li>3.如果存在其他合适的预创建的Estimator, 则运行试验来确定哪个预创建的Estimator效果好;</li>
<li>4.可以通过构建自定义的Estimator进一步改进模型;</li>
</ul></li>
</ul>
</div>
<div id="从-keras-模型创建-estimator" class="section level2">
<h2>3.从 Keras 模型创建 Estimator</h2>
<ul>
<li>可以将现有的Keras的模型转换为Estimator, 这样Keras模型就可以利用Estimator的优势, 比如进行分布式训练;</li>
</ul>
<pre class="python"><code>keras_inception_v3 = tf.keras.applications.keras_inception_v3.InceptionV3(weights = None)

keras_inception_v3.compile(optimizer = tf.keras.optimizers.SGD(lr = 0.0001, momentum = 0.9),
                           loss = &#39;categorical_crossentropy&#39;,
                           metric = &#39;accuracy&#39;)

est_inception_v3 = tf.keras.estimator.model_to_estimator(keras_model = keras_inception_v3)

keras_inception_v3.input_names

train_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(
      x = {&#39;input_1&#39;: train_data},
      y = train_labels,
      num_epochs = 1,
      shuffle = False
)

est_inception_v3.train(input_fn = train_input_fn, steps = 2000)</code></pre>
<p><strong>API:</strong></p>
<p>从一个给定的Keras模型中构造一个Estimator实例</p>
<pre class="python"><code>tf.keras.estimator.model_to_estimator(
      keras_model = None,
      keras_model_path = None,
      custom_objects = None,
      model_dir = None,
      config = None
)</code></pre>
</div>
</div>
<div id="todo" class="section level1">
<h1>TODO</h1>
<ul>
<li>Keras Sequential</li>
<li>模型假设, 网路只有一个输入和一个输出, 而且网络是层的线性堆叠;</li>
<li>有些网络需要多个独立的输入, 有些网络则需要多个输出, 而有些网络在层与层之间具有内部分支, 这样的网络看起来像是层构成的图(graph), 而不是层的线性堆叠;</li>
<li>多模态(multimodal)输入</li>
<li>元数据</li>
<li>文本描述</li>
<li>图片</li>
<li>预测输入数据的多个目标属性</li>
<li>类别</li>
<li>连续值</li>
<li>非线性地网络拓扑结构, 网络结构是有向无环图</li>
<li>Inception 系列网络
<ul>
<li>输入被多个并行的卷积分支所处理, 然后将这些分支的输出合并为单个张量;</li>
</ul></li>
<li>ResNet 系列网络
<ul>
<li>向模型中添加残差连接(residual connection), 将前面的输出张量与后面的输出张量相加,
从而将前面的表示重新注入下游数据流中, 这有助于防止信息处理流程中的信息损失;</li>
</ul></li>
</ul>
<div id="多输入模型" class="section level2">
<h2>1.多输入模型</h2>
<ul>
<li><p>Keras 函数式 API</p>
<ul>
<li>可以构建具有多个输入的模型, 通常情况下, 这种模型会在某一时刻用一个可以组合多个张量的层将不同输入分支合并, 张量组合方式可能是相加, 连接等, 比如:</li>
</ul></li>
<li><p><code>keras.layers.add</code></p></li>
<li><p><code>keras.layers.concatenate</code></p></li>
<li><p>问答模型:</p></li>
<li><p>输入:</p>
<ul>
<li>自然语言描述的问题</li>
<li>文本片段, 提供用于回答问题的信息</li>
</ul></li>
<li><p>输出</p>
<ul>
<li>一个回答, 在最简单的情况下, 这个回答只包含一个词, 可以通过对某个预定义的词表做softmax得到;</li>
</ul></li>
</ul>
<pre class="python"><code>import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import *

from tensorflow.keras.model import Model
# from keras.model import Model
from tensorflow.keras import layers, Input
# from keras import layers, Input

# =========================================================================
# 构建模型
# =========================================================================
text_vocabulary_size = 10000
question_vocabulary_size = 10000
answer_vocabulary_size = 500
# 文本片段
text_input = Input(
   shape = (None,), 
   dtype = &quot;int32&quot;, 
   name = &quot;text&quot;
)
embedded_text = layers.Embedding(text_vocabulary_size, 64)(text_input)
encoded_text = layres.LSTM(32)(embedded_text)
# 自然语言描述的问题
question_input = Input(
   shape = (None,),
   dtype = &quot;int32&quot;,
   name = &quot;question&quot;
)
embedded_question = layers.Embedding(question_vocabulary_size, 32)(question_input)
encoded_question = layers.LSTM(16)(embedded_question)

concatenated = layers.concatenate([encoded_text, encoded_question], axis = -1)

answer = layers.Dense(answer_vocabulary_size, activation = &quot;softmax&quot;)(concatenated)

model = Model(inputs = [text_input, question_input], outputs = answer)

model.compile(
   optimizer = &quot;rmsprop&quot;,
   loss = &quot;categorical_crossentropy&quot;,
   metrics = [&quot;acc&quot;]
)

# =========================================================================
# 训练模型
# =========================================================================
import numpy as np
num_samples = 1000
max_length = 100
text = np.random.randint(1, text_vocabulary_size, size = (num_samples, max_length))
question = np.random.randint(1, question_vocabulary_size, size = (num_samples, max_length))
answers = np.random.randint(answer_vocabulary_size, size = (num_samples))

answers = keras.utils.to_categorical(answers, answer_vocabulary_size)

model.fit([text, question], answers, epochs = 10, batch_size = 128)
model.fit(
   {
      &quot;text&quot;: text,
      &quot;question&quot;: question,
   },
   answers,
   epochs = 10,
   batch_size = 128
)</code></pre>
</div>
<div id="多输出模型" class="section level2">
<h2>2.多输出模型</h2>
<p>网络同时预测数据的不同性质</p>
<pre class="python"><code>from keras import layers, Input
from keras.models import Model

vocabulary_size = 50000
num_income_groups = 10

# 输入层
posts_input = Input(shape = (None,), dtype = &quot;int32&quot;, name = &quot;posts&quot;)
embedded_posts = layers.Embedding(256, vocabulary_size)(posts_input)
# 隐藏层
x = layers.Conv1D(128, 5, activation = &quot;relu&quot;)(embedded_posts)
x = layers.MaxPooling1D(5)(x)
x = layers.Conv1D(256, 5, activation = &quot;relu&quot;)(x)
x = layers.Conv1D(256, 5, activation = &quot;relu&quot;)(x)
x = layers.MaxPooling1D(5)(x)
x = layers.Conv1D(256, 5, activation = &quot;relu&quot;)(x)
x = layers.Conv1D(256, 5, activation = &quot;relu&quot;)(x)
x = layers.GlobalMaxPooling1D()(x)
x = layers.Dense(128, activation = &quot;relu&quot;)(x)
# 输出层
age_prediction = layers.Dense(1, name = &quot;age&quot;)(x)
income_prediction = layers.Dense(num_income_groups, activation = &quot;softmax&quot;, name = &quot;income&quot;)(x)
gender_prediction = layers.Dense(1, activation = &quot;sigmoid&quot;, name = &quot;gender&quot;)(x)
# 构建模型
model = Model(posts_input, [age_prediction, income_prediction, gender_prediction])

model.compile(optimizer = &quot;rmsprop&quot;, loss = [&quot;mse&quot;, &quot;categorical_crossentropy&quot;, &quot;binary_crossentropy&quot;])
model.compile(
   optimizer = &quot;rmsprop&quot;,
   loss = {
      &quot;age&quot;: &quot;mse&quot;,
      &quot;income&quot;: &quot;categorical_crossentropy&quot;,
      &quot;gender&quot;: &quot;binary_crossentropy&quot;
   }
)</code></pre>
</div>
<div id="经验总结" class="section level2">
<h2>3.经验总结</h2>
<div id="机器深度学习任务问题" class="section level3">
<h3>3.1 机器、深度学习任务问题</h3>
<ul>
<li>二分类</li>
<li>多分类</li>
<li>标量回归</li>
</ul>
</div>
<div id="回归问题" class="section level3">
<h3>3.2 回归问题</h3>
<ul>
<li>回归问题使用的损失函数
<ul>
<li>均方误差(MSE)</li>
</ul></li>
<li>回归问题使用的评估指标
<ul>
<li>平均绝对误差(MAE)</li>
</ul></li>
<li>回归问题网络的最后一层只有一个单元, 没有激活, 是一个线性层, 这是回归的典型设置, 添加激活函数会限制输出范围</li>
</ul>
</div>
<div id="二分类问题" class="section level3">
<h3>3.3 二分类问题</h3>
<ul>
<li>二分类问题使用的损失函数
<ul>
<li>对于二分类问题的 sigmoid 标量输出, <code>binary_crossentropy</code></li>
</ul></li>
<li>对于二分类问题, 网络的最后一层应该是只有一个单元并使用 sigmoid 激活的 Dense 层, 网络输出应该是 0~1 范围内的标量, 表示概率值</li>
</ul>
</div>
<div id="数据预处理问题" class="section level3">
<h3>3.4 数据预处理问题</h3>
<ul>
<li>在将原始数据输入神经网络之前, 通常需要对其进行预处理
<ul>
<li>结构化数据</li>
<li>图像数据</li>
<li>文本数据</li>
</ul></li>
<li>将取值范围差异很大的数据输入到神经网络中是有问题的
<ul>
<li>网路可能会自动适应这种取值范围不同的数据, 但学习肯定变得更加困难</li>
<li>对于这种数据, 普遍采用的最佳实践是对每个特征做标准化, 即对于输入数据的每个特征(输入数据矩阵中的列),
减去特征平均值, 再除以标准差, 这样得到的特征平均值为 0, 标准差为 1</li>
<li>用于测试数据标准化的均值和标准差都是在训练数据上计算得到的。在工作流程中, 不能使用测试数据上计算得到的任何结果,
即使是像数据标准化这么简单的事情也不行</li>
</ul></li>
<li>如果输入数据的特征具有不同的取值范围, 应该首先进行预处理, 对每个特征单独进行缩放</li>
</ul>
</div>
<div id="样本量问题" class="section level3">
<h3>3.5 样本量问题</h3>
<ul>
<li>如果可用的数据很少, 使用 K 折交叉验证可以可靠地评估模型</li>
<li>如果可用的训练数据很少, 最好使用隐藏层较少(通常只有一到两个)的小型模型, 以避免严重的过拟合
<ul>
<li>较小的网络可以降低过拟合</li>
</ul></li>
</ul>
</div>
<div id="网络结构选择问题" class="section level3">
<h3>3.6 网络结构选择问题</h3>
<ul>
<li>如果可用的训练数据很少, 最好使用隐藏层较少(通常只有一到两个)的小型模型, 以避免严重的过拟合</li>
<li>如果数据被分为多个类别, 那么中间层过小可能会导致信息瓶颈</li>
</ul>
</div>
<div id="优化器-1" class="section level3">
<h3>3.7 优化器</h3>
<ul>
<li>无论你的问题是什么, <code>rmsprop</code> 优化器通常都是足够好的选择</li>
</ul>
</div>
</div>
</div>
<div id="tensorflow-keras-后端" class="section level1">
<h1>TensorFlow Keras 后端</h1>
<div id="什么是-keras-后端" class="section level2">
<h2>1.什么是 Keras 后端？</h2>
<p>Keras 后端:</p>
<p>Keras 是一个模型级库, 为开发深度学习模型提供了高层次的构建模块。
它不处理诸如张量乘积和卷积等低级操作。</p>
<p>相反, 它依赖于一个专门的、优化的张量操作库来完成这个操作, 它可以作为 Keras 的「后端引擎」。
相比单独地选择一个张量库, 而将 Keras 的实现与该库相关联, Keras 以模块方式处理这个问题,
并且可以将几个不同的后端引擎无缝嵌入到 Keras 中。</p>
<p>目前可用的 Keras 后端:</p>
<ul>
<li>TensorFlow</li>
<li>Theano</li>
<li>CNTK</li>
</ul>
</div>
<div id="从一个后端切换到另一个后端" class="section level2">
<h2>2.从一个后端切换到另一个后端</h2>
<p>如果您至少运行过一次 Keras, 您将在以下位置找到 Keras 配置文件. 如果没有, 可以手动创建它.</p>
<p>Keras 配置文件位置:</p>
<pre class="bash"><code># Liunx or Mac
$ vim $HOME/.keras/keras.json

# Windows
$ vim %USERPROFILE%/.keras/keras.json</code></pre>
<p>Keras 配置文件创建:</p>
<pre class="bash"><code>$ cd ~/.keras
$ sudo subl keras.json</code></pre>
<p>也可以定义环境变量 <code>KERAS_BACKEND</code>, 不过这会覆盖配置文件 <code>$HOME/.keras/keras.json</code> 中定义的内容:</p>
<pre class="bash"><code>KERAS_BACKEND=tensorflow python -c &quot;from keras import backend&quot; 
Using TensorFlow backend.</code></pre>
<p>当前环境的 Keras 配置文件内容:</p>
<pre class="json"><code>{
   &quot;floatx&quot;: &quot;float32&quot;,
   &quot;epsilon&quot;: 1e-07,
   &quot;backend&quot;: &quot;tensorflow&quot;,
   &quot;image_data_format&quot;: &quot;channels_last&quot;
}</code></pre>
<p>自定义 Keras 配置文件:</p>
<ul>
<li><p>在 Keras 中, 可以加载除了 “tensorflow”, “theano” 和 “cntk”
之外更多的后端。Keras 也可以使用外部后端, 这可以通过更改 keras.json
配置文件和 “backend” 设置来执行。 假设您有一个名为 my_module 的 Python
模块, 您希望将其用作外部后端。keras.json 配置文件将更改如下.</p>
<ul>
<li><p>必须验证外部后端才能使用, 有效的后端必须具有以下函数:</p>
<ul>
<li><code>placeholder</code></li>
<li><code>variable</code></li>
<li><code>function</code></li>
</ul></li>
<li><p>如果由于缺少必需的条目而导致外部后端无效, 则会记录错误, 通知缺少哪些条目:</p>
<pre class="bash"><code>{
   &quot;image_data_format&quot;: &quot;channels_last&quot;,
   &quot;epsilon&quot;: 1e-07,
   &quot;floatx&quot;: &quot;float32&quot;,
   &quot;backend&quot;: &quot;my_package.my_module&quot;
}</code></pre></li>
</ul></li>
</ul>
</div>
<div id="keras.json-详细配置" class="section level2">
<h2>3.keras.json 详细配置</h2>
<ul>
<li><code>image_data_format</code>:
<ul>
<li><code>"channels_last"</code>
<ul>
<li>(rows, cols, channels)</li>
<li>(conv*dim1, convdim2, conv_dim3, channels)</li>
</ul></li>
<li><code>"channels_first"</code>
<ul>
<li>(channels, rows, cols)</li>
<li>(channels, convdim1, convdim2, conv_dim3)</li>
</ul></li>
<li>在程序中返回: <code>keras.backend.image_data_format()</code></li>
</ul></li>
<li><code>epsilon</code>:
<ul>
<li>浮点数, 用于避免在某些操作中被零除的数字模糊常量</li>
</ul></li>
<li><code>floatx</code>:
<ul>
<li>字符串: <code>float16</code>, <code>float32</code>, <code>float64</code> 。默认浮点精度</li>
</ul></li>
<li><code>backend</code>:
<ul>
<li>字符串: <code>tensorflow</code>, <code>theano</code>, <code>cntk</code></li>
</ul></li>
</ul>
</div>
<div id="backend-api" class="section level2">
<h2>5.Backend API</h2>
<ul>
<li><code>tf.keras.backend.clear_session()</code></li>
<li><code>tf.keras.backend.epsilon()</code>
<ul>
<li>返回数字表达式中使用的模糊因子的值</li>
</ul></li>
<li><code>tf.keras.backend.floatx()</code>
<ul>
<li>返回默认的 float 类型</li>
</ul></li>
<li><code>tf.keras.backend.get_uid()</code></li>
<li><code>tf.keras.backend.image_data_format()</code>
<ul>
<li>返回设置图像数据格式约定的值</li>
</ul></li>
<li><code>tf.keras.backend.is_keras_tensor()</code></li>
<li><code>tf.keras.backend.reset_uids()</code></li>
<li><code>tf.keras.backend.rnn()</code></li>
<li><code>tf.keras.backend.set_epsilon()</code>
<ul>
<li>设置数字表达式中使用的模糊因子的值</li>
</ul></li>
<li><code>tf.keras.backend.set_floatx()</code>
<ul>
<li>设置 float 类型</li>
</ul></li>
<li><code>tf.keras.backend.set_image_data_format()</code>
<ul>
<li>设置图像数据格式约定的值</li>
</ul></li>
</ul>
</div>
</div>
<div id="相关资料" class="section level1">
<h1>相关资料</h1>
<ul>
<li>数据
<ul>
<li><a href="http://yann.lecun.com/exdb/mnist/">MNIST 数据集主页</a></li>
</ul></li>
<li>网络论文
<ul>
<li><a href=""></a></li>
</ul></li>
</ul>
</div>
