---
title: DL Regularization
author: 王哲峰
date: '2022-07-15'
slug: dl-model-regularization
categories:
  - deeplearning
tags:
  - note
---

# Dropout



# Batch Normalization

## Batch Normalization 是什么？

在神经网络模型训练过程中，当我们更新之前的权重(weight)时，
每个中间激活层的输出分布会在每次迭代时发生变化，这种现象
称为内部协变量位移(ICS)。所以很自然的一件事就是，如果想
防止这种情况发生，就得需要修正所有的分布。简单地说，如果分布变动了，
会限制这个分布，不让它移动，以帮助梯度优化和防止梯度消失，
这样能帮助神经网络更快。因此减少这种内部协变量位移是推动 Batch Normalization 
发展的关键原则。

## Batch Normalization 原理

Batch Normalization 通过在 Batch 上减去经验平均值，
除以经验标准差来对前一个输出层的输出进行归一化。
这将是数据看起来想高斯分布。

$$\bar{x_{i}} = \frac{x_{i} - \mu_{B}}{\sqrt{\sigma^{2}_{B} + \epsilon}}$$

其中：

- $\mu_{B}$ 为 batch 均值
- $\sigma^{2}_{B}$ 为 batch 方差

$$y_{i} \leftarrow \gamma \hat{x}_{i} + \beta$$

并且，学习了新的平均值 $\gamma$ 和协方差 $\beta$，即，
可以认为 Batch Normalization 能够帮助控制 batch 分布的一阶和二阶动量。

## Batch Normalization 的优点

- 更快地收敛
- 降低初始权重的重要性
- 鲁棒的超参数
- 需要较少的数据进行泛化

![bn](images/bn.png)

## Batch Normalization 的缺点

### Batch Normalization 在使用 batch size 的时候不稳定

Batch Normalization 在训练时的时候必须计算平均值和方差，以便在 batch 中对之前的输出进行归一化。
如果 batch 比较大的话，这种统计估计是比较准确的，而随着 batch 减小，估计的准确性持续减小。

![bn](images/bn1.png)

以上是 ResNet-50 的验证错误图。可以推断，如果 batch 大小保持 为32，
它的最终验证误差在 23 左右，并且随着 batch 大小的减小，
误差会继续减小(batch 大小不能为 1，因为它本身就是平均值)。
损失有很大的不同(大约 10%)。

如果 batch 大小是一个问题，为什么我们不使用更大的 batch？
我们不能在每种情况下都使用更大的 batch。在 finetune 的时候，
我们不能使用大的 batch，以免过高的梯度对模型造成伤害。
在分布式训练的时候，大的 batch 最终将作为一组小 batch 分布在各个实例中。


### Batch Normalization 导致训练时间增加

NVIDIA 和卡耐基梅隆大学进行的实验结果表明，“尽管 Batch Normalization 不是计算密集型，
而且收敛所需的总迭代次数也减少了。” 但是每个迭代的时间显著增加了，
而且还随着batch大小的增加而进一步增加。

![bn](images/bn2.png)

<center>ResNet-50 在ImageNet上使用 Titan X Pascal</center>

你可以看到，batch normalization 消耗了总训练时间的 1/4。
原因是 batch normalization 需要通过输入数据进行两次迭代，
一次用于计算 batch 统计信息，另一次用于归一化输出。

### Batch Normalization 训练和推理时结果不一样

例如，在真实世界中做“物体检测”。在训练一个物体检测器时，
我们通常使用大 batch(YOLOv4 和 Faster-RCNN 都是在默认 batch=64 的情况下训练的)。
但在投入生产后，这些模型的工作并不像训练时那么好。
这是因为它们接受的是大 batch 的训练，而在实时情况下，它们的 batch 大小等于 1，
因为它必须一帧帧处理。考虑到这个限制，
一些实现倾向于基于训练集上使用预先计算的平均值和方差。
另一种可能是基于你的测试集分布计算平均值和方差值。

### Batch Normalization 对于在线学习不友好

![online-learning](images/online-learning.png)

<center>典型的在线学习 Pipeline</center>

与batch学习相比，在线学习是一种学习技术，在这种技术中，
系统通过依次向其提供数据实例来逐步接受训练，可以是单独的，
也可以是通过称为mini-batch的小组进行。每个学习步骤都是快速和便宜的，
所以系统可以在新的数据到达时实时学习。由于它依赖于外部数据源，
数据可能单独或批量到达。由于每次迭代中batch大小的变化，
对输入数据的尺度和偏移的泛化能力不好，最终影响了性能。

### Batch Normalization 对循环神经网络不友好

虽然 Batch Normalization可以显著提高卷积神经网络的训练和泛化速度，
但它们很难应用于递归结构。Batch Normalization 可以应用于RNN堆栈之间，
其中归一化是“垂直”应用的，即每个RNN的输出。但是它不能“水平地”应用，
例如在时间步之间，因为它会因为重复的重新缩放而产生爆炸性的梯度而伤害到训练。

## Batch Normalization 的可替代方法

在 Batch Normalization 无法很好工作的情况下，有几种可替代方法可用：

- Layer Normalization
- Instance Normalization
- Group Normalization (+ weight standardization)
- Synchronous Batch Normalization



# 神经网络过拟合

## 监督学习的目的

监督机器学习的目的是为了让建立的模型能够发现数据中普遍的一般的规律, 这个普遍的一般的规律无论对于训练集合适未知的测试集, 都具有较好的拟合能力。
假设空间中模型千千万, 当我们站在上帝视角, 心里相信总会有个最好的模型可以拟合我们的训练数据, 而且这个模型不会对训练集过度学习, 
它能够从训练集中尽可能的学到适用于所有潜在样本的“普遍规律”, 不会将数据中噪声也学习了。这样的模型也就是我们想要的、能够有较低的泛化误差的模型.

## 模型过拟合

- **过拟合含义:**
   - 过拟合是指在机器学习模型训练过程中, 模型对训练数据学习过度, 将数据中包含的噪声和误差也学习了, 使得模型在训练集上表现很好, 而在测试集上表现很差的一种现象。
- **发生过拟合的原因:**
   - 训练数据少
   - 模型比较初级, 无法解释复杂的数据
   - 模型拥有大量的参数(模型复杂度高)
- **解决或者缓解过拟合的方法:**
   - 获取更多的训练数据
   - 选用更好更加集成的模型
   - 为损失函数添加正则化项

## 监督学习正则化

- 监督机器学习的核心原理:
     `$$argmin \frac{1}{N}\sum_{i=1}^{N}L(y_i, f(x_i; \delta)) + \lambda J(f)$$`
  - 上述公式是机器学习中最核心、最关键、最能概述监督学习的核心思想的公式
- 监督机器学习的核心问题      
  - 确定正则化参数的同时最小化经验风险。最小化经验风险是为了让模型更加充分的拟合给定的训练数据, 
     而正则化参数则是控制这模型的复杂度, 防止我们过分的拟合训练数据, 从上面的公式可以看出, 
     监督机器学习的模型效果的控制有两项
- 经验风险项
  - 经验风险项主要是由训练数据集控制, 一般要求模型将最小化经验误差, 为的是模型极大程度的拟合训练数据, 如果该项过大则可能导致欠拟合, 欠拟合好办, 继续训练就是了.
  - 至少80%的单一机器学习模型都是上面这个公式可以解释的。无非就是对这两项变着法儿换样子而已。对于第一项的损失函数:
     - 如果形式是平方损失 (square loss) ,就是线性回归
     - 如果是对数损失 (log loss) , 就是对数几率回归
     - 如果是合页损失 (hingeloss) , 就是支持向量机
     - 如果是指数损失 (exp loss), 就是 Adaboost
- 正则化项
  - 从统计学习的角度来看, 对监督机器学习加入正则化项是结构风险最小化策略的实现, 正则化项一般是模型复杂度的单调递增函数, 模型越复杂, 正则化值就越大, 所以正则化项的存在能够使得我们的模型避免走向过拟合, 即防止模型过分拟合训练数据
  - 对于正则化项, `$\lambda` 是正则化系数, 通常是大于 0 的较小的正数(0.01, 0.001, ...), 是一种调整经验误差和正则化项之间关系的系数。所以, 在实际的训练过程中,  `$\lambda`
     作为一种超参数很大程度上决定了模型的好坏.
     - `$\lambda = 0` 时相当于该公式没有正则化项, 模型全力讨好第一项, 将经验误差进行最小化, 往往这也是最容易发生过拟合的时候.
     - 随着 `$\lambda` 逐渐增大, 正则化项在模型选择中的话语权越来越高, 对模型的复杂性的惩罚也越来越厉害, 模型中参数的值逐渐接近 `$0` 或等于 `$0`.
  - 对于正则化项, 正则化项的形式有很多, 但常见的也就是 `$L1` 和 `$L2` 正则化。也有 `$L0` 正则化.
     - `$L0` 正则化也就是 `$L0` 范数, 即矩阵中所有非 `$0` 元素的个数。`$L0` 范数就是希望要正则化的参数矩阵 `$W` 大多数元素都为 `$0`, 
        简单粗暴, 让参数矩阵 `$W` 大多数元素为 `$0` 就是实现稀疏而已.
     - `$L1` 范数就是矩阵中各元素绝对值之和, `$L1` 范数通常用于实现参数矩阵的稀疏性。稀疏通常是为了特征选择和易于解释方面的考虑.
        在机器学习领域, `$L0` 和 `$L1` 都可以实现矩阵的稀疏性, 但在实践中, `$L1` 要比 `$L0` 具备更好的泛化求解特性而广受青睐.
     - 相较于 `$L0` 和 `$L1$` , 其实 `$L2` 才是正则化中的天选之子。在各种防止过拟合和正则化处理过程中, `$L2`
        正则化可谓风头无二。`$L2` 范数是指矩阵中各元素的平方和后的求根结果。采用 `$L2`
        范数进行正则化的原理在于最小化参数矩阵的每个元素, 使其无限接近于 `$0` 但又不像 `$L1` 那样等于 `$0`, 
        为什么参数矩阵中每个元素变得很小就能防止过拟合？用深度神经网络来举例, 在 `$L2` 正则化中, 如果正则化系数变得比较大, 
        参数矩阵 `$W` 中的每个元素都在变小, 线性计算的和 `$Z` 也会变小, 激活函数在此时相对呈线性状态, 
        这样就大大简化了深度神经网络的复杂性, 因而可以防止过拟合.

## 常用正则化方法

 - `$L1` 正则化, Lasso         
       `$$\min \frac{1}{N}\sum_{i=1}^{N}L(y_i, f(x_i)) + \lambda ||w||$$`
 - `$L2` 正则化, 岭回归(Ridge Regression)         
       `$$\min \frac{1}{N}\sum_{i=1}^{N}L(y_i, f(x_i)) + \frac{\lambda}{2} ||w||^{2}$$`
- 两者都是对回归损失函数加一个约束项, lasso 加的是回归系数的 `$L1` 范数, 岭回归加的是回归系数 `$L2` 范数.

# 神经网络正则化

神经网络的正则化方法:

- 权重衰减
    - `$L_2$` 正则化
    - `$L_1$` 正则化
    - `$L_\infty$` 正则化
- Dropout

## 权值衰减(L1,L2 正则化)

- 未正则化的交叉熵损失函数
     `$$J = -\frac{1}{m}\sum_{i=1}^{m}\Big(y^{(i)}\log(\hat{y}^{(L)(i)}) + (1 - y^{(i)})\log(1 - \hat{y}^{(L)(i)})\Big)$$`
- L1 正则化的交叉熵损失函数         
     `$$J = \underbrace{-\frac{1}{m}\sum_{i=1}^{m}\Big(y^{(i)}\log(\hat{y}^{(L)(i)}) + (1 - y^{(i)})\log(1 - \hat{y}^{(L)(i)})\Big)}\_{\text{cross-entropy cost}} + \underbrace{\frac{1}{m}\lambda\sum_{l}\sum_{k}\sum_{j} ||W_{k,j}^{[L]}||}\_{\text{L1 regularization cost}}$$`
- L2 正则化的交叉熵损失函数
     `$$J = \underbrace{-\frac{1}{m}\sum_{i=1}^{m}\Big(y^{(i)}\log(\hat{y}^{(L)(i)}) + (1 - y^{(i)})\log(1 - \hat{y}^{(L)(i)})\Big)}\_{\text{cross-entropy cost}} + \underbrace{\frac{1}{m}\frac{\lambda}{2}\sum_{l}\sum_{k}\sum_{j} W_{k,j}^{[L]2}}\_{\text{L2 regularization cost}}$$`

## Dropout

- 当网络的模型变得很复杂时, 权值衰减方法不能有效地对过拟合进行抑制; 
- Dropout在神经网络学习的过程中随机删除神经元:
- 训练时, 随机选出隐藏层的神经元, 然后将其删除, 被删除的神经元不再进行信号的传递
- 训练时, 每传递一次数据, 就会随机选择要删除的神经元
- 测试时, 虽然会传递所有的神经元信号, 但是对于各个神经元的输出, 要乘上训练时的删除比例后再输出

Dropout 是指在神经网络训练的过程中, 对所有神经元按照一定的概率进行消除的处理方式。在训练深度神经网络时, Dropout
能够在很大程度上简化神经网络结构, 防止神经网络过拟合。所以, 从本质上而言, Dropout 也是一种神经网络的正则化方法。

假设我们要训练了一个多隐藏层的神经网络, 该神经网络存在着过拟合。于是我们决定使用 Dropout 方法来处理, 
Dropout 为该网络每一层的神经元设定一个失活 (drop) 概率, 在神经网络训练过程中, 我们会丢弃一些神经元节点, 
在网络图上则表示为该神经元节点的进出连线被删除。最后我们会得到一个神经元更少、模型相对简单的神经网络, 
这样一来原先的过拟合情况就会大大的得到缓解。这样说似乎并没有将 Dropout 正则化原理解释清楚.

为什么 Dropout 可以可以通过正则化发挥防止过拟合的功能？

- 因为 Dropout 可以随时随机的丢弃任何一个神经元, 神经网络的训练结果不会依赖于任何一个输入特征, 每一个神经元都以这种方式进行传播, 
   并为神经元的所有输入增加一点权重, Dropout 通过传播所有权重产生类似于 L2 正则化收缩权重的平方范数的效果, 这样的权重压缩类似于 L2
   正则化的权值衰减, 这种外层的正则化起到了防止过拟合的作用。

所以说, 总体而言, Dropout 的功能类似于 L2 正则化, 但又有所区别。另外需要注意的一点是, 对于一个多层的神经网络, 
我们的 Dropout 某层神经元的概率并不是一刀切的。对于不同神经元个数的神经网络层, 我们可以设置不同的失活或者保留概率, 
对于含有较多权值的层, 我们可以选择设置较大的失活概率 (即较小的保留概率) 。所以, 总结来说就是如果你担心某些层所含神经
元较多或者比其他层更容易发生过拟合, 我们可以将该层的失活概率设置的更高一些。

## 数据增强


## 提前终止
