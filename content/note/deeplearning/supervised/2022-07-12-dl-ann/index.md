---
title: ANN 和 DNN
subtitle: 人工神经网络、深度神经网络
author: 王哲峰
date: '2022-07-12'
slug: dl-ann
categories:
  - deeplearning
tags:
  - model
---

<style>
details {
    border: 1px solid #aaa;
    border-radius: 4px;
    padding: .5em .5em 0;
}
summary {
    font-weight: bold;
    margin: -.5em -.5em 0;
    padding: .5em;
}
details[open] {
    padding: .5em;
}
details[open] summary {
    border-bottom: 1px solid #aaa;
    margin-bottom: .5em;
}
</style>

<details><summary>目录</summary><p>

- [深度学习](#深度学习)
- [神经元](#神经元)
- [人工神经网络](#人工神经网络)
  - [深度神经网络](#深度神经网络)
    - [从感知机到神经网络](#从感知机到神经网络)
    - [DNN 的基本结构](#dnn-的基本结构)
    - [DNN 前向传播算法](#dnn-前向传播算法)
    - [DNN 反向传播算法](#dnn-反向传播算法)
  - [前馈网络](#前馈网络)
    - [深度前馈网络](#深度前馈网络)
  - [记忆网络](#记忆网络)
  - [图网络](#图网络)
</p></details><p></p>

# 深度学习

**机器学习** 就是从历史数据中探索和训练出数据的普遍规律，将其归纳为相应的数学模型，并对未知的数据进行预测的过程。
在这个过程中会碰到各种各样的问题，比如下面一系列关乎机器学习模型生死的问题：数据质量、模型评价标准、训练优化方法、过拟合

在机器学习中，有很多已经相当成熟的模型，在这些机器学习模型中，**人工神经网络** 就是一种比较厉害的模型；
人工神经网络从早期的感知机发展而来，对任何函数都有较好的拟合性。但自上个世纪 90 年代一直到 2012 年深度学习集中爆发前夕，
神经网络受制于计算资源的限制和较差的可解释性，一直处于发展的低谷阶段。之后大数据兴起，计算资源也迅速跟上，
加之 2012 年 ImageNet 竞赛冠军采用的 AlexNet 卷积神经网络一举将图片预测的 top5 错误率降至 16.4%，
震惊了当时的学界和业界。从此之后，原本处于研究边缘状态的神经网络又迅速热了起来，深度学习也逐渐占据了计算机视觉的主导地位

以神经网络为核心的深度学习理论是机器学习的一个领域分支，所以深度学习其本质上也必须遵循一些机器学习的基本要义和法则。
从机器学习的角度来开，神经网络一般可以看作一个非线性模型，其基本组成单元为具有非线性激活函数的神经元，通过大量神经元之间的连接，
使得神经网络成为一种高度非线性的模型。神经元之间的连接权重就是需要学习的参数，可以在机器学习的框架下通过梯度下降方法来进行学习

# 神经元

人工神经元(Artificial Neuron)，简称神经元(Neuron)，是构成神经网络(ANN)的基本单元，接收一组输入信号并产生输出

现代神经网络中的神经元和 MP 神经元的结构并无太多变化。不同的是，MP 神经元中的激活函数 `$f$` 为 0 或 1 的阶跃函数，
而现代神经元中的激活函数通常要求是连续可导的函数

假设一个神经元接收 `$D$` 个输入 `$x_{1}, x_{2}, \ldots, x_{D}$`，
令向量 `$\boldsymbol{x}=[x_{1}; x_{2}; \ldots; x_{D}]$` 来表示这组输入，
并用净输入(Net Input) `$\textbf{z} \in \mathbb{R}$` 表示一个神经元所获得的输入信号 `$x$` 的加权和

`$$\begin{aligned}
\textbf{z} &= \sum_{d=1}^{D}\omega_{d}x_{d} + b \\
           &=\boldsymbol{\omega}^{T}\boldsymbol{x}+b
\end{aligned}$$`

其中：

* `$\boldsymbol{\omega} = [\omega_{1}; \omega_{2}; \ldots; \omega_{D}] \in \mathbb{R}^{D}$` 是 `$D$` 维的权重向量
* `$b \in \mathbb{R}$` 是偏置

净输入 `$\textbf{z}$` 在经过一个非线性函数 `$f(\cdot)$` 后，得到神经元的活性值(Activation) 

`$$a = f(\textbf{z})$$`

其中非线性函数 `$f(\cdot)$` 称为激活函数(Activation Function)

![img](images/neuron.png)

# 人工神经网络

> ANN

![img](images/network.png)

深度神经网络设计：

1. 选择优化模型
2. 选择损失函数
3. 选择输出单元形式
4. 选择用于计算隐藏层值激活函数(activation function)
5. 设计网络的结构, 包括
    - 网络应该包含多少层
    - 层与层之间应该如何连接
    - 每一层包含多少单元
6. 反向传播(back propagation)算法和推广

## 深度神经网络

> Deep Neural Network, DNN

深度神经网络(Deep Neural Networks, DNN)是深度学习的基础，而要理解 DNN，
首先要理解 DNN 模型，下面就对 DNN 的模型与前向传播算法做一个总结

### 从感知机到神经网络

> NN

### DNN 的基本结构


### DNN 前向传播算法


### DNN 反向传播算法


## 前馈网络

> Feedforward Neural Newwork, FNN

### 深度前馈网络

深度前馈网络的目标是：近似某个函数 `$f^{*}$`。深度前馈网络定义了一个映射 `$y = f(x; \theta)$`，并且学习参数 `$\theta$` 的值，
使它能够得到最佳的函数近似 `$f^{*}$`

深度前馈网络之所以被称为前馈(feedforward)的，是因为信息流过 `$x$` 的函数，流经用于定义 `$f$` 的中间计算过程，
最终到达输出 `$y$`。在模型的输出和模型本身之间没有反馈(feedback)连接。当深度前馈网络被扩展成包含反馈连接时，
被称为循环神经网络(Recurrent Reural Network, RNN)

深度前馈网络之所以被称为网络(network)，是因为它们通常用许多不同函数复合在一起来表示，
该模型与一个有向无环图相关联，而图描述了函数是如何复合在一起的。网络链的全长称为模型的深度(depth)

深度前馈网络之所以被称为神经网路，是因为他们或多或少地受到神经科学的启发。
网络中每个隐藏层通常都是向量值的。这些隐藏层的维数决定了模型的宽度(width)。
向量的每个元素都可以被视为起到类似一个神经元的作用。
除了将层想象成向量到向量的单个函数，也可以把层想象成由许多并行操作单元(unit)组成， 
每个单元表示一个向量到标量的函数。每个单元在某种意义上类似一个神经元，
它接收的输入来源于许多其他的单元，并计算自己的激活值

## 记忆网络

## 图网络

