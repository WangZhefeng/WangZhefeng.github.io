---
title: DL ReinForcement Learning
author: 王哲峰
date: .na.character
slug: dl-reinforcementlearning
categories:
  - deeplearning
tags:
  - tool
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<div id="深度强化学习" class="section level1">
<h1>1.深度强化学习</h1>
<p>强化学习(Reinforcement learning, RL)强调如何基于环境而行动，以取得最大化的预期利益。
结合了深度学习技术后的强化学习更是如虎添翼。近两年广为人知的 AlphaGo 既是深度强化学习(DRL)的典型应用。</p>
</div>
<div id="强化学习示例" class="section level1">
<h1>2.强化学习示例</h1>
<p>以下示例使用深度强化学习玩 CartPole(倒立摆)游戏，倒立摆是控制论中的经典问题，
在这个游戏中，一根杆的底部与一个小车通过轴相连，而杆的重心在轴之上，因此是一个不稳定的系统。
在重力的作用下，杆很容易倒下。需要控制小车在水平的轨道上进行左右运动，使得杆一直保持竖直平衡状态。</p>
<div id="开发环境" class="section level2">
<h2>2.1 开发环境</h2>
<ul>
<li>OpenAI 推出的 Gym 环境库中的 CartPole 游戏环境
<ul>
<li>和 Gym 的交互过程很像一个回合制游戏：
<ul>
<li>首先，获得游戏的初始状态(比如杆的角度、小车位置)</li>
<li>然后，在每个回合，都需要在当前可行的动作中选择一个并交由 Gym 执行
<ul>
<li>比如：向左或者右推动小车，每个回合中二者只能选择一</li>
</ul></li>
<li>Gym 在执行动作后，会返回动作执行后的下一个状态和当前回合所获得的奖励值
<ul>
<li>比如：选择向左推动小车并执行后，小车位置更加偏左，而杆的角度更加偏右，Gym 将新的角度和位置返回，
而如果在这一回合杆没有倒下，Gym 同时返回一个小的正奖励</li>
</ul></li>
<li>上述过程可以一直迭代下去，直到游戏结束
<ul>
<li>比如：杆倒下了</li>
</ul></li>
</ul></li>
</ul></li>
<li>Gym 环境库中的 CartPole 游戏环境库安装</li>
</ul>
<pre class="bash"><code>$ pip install gym</code></pre>
<ul>
<li>Gym 的 Python 基本调用方法</li>
</ul>
<pre class="python"><code>import gym

env = gym.make(&quot;CartPole-v1&quot;)                         # 实例化一个游戏环境，参数为游戏名称
state = env.reset()                                   # 初始化环境，获得初始状态
while True:
    env.render()                                      # 对当前帧进行渲染，绘图到屏幕
    action = model.predict(state)                     # 假设我们有一个训练好的模型，能够通过当前状态预测出这时应该进行的动作
    next_state, reward, done, info = env.step(action) # 让环境执行动作，获得执行完动作的下一个状态，动作的奖励，游戏是否一结束以及额外信息
    if done:                                          # 如果游戏结束，则退出循环
        break</code></pre>
</div>
<div id="cartpole-示例" class="section level2">
<h2>2.2 CartPole 示例</h2>
<ul>
<li><p>任务</p>
<ul>
<li>训练出一个模型，能够根据当前的状态预测出应该进行的一个好的动作。
粗略地说，一个好的动作应当能够最大化整个游戏过程中获得的奖励之和，
这也是强化学习的目标。</li>
<li>CartPole 游戏中的目标是，希望做出的合适的动作使得杆一直不倒，
即游戏交互的回合数尽可能多，而每进行一回合，都会获得一个小的正奖励，
回合数越多则积累的奖励值也越高。因此，最大化游戏过程中的奖励之和与最终目标是一致的。</li>
<li>使用深度强化学习中的 Deep Q-Learning 方法来训练模型</li>
</ul></li>
</ul>
<ol style="list-style-type: decimal">
<li>首先，引入常用库，定义一些模型超参数</li>
</ol>
<pre class="python"><code>import tensorflow as tf</code></pre>
<p>2.使用 <code>tf.keras.Model</code> 建立一个 Q 函数网络，用于拟合 Q-Learning 中的 Q 函数</p>
<pre><code>- 使用较简单的多层全连接神经网络进行拟合，该网络输入当前状态，输入各个动作下的 Q-Value(CartPole 下为二维，即向左和向右推动小车)</code></pre>
<pre class="python"><code>class QNetwork(tf.keras.Model):
    pass</code></pre>
</div>
</div>
