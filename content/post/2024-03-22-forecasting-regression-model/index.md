---
title: 预测：方法与实践--时间序列回归模型
author: 王哲峰
date: '2024-03-22'
slug: forecasting-regression-model
categories:
  - timeseries
tags:
  - book
---

<style>
details {
    border: 1px solid #aaa;
    border-radius: 4px;
    padding: .5em .5em 0;
}
summary {
    font-weight: bold;
    margin: -.5em -.5em 0;
    padding: .5em;
}
details[open] {
    padding: .5em;
}
details[open] summary {
    border-bottom: 1px solid #aaa;
    margin-bottom: .5em;
}
img {
    pointer-events: none;
}
</style>

<details><summary>目录</summary><p>

- [线性模型](#线性模型)
    - [简单线性回归](#简单线性回归)
    - [多元线性回归](#多元线性回归)
    - [假设条件](#假设条件)
- [最小二乘估计](#最小二乘估计)
    - [模型拟合](#模型拟合)
    - [拟合值](#拟合值)
    - [拟合优度](#拟合优度)
    - [回归的标准误差](#回归的标准误差)
- [回归模型的评估](#回归模型的评估)
    - [残差的性质](#残差的性质)
    - [残差时序图](#残差时序图)
    - [残差的自相关函数图](#残差的自相关函数图)
    - [残差直方图](#残差直方图)
    - [预测变量与残差的关系图](#预测变量与残差的关系图)
    - [拟合值与残差的关系图](#拟合值与残差的关系图)
    - [异常值点和强影响点](#异常值点和强影响点)
    - [伪回归](#伪回归)
- [回归模型特征构建](#回归模型特征构建)
    - [趋势](#趋势)
    - [虚拟变量](#虚拟变量)
    - [季节性虚拟变量](#季节性虚拟变量)
    - [干预变量](#干预变量)
    - [交易日](#交易日)
    - [分布滞后](#分布滞后)
    - [复活节](#复活节)
    - [傅里叶级数](#傅里叶级数)
- [预测变量的筛选](#预测变量的筛选)
    - [调整的可决系数](#调整的可决系数)
    - [交叉检验](#交叉检验)
    - [赤池信息准则](#赤池信息准则)
    - [修正的赤池信息准则](#修正的赤池信息准则)
    - [施瓦茨的贝叶斯信息准则](#施瓦茨的贝叶斯信息准则)
    - [准则的选择](#准则的选择)
- [回归预测](#回归预测)
- [非线性回归](#非线性回归)
- [相关关系、因果关系和预测](#相关关系因果关系和预测)
- [矩阵方程](#矩阵方程)
</p></details><p></p>

线性回归模型的核心思路是：我们预测时间序列 `$y$` 时假设它与其它时间序列 `$x$` 之间存在线性关系。
例如，我们可以通过广告总花费 `$x$` 来预测月度销量 `$y$`；同样的，
我们可以通过气温数据 `$x_{1}$` 和星期数据 `$x_{2}$` 来预测日耗电量 `$y$`。

* 被预测变量 `$y$` 有时还称作回归变量、因变量或被解释变量。
* 预测变量 `$x$` 有时也叫作回归量、自变量或解释变量。这里我们称它们为“被预测变量”和“预测变量”。

# 线性模型

## 简单线性回归

最简单的线性回归模型假设被预测变量 `$y$` 和单个预测变量 `$x$` 之间存在如下线性关系：

`$$y_{t} = \beta_{0} + \beta_{1}x_{t} + \varepsilon_{t}$$`

观测值并不全部落在回归线上，而是分布在回归线的周围。我们可以这样理解：
每个观测值 `$y_{t}$` 都包含可解释部分 `$\beta_{0}+\beta_{1}x_{t}$` 和随机误差项 `$\varepsilon_{t}$`。
随机误差项并不意味着错误，而是指观测值与线性模型的偏差。
它捕捉到了除 `$x_{t}$` 外其他影响 `$y_{t}$` 的信息。

## 多元线性回归

当预测变量有两个甚至更多时，模型被称为多元线性回归模型。多元线性回归模型的一般形式如下：

`$$y_{t}=\beta_{0}+\beta_{1}x_{1，t}+\beta_{2}x_{2，t}+\cdots+\beta_{k}x_{k，t}+\varepsilon_{t}$$`

其中，`$y$` 是被预测变量，`$x_{1}，\cdots，x_{k}$` 是 `$k$` 个预测变量，
每个预测变量都必须为数值型变量。系数 `$\beta{1}，\cdots，\beta_{k}$` 分别衡量了在保持其他所有预测变量不变的情况下，
该预测变量对被预测变量的影响程度。因此，系数衡量了对应预测变量对被预测变量的边际影响。

## 假设条件

当我们想要使用线性回归模型时，需要对变量做出一些基本假设。

1. 首先，我们假设线性模型是对现实情况的合理近似；也就是说，预测变量和被预测变量之间的关系基本满足这个线性方程。
2. 其次，我们对误差项 `$(\varepsilon_{1}，\cdots，\varepsilon_{T})$` 做出如下假设：
    * 期望为零；否则预测结果会产生系统性偏差。
    * 随机误差项彼此不相关；否则预测效果会很差，因为这表明数据中尚有很多可用信息没有包含在模型中。
    * 与预测变量不相关；若误差项与预测变量相关，则表明模型的系统部分中应该包含更多信息。
    * 为了方便得到预测区间，我们还需要假设随机误差项服从方差为 `$\sigma^{2}$` 的正态分布。

线性回归模型还有一个重要的假设是 <span style='border-bottom:1.5px dashed red;'>预测变量 `$x$` 不是随机变量</span>。在进行模拟实验时，
我们可以控制每个 `$x$` 的值（所以 `$x$` 不会是随机的）并观察 `$y$` 的结果值。
但在实际生活中，我们只能得到观察数据（包括商业和经济学中的大多数数据），
而不能控制 `$x$` 的值。因此，我们需要做出如上假设。

# 最小二乘估计

## 模型拟合

在实际问题中，有一系列的观察值，
但是我们不知道模型系数 `$\beta_{0}, \beta_{1}, \cdots, \beta_{k}$` 的具体值。
因此，我们需要利用模型对这些参数进行估计。

最小二乘估计方法通过最小化残差平方和来确定模型的各个参数。
也就是说，我们通过最小化下式来确定 `$\beta_{0}, \beta_{1}, \cdots, \beta_{k}$`的估计值：

`$$\sum_{t=1}^{T}\varepsilon_{t}^{2}=\sum_{t=1}^{T}(y_{t} - \beta_{0} - \beta_{1}x_{1,t}-\beta_{2}x_{2,t} - \cdots - \beta_{k}x_{k,t})^{2}$$`

由于它的目标是最小化残差平方和，因此被称为最小二乘估计。寻找最优参数的过程，一般被称为“拟合”模型，
或者被称为模型的“学习”或者“训练”。参数估计值，
一般用 `$\hat{\beta}_{0}, \hat{\beta}_{1}, \cdots, \hat{\beta}_{k}$` 来表示。

## 拟合值

可以利用回归方程中的估计系数并将误差项设置为零来预测 `$y$`。我们通常将模型写成如下形式：

`$$\hat{y}_{t}=\hat{\beta}_{0}+\hat{\beta}_{1}x_{1,t}+\cdots+\hat{\beta}_{k}x_{k,t}$$`

将训练样本中 `$x_{1,t}, \cdots, x_{k,t}$`（其中 `$t=1,\cdots, T$`）的值代入模型中，
我们将会得到 `$y_{t}$` 的预测值，即为模型的拟合值。需要注意的是，这是模型估计得到的训练样本的预测值，
而不是 `$y$` 未来真实值的预测值。

## 拟合优度

一般用可决系数 `$R^{2}$` 评价线性回归模型对数据的拟合程度。
它可以通过计算观测值 `$y$` 和预测值 `$\hat{y}$` 之间的相关性来得出。或者，通过下式计算：

`$$R^{2}=\frac{\sum(\hat{y}_{t}-\bar{y})^{2}}{\sum(y_{t}-\bar{y})^{2}}$$`

可决系数反映了回归模型所能解释的被预测变量的变异占被预测变量总变异的比例。

在简单线性回归模型中，`$R^{2}$` 也等于 `$y$` 和 `$x$` 的相关系数的平方（假设存在截距项）。
预测值越接近于真实值，`$R^{2}$` 则会越接近于 `$1$`。
相反，若预测值和真实值不相关，则 `$R^{2}=0$` （假设存在截距项）。
在其它情况下，`$R^{2}$` 的值会处在 `$0$` 和 `$1$` 之间。

但是仅仅利用 `$R^{2}$` 来衡量模型是远远不够的。因为当增加解释变量的个数时，
`$R^{2}$` 值将会不断增加，但这并不意味着更好的模型效果。目前并不存在衡量 `$R^{2}$` 值好坏的规则， 
`$R^{2}$` 值的有效性需要视具体情况而定。
因此，利用模型在测试集上的预测结果来衡量模型好坏比直接根据 `$R^{2}$` 大小来衡量模型更加有效。

## 回归的标准误差

另外一个衡量模型拟合效果的指标是残差的标准偏差，通常称之为“残差标准误差”。
它可以通过下式来计算：

`$$\hat{\sigma}_{e}=\sqrt{\frac{1}{T-k-1}\sum_{t=1}^{T}{e_t^2}}$$`

其中：

* `$k$` 是模型中预测变量的个数
* `$T$` 是样本数量
* `$e_{t}^{2}$` 是模型拟合值 `$\hat{y}$` 和实际观测值 `$y$` 之间的差值，被称为训练误差或“残差”

需要注意的是，由于需要估计的参数个数为 `$k+1$`（截距项和 `$k$` 个解释变量），
因此上式中分母为 `$T-k-1$`。

模型的标准误差和平均误差有一定联系。我们可以将标准误与 `$y$` 的均值或标准差做对比，
得到一些关于模型精度的结论。在生成被预测变量的预测区间时，标准误差将十分有用。

# 回归模型的评估

## 残差的性质

模型拟合值 `$\hat{y}$` 和实际观测值 `$y$` 之间的差值，被称为训练误差或“残差”，其表达式如下：

`$$
\begin{align*}
  e_t &= y_t - \hat{y}_t \\
      &= y_t - \hat\beta_{0} - \hat\beta_{1} x_{1,t} - \hat\beta_{2} x_{2,t} - \cdots - \hat\beta_{k} x_{k,t}
\end{align*}$$`

其中：

* `$t=1,\cdots, T$`
* 每个残差 `$e_{t}$` 都是观测值中不可预测的部分 

残差项有两个非常有用的性质：

`$$\sum_{t=1}^{T}e_{t}=0 \quad and \quad \sum_{t=1}^{T}x_{k,t}e_{t}=0 \quad \text{for all} \quad k.$$` 

从以上两式可以明显看出：

1. 残差的均值为零；
2. 残差项和预测变量之间相关性为零。（当模型中没有截距项时，零相关假设不一定成立。）

在选择回归变量并拟合回归模型之后，有必要绘制残差图以检查模型的假设是否已经满足。
此外应该生成一系列图表，以检查拟合模型的不同方面和基本假设是否成立。下面我们将逐个分析。

## 残差时序图

残差时序图显示了不同时间下的残差的变化，如果残差存在异方差性，这种异方差性会导致预测区间的不准确。

## 残差的自相关函数图

> 残差的自相关函数，简称 ACF

对于时间序列数据而言，在当前时间段观测到的变量值很可能与历史时段的变量值很相似。
因此，当采用回归模型拟合时间序列数据时，残差经常会出现自相关效应。
此时，模型违背了残差中无序列自相关的假设，并会导致模型的预测效率低下。
为了获得更为准确的预测值，在模型中应该考虑更多的信息。当残差项存在序列自相关时，
模型的预测结果仍然是无偏的，但此时得到预测区间范围通常会比我们需要的预测区间范围更大。
因此我们应当重点关注模型残差的 ACF 图。

另一个用于检验残差自相关的效果较好的检验方法是 Breusch-Godfrey 检验，
也被称为 LM （拉格朗日乘数）检验。假如 `$p$` 值小于一个特定值（例如 0.05），
则表明残差中存在显著的自相关性。Breusch-Godfrey 检验类似于 Ljung-Box 检验，
但它是专门用于回归模型的残差检验。

## 残差直方图

检查残差是否服从正态分布也是很有必要的。正如之前我们所解释的一样，
它对预测值并不重要，但它可以让我们更加容易的确定预测区间。

残差直方图显示了残差分布是否存在左偏或右偏，如果存在，则可能影响预测区间的准确度。

## 预测变量与残差的关系图

我们期望残差是随机分布的并且不显示任何规律，
一个简单快捷的检验方法是查看每个预测变量与残差的散点图。
如果这些散点图表现出明显的规律，则该关系可能是非线性的，
并且需要相应地修改模型，可能需要使用非线性模型。

此外，还需要对没有加入到模型中的预测变量绘制其与残差的散点图。
如果某个残差图显示出明显的规律，
则需要将对应的预测变量加入到模型之中（可能以非线性形式加入）。

## 拟合值与残差的关系图

残差与拟合值之间也应没有明显规律。如果观察到明显规律，
则残差中可能存在“异方差性”，这意味着残差的方差不是固定的。
如果出现异方差性，可能需要对预测变量做对数或者平方根变换。

## 异常值点和强影响点

与大多数数据相差甚远的点被称为“异常值点”。
对模型的参数估计有重大影响的观测点被称为“强影响点”。
通常情况下，强影响点在 `$x$` 方向也是极端的异常值。

异常值的一个来源是不正确的数据录入。简单的数据描述性统计可以识别出异常的最小值和最大值。
如果识别出这样的观察结果，则应立即对样本进行校正或删除。

当某些观测点完全不同时，也会出现异常值点。在这种情况下，将这些观测点全部删除是不可取的。
如果观察结果已被确定为异常值，则必须对其进行研究并分析其背后的可能原因。
删除或保留观察可能是一个艰难决定（特别是当异常值是有影响力的观察时）。
因此，可以分别对删除观测值和保留观测值做分析。

## 伪回归

时间序列数据一般都是“不平稳的”；也就是说，时间序列数据没有固定的均值和方差。
因此我们需要解决非平稳数据对回归模型的影响，后面我们会详细讨论时间序列的平稳性。
在这里，我们需要强调非平稳数据对回归模型的影响。

不平稳的时间序列会导致伪回归。
伪回归的特点是高 `$R^{2}$` 值和高残差自相关共存。
伪回归模型似乎可以给出合理的短期预测，
但在长期时间中，伪回归是无效的。

# 回归模型特征构建

> 原来的题目：一些有用的预测变量

## 趋势

很多时间序列存在趋势。当存在简单的线性趋势时，可以直接使用 `$x_{1,t}=t$` 作为预测变量：

`$$y_{t} = \beta_{0} + \beta_{1}t + \varepsilon_{t}$$`

其中：

* `$t = 1, 2, \cdots, T$`

## 虚拟变量

目前，我们讨论的每个预测变量都是数值型变量。
但是当某个预测变量为分类变量且只有两个取值时（例如，“是”或“否”）应当怎么处理？
例如，当你想要预测日销量时，
你想把当天是否为 <span style='border-bottom:1.5px dashed red;'>法定节假日</span> 考虑进来。
此时，则需要引入一个预测变量，当天为法定节假日时该变量取值为“是”，否则取值为“否”。

在这种情况下，我们可以通过在多元模型中添加“虚拟变量”来进行处理。
当虚拟变量的取值为 1 时，代表“是”；取值为 0 时代表“否”。
虚拟变量通常也被称为“指示变量”。

虚拟变量也可以用来处理数据中的 <span style='border-bottom:1.5px dashed red;'>离群点</span> 。虚拟变量不会省略异常值，而是会消除其效果。当该观测值是离群点时，虚拟变量取值为 1，
在其他观测值处，虚拟变量取值均为 0。虚拟变量也可以表示特殊事件是否发生。

如果有两个以上的类别，则可以使用多个虚拟变量（需要注意的是，虚拟变量个数应比类别数少1）对变量进行编码。

## 季节性虚拟变量

假设我们想要预测日度数据，且想把星期数（周一、周二等等）作为预测变量。我们可以构造如下的虚拟变量：

|            | `$d_{1,t}$` | `$d_{2,t}$` | `$d_{3,t}$` | `$d_{4,t}$` | `$d_{5,,t}$` | `$d_{6,t}$`| 
|------------|-------------|-------------|-------------|-------------|--------------|------------|
| 周一        | 1           | 0           | 0           | 0           | 0            | 0          |
| 周二        | 0           | 1           | 0           | 0           | 0            | 0          |
| 周三        | 0           | 0           | 1           | 0           | 0            | 0          |
| 周四        | 0           | 0           | 0           | 1           | 0            | 0          |
| 周五        | 0           | 0           | 0           | 0           | 1            | 0          |
| 周六        | 0           | 0           | 0           | 0           | 0            | 1          |
| 周日        | 0           | 0           | 0           | 0           | 0            | 0          |
| 周一        | 1           | 0           | 0           | 0           | 0            | 0          |
| `$\cdots$` | `$\cdots$`  | `$\cdots$`  | `$\cdots$`  | `$\cdots$`  | `$\cdots$`   | `$\cdots$` |

值得注意的是，编码七个类别只需要六个虚拟变量。当所有虚拟变量都取0时，即可表示第七类（上例中的周日）。

许多初学者会给第七类添加第七个虚拟变量，这会导致模型预测变量之间出现完全共线性，
一般被称为“虚拟变量陷阱”，它会导致回归失败。因此如果定性变量有 `$m$` 个类别，
只需要引入 `$m-1$` 个虚拟变量。例如对于季度数据，需要引入 3 个虚拟变量；
对于月度数据，需要引入 11 个虚拟变量；对于日度数据，需要引入 6 个虚拟变量。

与虚拟变量相关的每个系数的解释是 该类别相对于忽略的类别对模型的影响程度 。
在上例中，“周一”的系数 `$d_{1,t}$` 即是与“周日”相比，“周一”对被预测变量的影响。

## 干预变量

建模时，我们通常需要考虑可能对被预测变量的产生影响的干预因素。
例如竞争对手的活动、广告支出、工业行动等等都会对被预测变量产生影响。

* 当干预因素的影响仅持续一个时期时，
  我们可以使用 <span style='border-bottom:1.5px dashed red;'>“尖峰”变量</span> 来描述。
  尖峰变量的处理方法和处理离群点非常相似，也是构造一个虚拟变量，
  在干预因素作用期间取值1，在其他地方取 0。
* 干预因素的影响还可能是长期或永久的。
    - 如果干预因素导致水平偏移（即序列的值从干预时间点之后突然且永久地改变），
      那么我们使用 <span style='border-bottom:1.5px dashed red;'>“阶梯”变量</span>。
      阶梯变量在干预产生之前取值为 0，从干预产生之后取值为 1。
    - 干预因素的另一种长远影响是斜率的变化。
      此时需采取 <span style='border-bottom:1.5px dashed red;'>分段处理</span>，
      在干预因素产生影响前后斜率是不同的，因此模型是非线性的。

## 交易日

一个月的交易日数可能会有很大差异，并会对销售数据产生重大影响。
为此，可以将每个月的交易日数作为预测变量。

对于月度或季度数据，可以计算出每个时期内的交易日数。可以引入 7 个解释变量，每个解释变量定义如下：

`$$x_{1} = \text{当月中周一的数目}$$`
`$$x_{2} = \text{当月中周二的数目}$$`
`$$\cdots$$`
`$$x_{7} = \text{当月中周日的数目}$$`

## 分布滞后

通常情况下，把广告支出作为解释变量会十分有效。但是，广告效应往往会具有滞后性。
因此，我们可以使用如下变量：

`$$x_{1} = \text{一个月前的广告支出}$$`
`$$x_{2} = \text{两个月前的广告支出}$$`
`$$\cdots$$`
`$$x_{m} = \text{m 个月前的广告支出}$$`

一般情况下，系数随着滞后阶数的增加而减小。

## 复活节

复活节与其他大多数的假期不同，因为它不是每年在同一天举行，并且其影响可持续一段时间。
在这种情况下，在复活节特定的时间段内，虚拟变量取值为 1，在其他时间段内取值为 0。
当复活节从 3 月开始到 4 月结束时，虚拟变量在月份之间按比例分配。

当数据为月度数据时，若复活节在三月份，那么虚拟变量在三月份时取 1；
同样的，若复活节在四月份时，虚拟变量在四月份时取 1。
当复活节从 3 月开始到 4 月结束时，虚拟变量在月份之间按比例分配。

## 傅里叶级数

对于季节性虚拟变量，尤其是长季节周期，通常可以采用傅里叶级数。
让·巴普蒂斯·约瑟夫·傅里叶是一位出生于 18 世纪的法国数学家。
他表明一定频率的一系列正弦和余弦项可以逼近任何周期函数。
我们可以把傅里叶级数用于季节模式中。

当序列的季节周期为 `$mn$` 时，其傅里叶级数的前几项为：

`$$x_{1,t} = sin\Big(\frac{2\pi t}{m}\Big)$$`
`$$x_{2,t} = cos\Big(\frac{2\pi t}{m}\Big)$$`
`$$x_{3,t} = sin\Big(\frac{4\pi t}{m}\Big)$$`
`$$x_{4,t} = cos\Big(\frac{4\pi t}{m}\Big)$$`
`$$x_{5,t} = sin\Big(\frac{6\pi t}{m}\Big)$$`
`$$x_{6,t} = cos\Big(\frac{6\pi t}{m}\Big)$$`

如果数据中存在月度季节性，那么我们使用这些预测变量中的前 11 个，
我们将得到与使用 11 个虚拟变量完全相同的预测。

当采用傅里叶级数时，尤其当 `$m$` 值很大时，
我们通常可以用较少的预测变量得到与采用虚拟变量相同的预测结果。
例如，周度数据中 `$m\approx 52$`，因此这对于周度数据非常有效。
对于短季节周期数据（例如，季度数据），使用傅里叶级数相比于季节性虚拟变量几乎没有优势。

如果仅使用前两个傅立叶项（`$x_{1,t}$` 和 `$x_{2,t}$`），
此时季节性模式将遵循简单的正弦波。由于连续的傅里叶项表示前两个傅立叶项的谐波，
因此包含傅里叶项的回归模型通常称为 <span style='border-bottom:1.5px dashed red;'>谐波回归</span>。

# 预测变量的筛选

当存在很多备选的预测变量时，我们需要从中筛选出一部分较好的预测变量供回归模型使用。

一个常见的但是不推荐的方法是画出被预测变量和特定的预测变量之间的关系图，
如果不能看出明显的相关关系，则删除该预测变量。但这个方法常常会失效，
尤其在未考虑其他预测变量时，散点图并不总能正确的反映两个变量之间的关系。

另一种常见的无效方法是对所有预测变量进行多元线性回归，并删除所有 `$p$` 值大于 0.05 的所有变量。
统计显著性并不总能表示预测变量的预测价值。因为当两个或者多个预测变量相互关联时，
`$p$` 值可能会是错误的结果。

因此，我们通过计算 <span style='border-bottom:1.5px dashed red;'>模型精度</span> 来筛选变量。

## 调整的可决系数

## 交叉检验


## 赤池信息准则

## 修正的赤池信息准则


## 施瓦茨的贝叶斯信息准则

## 准则的选择



# 回归预测


# 非线性回归


# 相关关系、因果关系和预测


# 矩阵方程

